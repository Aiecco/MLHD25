{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MLTFsF3ieBbv",
    "outputId": "70644b29-9c90-438e-b16d-daadff836426"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# import functions from Gender CNN notebook\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks, regularizers, models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob  # Can be useful but we use os\n",
    "import time\n",
    "\n",
    "MODEL_SAVE_PATH = 'Models/attention_model.keras'\n",
    "LEARNING_RATE      = 5e-4\n",
    "IMG_HEIGHT         = 300\n",
    "IMG_WIDTH          = 300\n",
    "BATCH_SIZE         = 32\n",
    "EPOCHS             = 50\n",
    "AUTOTUNE      = tf.data.AUTOTUNE\n",
    "CHANNELS = 1\n",
    "SIZE = 300\n",
    "PATIENCE_ES = 10\n",
    "PATIENCE_RLR = 5\n",
    "LR_FACTOR = 0.5\n",
    "MIN_LR = 1e-6\n",
    "base_dir = '/content/drive/MyDrive/MLHD'\n",
    "\n",
    "# Data Paths\n",
    "train_csv_path = os.path.join(base_dir, 'Train', 'train_labels.csv')\n",
    "val_csv_path = os.path.join(base_dir, 'Val', 'val_labels.csv')\n",
    "test_csv_path = os.path.join(base_dir, 'Test', 'test_labels.csv')\n",
    "\n",
    "train_image_dir = os.path.join(base_dir, 'Train', 'train_samples_pp')\n",
    "val_image_dir = os.path.join(base_dir, 'Val', 'val_samples_pp')\n",
    "test_image_dir = os.path.join(base_dir, 'Test', 'test_samples_pp')\n",
    "\n",
    "\n",
    "\n",
    "def load_labels(csv_path):\n",
    "    df = pd.read_csv(csv_path, index_col='id')\n",
    "\n",
    "    df = df[['boneage', 'male']].rename(columns={'male': 'gender'})\n",
    "    df['gender'] = df['gender'].astype(np.float32)\n",
    "    df['boneage'] = df['boneage'].astype(np.float32)\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_dataframe(image_dir, labels_df):\n",
    "    data = []\n",
    "\n",
    "    for filename in os.listdir(image_dir):\n",
    "        file_id = int(filename.split('.')[0])\n",
    "        if file_id in labels_df.index:\n",
    "            boneage = labels_df.loc[file_id, 'boneage']\n",
    "            gender = labels_df.loc[file_id, 'gender']\n",
    "            full_path = os.path.join(image_dir, filename)\n",
    "            data.append({'file_path': full_path, 'boneage': boneage, 'gender': gender})\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def preprocess_image(path, boneage):\n",
    "    # Read + decode to [H,W,CHANNELS]\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.io.decode_image(img, channels=CHANNELS, expand_animations=False)\n",
    "    img = tf.image.resize(img, [SIZE, SIZE])\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    return img, boneage\n",
    "\n",
    "\n",
    "def image_label_generator(file_paths, boneage_labels, gender_labels):\n",
    "    \"\"\"tuple of (boneage, gender) labels.\"\"\"\n",
    "    for path, boneage, gender in zip(file_paths, boneage_labels, gender_labels):\n",
    "        try:\n",
    "            img_bytes = tf.io.read_file(path)\n",
    "\n",
    "            image = tf.io.decode_image(img_bytes, channels=CHANNELS, expand_animations=False)\n",
    "\n",
    "            # allow dynamic height/width initially\n",
    "            image.set_shape([None, None, CHANNELS])\n",
    "            yield image, (boneage, gender)  # Yield image and label tuple\n",
    "        except tf.errors.InvalidArgumentError as e:\n",
    "            print(f\"Warning: Skipping file {path}. Error decoding image: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Skipping file {path}. Unexpected error: {e}\")\n",
    "\n",
    "\n",
    "def create_tf_dataset(df, shuffle: bool, repeat: bool):\n",
    "    \"\"\"\n",
    "    Returns a tf.data.Dataset yielding (image, boneage) pairs,\n",
    "    batched, shuffled/prefetched as specified.\n",
    "    \"\"\"\n",
    "    paths    = df['file_path'].values\n",
    "    boneages = df['boneage'].values\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, boneages))\n",
    "    ds = ds.map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(df), reshuffle_each_iteration=True)\n",
    "    if repeat:\n",
    "        ds = ds.repeat()\n",
    "\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    ds = ds.prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "\n",
    "# labels\n",
    "train_labels_df = load_labels(train_csv_path)\n",
    "val_labels_df = load_labels(val_csv_path)\n",
    "test_labels_df = load_labels(test_csv_path)\n",
    "\n",
    "# df\n",
    "training_dataframe = create_dataframe(train_image_dir, train_labels_df)\n",
    "validation_dataframe = create_dataframe(val_image_dir, val_labels_df)\n",
    "test_dataframe = create_dataframe(test_image_dir, test_labels_df)\n",
    "\n",
    "\n",
    "train_dataset = create_tf_dataset(training_dataframe, shuffle=True,  repeat=True)\n",
    "val_dataset   = create_tf_dataset(validation_dataframe,   shuffle=False, repeat=True)\n",
    "test_dataset  = create_tf_dataset(test_dataframe,          shuffle=False, repeat=False)\n"
   ],
   "metadata": {
    "id": "Oc4cez1mOxi8"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def build_model(img_size=(IMG_HEIGHT, IMG_WIDTH)):\n",
    "    inp = layers.Input(shape=(*img_size, 1), name='input_image')\n",
    "\n",
    "    # --- CNN Backbone ---\n",
    "    def cnn_block(x, filters, prefix):\n",
    "        x = layers.Conv2D(filters, 3, padding='same',\n",
    "                          kernel_regularizer=regularizers.l2(1e-4),\n",
    "                          name=f'{prefix}_conv_a')(x)\n",
    "        x = layers.BatchNormalization(name=f'{prefix}_bn_a')(x)\n",
    "        x = layers.Activation('relu', name=f'{prefix}_relu_a')(x)\n",
    "        x = layers.Conv2D(filters, 3, padding='same',\n",
    "                          kernel_regularizer=regularizers.l2(1e-4),\n",
    "                          name=f'{prefix}_conv_b')(x)\n",
    "        x = layers.BatchNormalization(name=f'{prefix}_bn_b')(x)\n",
    "        x = layers.Activation('relu', name=f'{prefix}_relu_b')(x)\n",
    "        return layers.MaxPooling2D(2, 2, name=f'{prefix}_pool')(x)\n",
    "\n",
    "    x = inp\n",
    "    for i, f in enumerate([32, 64, 128, 256, 256], start=1):\n",
    "        x = cnn_block(x, f, prefix=f'block{i}')\n",
    "\n",
    "    # --- Spatial Attention ---\n",
    "    # CORRECTED: Wrap tf functions in Lambda layers\n",
    "    avg_pool = layers.Lambda(lambda t: tf.reduce_mean(t, axis=-1, keepdims=True),\n",
    "                             name='att_avg_pool')(x)\n",
    "    max_pool = layers.Lambda(lambda t: tf.reduce_max(t, axis=-1, keepdims=True),\n",
    "                             name='att_max_pool')(x)\n",
    "\n",
    "    concat   = layers.Concatenate(name='att_concat')([avg_pool, max_pool])\n",
    "    att_mid  = layers.Conv2D(32, 5, padding='same', activation='relu',\n",
    "                              name='att_inter_conv')(concat)\n",
    "    att_map  = layers.Conv2D(1, 7, padding='same', activation='sigmoid',\n",
    "                              use_bias=False, name='att_final_conv')(att_mid)\n",
    "    x = layers.Multiply(name='apply_attention')([x, att_map])\n",
    "\n",
    "    # --- Regression Head ---\n",
    "    x = layers.Flatten(name='flatten')(x)\n",
    "    for units, drop, name in [(512, .4, 'fc1'), (256, .4, 'fc2'), (128, .3, 'fc3')]:\n",
    "        x = layers.Dense(units, activation='relu',\n",
    "                         kernel_regularizer=regularizers.l2(1e-4),\n",
    "                         name=f'{name}_dense')(x)\n",
    "        x = layers.BatchNormalization(name=f'{name}_bn')(x)\n",
    "        x = layers.Dropout(drop, name=f'{name}_dropout')(x)\n",
    "\n",
    "    lin_out = layers.Dense(1, name='age_linear',\n",
    "                           kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    out     = layers.Activation('relu', name='age_output')(lin_out)\n",
    "\n",
    "    model = models.Model(inputs=inp, outputs=out, name='AgePredModel')\n",
    "\n",
    "    # --- COMPILE ---\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mae',\n",
    "                  metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# --- INSTANTIATE MODEL ---\n",
    "model = build_model()\n",
    "model.summary()\n",
    "# --- CALLBACKS ---\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        filepath=MODEL_SAVE_PATH,\n",
    "        monitor='val_mae',\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_mae',\n",
    "        patience=PATIENCE_ES,\n",
    "        mode='min',\n",
    "        verbose=1,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_mae',\n",
    "        factor=LR_FACTOR,\n",
    "        patience=PATIENCE_RLR,\n",
    "        min_lr=MIN_LR,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3zeUOuEEdHcA",
    "outputId": "92326ee7-59b1-418a-a65a-ee184e423db0"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"AgePredModel\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"AgePredModel\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape     \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m   Param #\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mConnected to     \u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_image         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m300\u001B[0m, \u001B[38;5;34m300\u001B[0m,  │          \u001B[38;5;34m0\u001B[0m │ -                 │\n",
       "│ (\u001B[38;5;33mInputLayer\u001B[0m)        │ \u001B[38;5;34m1\u001B[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block1_conv_a       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m300\u001B[0m, \u001B[38;5;34m300\u001B[0m,  │        \u001B[38;5;34m320\u001B[0m │ input_image[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m] │\n",
       "│ (\u001B[38;5;33mConv2D\u001B[0m)            │ \u001B[38;5;34m32\u001B[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block1_bn_a         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m300\u001B[0m, \u001B[38;5;34m300\u001B[0m,  │        \u001B[38;5;34m128\u001B[0m │ block1_conv_a[\u001B[38;5;34m0\u001B[0m]… │\n",
       "│ (\u001B[38;5;33mBatchNormalizatio…\u001B[0m │ \u001B[38;5;34m32\u001B[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block1_relu_a       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m300\u001B[0m, \u001B[38;5;34m300\u001B[0m,  │          \u001B[38;5;34m0\u001B[0m │ block1_bn_a[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m] │\n",
       "│ (\u001B[38;5;33mActivation\u001B[0m)        │ \u001B[38;5;34m32\u001B[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block1_conv_b       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m300\u001B[0m, \u001B[38;5;34m300\u001B[0m,  │      \u001B[38;5;34m9,248\u001B[0m │ block1_relu_a[\u001B[38;5;34m0\u001B[0m]… │\n",
       "│ (\u001B[38;5;33mConv2D\u001B[0m)            │ \u001B[38;5;34m32\u001B[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block1_bn_b         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m300\u001B[0m, \u001B[38;5;34m300\u001B[0m,  │        \u001B[38;5;34m128\u001B[0m │ block1_conv_b[\u001B[38;5;34m0\u001B[0m]… │\n",
       "│ (\u001B[38;5;33mBatchNormalizatio…\u001B[0m │ \u001B[38;5;34m32\u001B[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block1_relu_b       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m300\u001B[0m, \u001B[38;5;34m300\u001B[0m,  │          \u001B[38;5;34m0\u001B[0m │ block1_bn_b[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m] │\n",
       "│ (\u001B[38;5;33mActivation\u001B[0m)        │ \u001B[38;5;34m32\u001B[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block1_pool         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m150\u001B[0m, \u001B[38;5;34m150\u001B[0m,  │          \u001B[38;5;34m0\u001B[0m │ block1_relu_b[\u001B[38;5;34m0\u001B[0m]… │\n",
       "│ (\u001B[38;5;33mMaxPooling2D\u001B[0m)      │ \u001B[38;5;34m32\u001B[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block2_conv_a       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m150\u001B[0m, \u001B[38;5;34m150\u001B[0m,  │     \u001B[38;5;34m18,496\u001B[0m │ block1_pool[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m] │\n",
       "│ (\u001B[38;5;33mConv2D\u001B[0m)            │ \u001B[38;5;34m64\u001B[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block2_bn_a         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m150\u001B[0m, \u001B[38;5;34m150\u001B[0m,  │        \u001B[38;5;34m256\u001B[0m │ block2_conv_a[\u001B[38;5;34m0\u001B[0m]… │\n",
       "│ (\u001B[38;5;33mBatchNormalizatio…\u001B[0m │ \u001B[38;5;34m64\u001B[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block2_relu_a       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m150\u001B[0m, \u001B[38;5;34m150\u001B[0m,  │          \u001B[38;5;34m0\u001B[0m │ block2_bn_a[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m] │\n",
       "│ (\u001B[38;5;33mActivation\u001B[0m)        │ \u001B[38;5;34m64\u001B[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block2_conv_b       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m150\u001B[0m, \u001B[38;5;34m150\u001B[0m,  │     \u001B[38;5;34m36,928\u001B[0m │ block2_relu_a[\u001B[38;5;34m0\u001B[0m]… │\n",
       "│ (\u001B[38;5;33mConv2D\u001B[0m)            │ \u001B[38;5;34m64\u001B[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block2_bn_b         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m150\u001B[0m, \u001B[38;5;34m150\u001B[0m,  │        \u001B[38;5;34m256\u001B[0m │ block2_conv_b[\u001B[38;5;34m0\u001B[0m]… │\n",
       "│ (\u001B[38;5;33mBatchNormalizatio…\u001B[0m │ \u001B[38;5;34m64\u001B[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block2_relu_b       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m150\u001B[0m, \u001B[38;5;34m150\u001B[0m,  │          \u001B[38;5;34m0\u001B[0m │ block2_bn_b[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m] │\n",
       "│ (\u001B[38;5;33mActivation\u001B[0m)        │ \u001B[38;5;34m64\u001B[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block2_pool         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m75\u001B[0m, \u001B[38;5;34m75\u001B[0m,    │          \u001B[38;5;34m0\u001B[0m │ block2_relu_b[\u001B[38;5;34m0\u001B[0m]… │\n",
       "│ (\u001B[38;5;33mMaxPooling2D\u001B[0m)      │ \u001B[38;5;34m64\u001B[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block3_conv_a       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m75\u001B[0m, \u001B[38;5;34m75\u001B[0m,    │     \u001B[38;5;34m73,856\u001B[0m │ block2_pool[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m] │\n",
       "│ (\u001B[38;5;33mConv2D\u001B[0m)            │ \u001B[38;5;34m128\u001B[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block3_bn_a         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m75\u001B[0m, \u001B[38;5;34m75\u001B[0m,    │        \u001B[38;5;34m512\u001B[0m │ block3_conv_a[\u001B[38;5;34m0\u001B[0m]… │\n",
       "│ (\u001B[38;5;33mBatchNormalizatio…\u001B[0m │ \u001B[38;5;34m128\u001B[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block3_relu_a       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m75\u001B[0m, \u001B[38;5;34m75\u001B[0m,    │          \u001B[38;5;34m0\u001B[0m │ block3_bn_a[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m] │\n",
       "│ (\u001B[38;5;33mActivation\u001B[0m)        │ \u001B[38;5;34m128\u001B[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block3_conv_b       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m75\u001B[0m, \u001B[38;5;34m75\u001B[0m,    │    \u001B[38;5;34m147,584\u001B[0m │ block3_relu_a[\u001B[38;5;34m0\u001B[0m]… │\n",
       "│ (\u001B[38;5;33mConv2D\u001B[0m)            │ \u001B[38;5;34m128\u001B[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block3_bn_b         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m75\u001B[0m, \u001B[38;5;34m75\u001B[0m,    │        \u001B[38;5;34m512\u001B[0m │ block3_conv_b[\u001B[38;5;34m0\u001B[0m]… │\n",
       "│ (\u001B[38;5;33mBatchNormalizatio…\u001B[0m │ \u001B[38;5;34m128\u001B[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block3_relu_b       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m75\u001B[0m, \u001B[38;5;34m75\u001B[0m,    │          \u001B[38;5;34m0\u001B[0m │ block3_bn_b[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m] │\n",
       "│ (\u001B[38;5;33mActivation\u001B[0m)        │ \u001B[38;5;34m128\u001B[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block3_pool         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m37\u001B[0m, \u001B[38;5;34m37\u001B[0m,    │          \u001B[38;5;34m0\u001B[0m │ block3_relu_b[\u001B[38;5;34m0\u001B[0m]… │\n",
       "│ (\u001B[38;5;33mMaxPooling2D\u001B[0m)      │ \u001B[38;5;34m128\u001B[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block4_conv_a       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m37\u001B[0m, \u001B[38;5;34m37\u001B[0m,    │    \u001B[38;5;34m295,168\u001B[0m │ block3_pool[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m] │\n",
       "│ (\u001B[38;5;33mConv2D\u001B[0m)            │ \u001B[38;5;34m256\u001B[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block4_bn_a         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m37\u001B[0m, \u001B[38;5;34m37\u001B[0m,    │      \u001B[38;5;34m1,024\u001B[0m │ block4_conv_a[\u001B[38;5;34m0\u001B[0m]… │\n",
       "│ (\u001B[38;5;33mBatchNormalizatio…\u001B[0m │ \u001B[38;5;34m256\u001B[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block4_relu_a       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m37\u001B[0m, \u001B[38;5;34m37\u001B[0m,    │          \u001B[38;5;34m0\u001B[0m │ block4_bn_a[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m] │\n",
       "│ (\u001B[38;5;33mActivation\u001B[0m)        │ \u001B[38;5;34m256\u001B[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block4_conv_b       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m37\u001B[0m, \u001B[38;5;34m37\u001B[0m,    │    \u001B[38;5;34m590,080\u001B[0m │ block4_relu_a[\u001B[38;5;34m0\u001B[0m]… │\n",
       "│ (\u001B[38;5;33mConv2D\u001B[0m)            │ \u001B[38;5;34m256\u001B[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block4_bn_b         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m37\u001B[0m, \u001B[38;5;34m37\u001B[0m,    │      \u001B[38;5;34m1,024\u001B[0m │ block4_conv_b[\u001B[38;5;34m0\u001B[0m]… │\n",
       "│ (\u001B[38;5;33mBatchNormalizatio…\u001B[0m │ \u001B[38;5;34m256\u001B[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block4_relu_b       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m37\u001B[0m, \u001B[38;5;34m37\u001B[0m,    │          \u001B[38;5;34m0\u001B[0m │ block4_bn_b[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m] │\n",
       "│ (\u001B[38;5;33mActivation\u001B[0m)        │ \u001B[38;5;34m256\u001B[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block4_pool         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m18\u001B[0m, \u001B[38;5;34m18\u001B[0m,    │          \u001B[38;5;34m0\u001B[0m │ block4_relu_b[\u001B[38;5;34m0\u001B[0m]… │\n",
       "│ (\u001B[38;5;33mMaxPooling2D\u001B[0m)      │ \u001B[38;5;34m256\u001B[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block5_conv_a       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m18\u001B[0m, \u001B[38;5;34m18\u001B[0m,    │    \u001B[38;5;34m590,080\u001B[0m │ block4_pool[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m] │\n",
       "│ (\u001B[38;5;33mConv2D\u001B[0m)            │ \u001B[38;5;34m256\u001B[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block5_bn_a         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m18\u001B[0m, \u001B[38;5;34m18\u001B[0m,    │      \u001B[38;5;34m1,024\u001B[0m │ block5_conv_a[\u001B[38;5;34m0\u001B[0m]… │\n",
       "│ (\u001B[38;5;33mBatchNormalizatio…\u001B[0m │ \u001B[38;5;34m256\u001B[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block5_relu_a       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m18\u001B[0m, \u001B[38;5;34m18\u001B[0m,    │          \u001B[38;5;34m0\u001B[0m │ block5_bn_a[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m] │\n",
       "│ (\u001B[38;5;33mActivation\u001B[0m)        │ \u001B[38;5;34m256\u001B[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block5_conv_b       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m18\u001B[0m, \u001B[38;5;34m18\u001B[0m,    │    \u001B[38;5;34m590,080\u001B[0m │ block5_relu_a[\u001B[38;5;34m0\u001B[0m]… │\n",
       "│ (\u001B[38;5;33mConv2D\u001B[0m)            │ \u001B[38;5;34m256\u001B[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block5_bn_b         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m18\u001B[0m, \u001B[38;5;34m18\u001B[0m,    │      \u001B[38;5;34m1,024\u001B[0m │ block5_conv_b[\u001B[38;5;34m0\u001B[0m]… │\n",
       "│ (\u001B[38;5;33mBatchNormalizatio…\u001B[0m │ \u001B[38;5;34m256\u001B[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block5_relu_b       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m18\u001B[0m, \u001B[38;5;34m18\u001B[0m,    │          \u001B[38;5;34m0\u001B[0m │ block5_bn_b[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m] │\n",
       "│ (\u001B[38;5;33mActivation\u001B[0m)        │ \u001B[38;5;34m256\u001B[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block5_pool         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m9\u001B[0m, \u001B[38;5;34m9\u001B[0m, \u001B[38;5;34m256\u001B[0m) │          \u001B[38;5;34m0\u001B[0m │ block5_relu_b[\u001B[38;5;34m0\u001B[0m]… │\n",
       "│ (\u001B[38;5;33mMaxPooling2D\u001B[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ att_avg_pool        │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m9\u001B[0m, \u001B[38;5;34m9\u001B[0m, \u001B[38;5;34m1\u001B[0m)   │          \u001B[38;5;34m0\u001B[0m │ block5_pool[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m] │\n",
       "│ (\u001B[38;5;33mLambda\u001B[0m)            │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ att_max_pool        │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m9\u001B[0m, \u001B[38;5;34m9\u001B[0m, \u001B[38;5;34m1\u001B[0m)   │          \u001B[38;5;34m0\u001B[0m │ block5_pool[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m] │\n",
       "│ (\u001B[38;5;33mLambda\u001B[0m)            │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ att_concat          │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m9\u001B[0m, \u001B[38;5;34m9\u001B[0m, \u001B[38;5;34m2\u001B[0m)   │          \u001B[38;5;34m0\u001B[0m │ att_avg_pool[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m…\u001B[0m │\n",
       "│ (\u001B[38;5;33mConcatenate\u001B[0m)       │                   │            │ att_max_pool[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m…\u001B[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ att_inter_conv      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m9\u001B[0m, \u001B[38;5;34m9\u001B[0m, \u001B[38;5;34m32\u001B[0m)  │      \u001B[38;5;34m1,632\u001B[0m │ att_concat[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]  │\n",
       "│ (\u001B[38;5;33mConv2D\u001B[0m)            │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ att_final_conv      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m9\u001B[0m, \u001B[38;5;34m9\u001B[0m, \u001B[38;5;34m1\u001B[0m)   │      \u001B[38;5;34m1,568\u001B[0m │ att_inter_conv[\u001B[38;5;34m0\u001B[0m… │\n",
       "│ (\u001B[38;5;33mConv2D\u001B[0m)            │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ apply_attention     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m9\u001B[0m, \u001B[38;5;34m9\u001B[0m, \u001B[38;5;34m256\u001B[0m) │          \u001B[38;5;34m0\u001B[0m │ block5_pool[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m… │\n",
       "│ (\u001B[38;5;33mMultiply\u001B[0m)          │                   │            │ att_final_conv[\u001B[38;5;34m0\u001B[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (\u001B[38;5;33mFlatten\u001B[0m)   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m20736\u001B[0m)     │          \u001B[38;5;34m0\u001B[0m │ apply_attention[\u001B[38;5;34m…\u001B[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ fc1_dense (\u001B[38;5;33mDense\u001B[0m)   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m512\u001B[0m)       │ \u001B[38;5;34m10,617,344\u001B[0m │ flatten[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ fc1_bn              │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m512\u001B[0m)       │      \u001B[38;5;34m2,048\u001B[0m │ fc1_dense[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]   │\n",
       "│ (\u001B[38;5;33mBatchNormalizatio…\u001B[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ fc1_dropout         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m512\u001B[0m)       │          \u001B[38;5;34m0\u001B[0m │ fc1_bn[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]      │\n",
       "│ (\u001B[38;5;33mDropout\u001B[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ fc2_dense (\u001B[38;5;33mDense\u001B[0m)   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m)       │    \u001B[38;5;34m131,328\u001B[0m │ fc1_dropout[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ fc2_bn              │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m)       │      \u001B[38;5;34m1,024\u001B[0m │ fc2_dense[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]   │\n",
       "│ (\u001B[38;5;33mBatchNormalizatio…\u001B[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ fc2_dropout         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m)       │          \u001B[38;5;34m0\u001B[0m │ fc2_bn[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]      │\n",
       "│ (\u001B[38;5;33mDropout\u001B[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ fc3_dense (\u001B[38;5;33mDense\u001B[0m)   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)       │     \u001B[38;5;34m32,896\u001B[0m │ fc2_dropout[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ fc3_bn              │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)       │        \u001B[38;5;34m512\u001B[0m │ fc3_dense[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]   │\n",
       "│ (\u001B[38;5;33mBatchNormalizatio…\u001B[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ fc3_dropout         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)       │          \u001B[38;5;34m0\u001B[0m │ fc3_bn[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]      │\n",
       "│ (\u001B[38;5;33mDropout\u001B[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ age_linear (\u001B[38;5;33mDense\u001B[0m)  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)         │        \u001B[38;5;34m129\u001B[0m │ fc3_dropout[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ age_output          │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)         │          \u001B[38;5;34m0\u001B[0m │ age_linear[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]  │\n",
       "│ (\u001B[38;5;33mActivation\u001B[0m)        │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_image         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block1_conv_a       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │ input_image[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block1_bn_a         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ block1_conv_a[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block1_relu_a       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block1_bn_a[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block1_conv_b       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │ block1_relu_a[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block1_bn_b         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ block1_conv_b[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block1_relu_b       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block1_bn_b[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block1_pool         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block1_relu_b[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block2_conv_a       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>,  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ block1_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block2_bn_a         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ block2_conv_a[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block2_relu_a       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block2_bn_a[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block2_conv_b       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>,  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │ block2_relu_a[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block2_bn_b         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ block2_conv_b[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block2_relu_b       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block2_bn_b[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block2_pool         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block2_relu_b[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block3_conv_a       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │ block2_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block3_bn_a         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ block3_conv_a[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block3_relu_a       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block3_bn_a[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block3_conv_b       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │ block3_relu_a[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block3_bn_b         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ block3_conv_b[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block3_relu_b       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block3_bn_b[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block3_pool         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block3_relu_b[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block4_conv_a       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │ block3_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block4_bn_a         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ block4_conv_a[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block4_relu_a       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block4_bn_a[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block4_conv_b       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │ block4_relu_a[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block4_bn_b         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ block4_conv_b[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block4_relu_b       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block4_bn_b[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block4_pool         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block4_relu_b[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block5_conv_a       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │ block4_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block5_bn_a         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ block5_conv_a[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block5_relu_a       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block5_bn_a[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block5_conv_b       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │ block5_relu_a[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block5_bn_b         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ block5_conv_b[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block5_relu_b       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block5_bn_b[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block5_pool         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block5_relu_b[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ att_avg_pool        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block5_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)            │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ att_max_pool        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block5_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)            │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ att_concat          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ att_avg_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ att_max_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ att_inter_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,632</span> │ att_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ att_final_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,568</span> │ att_inter_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ apply_attention     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block5_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)          │                   │            │ att_final_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20736</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ apply_attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ fc1_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">10,617,344</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ fc1_bn              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ fc1_dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ fc1_dropout         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ fc1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ fc2_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ fc1_dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ fc2_bn              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ fc2_dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ fc2_dropout         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ fc2_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ fc3_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ fc2_dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ fc3_bn              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ fc3_dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ fc3_dropout         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ fc3_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ age_linear (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ fc3_dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ age_output          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ age_linear[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m13,146,209\u001B[0m (50.15 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,146,209</span> (50.15 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m13,141,473\u001B[0m (50.13 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,141,473</span> (50.13 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m4,736\u001B[0m (18.50 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,736</span> (18.50 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Number of CV folds\n",
    "N_SPLITS = 5\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "# Will hold the dicts of loss/metric curves per fold\n",
    "all_hist = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(training_dataframe), start=1):\n",
    "    print(f\"\\n>>> Fold {fold}/{N_SPLITS}\")\n",
    "\n",
    "    # 1) Split the dataframe\n",
    "    df_train = training_dataframe.iloc[train_idx]\n",
    "    df_val   = training_dataframe.iloc[val_idx]\n",
    "\n",
    "    # 2) Build datasets\n",
    "    train_ds = create_tf_dataset(df_train, shuffle=True,  repeat=True)\n",
    "    val_ds   = create_tf_dataset(df_val,   shuffle=False, repeat=False)\n",
    "\n",
    "    # 3) Compute steps\n",
    "    steps_per_epoch = len(df_train) // BATCH_SIZE\n",
    "    validation_steps = len(df_val) // BATCH_SIZE\n",
    "\n",
    "    # 4) Fresh model for this fold\n",
    "    model = build_model()  # returns a compiled tf.keras.Model\n",
    "\n",
    "    # 5) (Re-)create callbacks if you want per-fold saving/early stopping\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            filepath=f'model_fold{fold}.keras',\n",
    "            monitor='val_mae',\n",
    "            save_best_only=True,\n",
    "            mode='min',\n",
    "            verbose=1\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor='val_mae',\n",
    "            patience=PATIENCE_ES,\n",
    "            mode='min',\n",
    "            verbose=1,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_mae',\n",
    "            factor=LR_FACTOR,\n",
    "            patience=PATIENCE_RLR,\n",
    "            min_lr=MIN_LR,\n",
    "            mode='min',\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # 6) Train\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=EPOCHS,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=val_ds,\n",
    "        validation_steps=validation_steps,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # 7) Store history and (optionally) final evaluation\n",
    "    all_hist.append(history.history)\n",
    "\n",
    "# After loop: all_hist is a list of dicts,\n",
    "# where each dict maps 'loss','mae','val_loss','val_mae' → list over epochs.\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zPdRXoGDdpFg",
    "outputId": "72de5884-4a25-40de-95bd-4d3c76722451"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      ">>> Fold 1/5\n",
      "Epoch 1/50\n",
      "\u001B[1m300/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 118.5160 - mae: 118.2283\n",
      "Epoch 1: val_mae improved from inf to 107.17170, saving model to model_fold1.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m124s\u001B[0m 63ms/step - loss: 118.4850 - mae: 118.1973 - val_loss: 107.4567 - val_mae: 107.1717 - learning_rate: 5.0000e-04\n",
      "Epoch 2/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 98.6505 - mae: 98.3610\n",
      "Epoch 2: val_mae improved from 107.17170 to 96.44464, saving model to model_fold1.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 98.6304 - mae: 98.3408 - val_loss: 96.7676 - val_mae: 96.4446 - learning_rate: 5.0000e-04\n",
      "Epoch 3/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step - loss: 74.4978 - mae: 74.1593\n",
      "Epoch 3: val_mae improved from 96.44464 to 55.88615, saving model to model_fold1.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 57ms/step - loss: 74.4820 - mae: 74.1435 - val_loss: 56.3010 - val_mae: 55.8861 - learning_rate: 5.0000e-04\n",
      "Epoch 4/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 58.9533 - mae: 58.5147\n",
      "Epoch 4: val_mae improved from 55.88615 to 44.10356, saving model to model_fold1.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 58.9434 - mae: 58.5048 - val_loss: 44.5868 - val_mae: 44.1036 - learning_rate: 5.0000e-04\n",
      "Epoch 5/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 48.9322 - mae: 48.4349\n",
      "Epoch 5: val_mae improved from 44.10356 to 40.85584, saving model to model_fold1.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 48.9299 - mae: 48.4325 - val_loss: 41.4168 - val_mae: 40.8558 - learning_rate: 5.0000e-04\n",
      "Epoch 6/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 46.1957 - mae: 45.6069\n",
      "Epoch 6: val_mae improved from 40.85584 to 33.73026, saving model to model_fold1.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 55ms/step - loss: 46.1891 - mae: 45.6002 - val_loss: 34.3515 - val_mae: 33.7303 - learning_rate: 5.0000e-04\n",
      "Epoch 7/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 40.6286 - mae: 40.0083\n",
      "Epoch 7: val_mae did not improve from 33.73026\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 54ms/step - loss: 40.6267 - mae: 40.0064 - val_loss: 35.3507 - val_mae: 34.7292 - learning_rate: 5.0000e-04\n",
      "Epoch 8/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 38.7676 - mae: 38.1471\n",
      "Epoch 8: val_mae improved from 33.73026 to 32.90109, saving model to model_fold1.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 55ms/step - loss: 38.7665 - mae: 38.1459 - val_loss: 33.5217 - val_mae: 32.9011 - learning_rate: 5.0000e-04\n",
      "Epoch 9/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 36.9486 - mae: 36.3286\n",
      "Epoch 9: val_mae did not improve from 32.90109\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 36.9486 - mae: 36.3287 - val_loss: 34.4084 - val_mae: 33.7951 - learning_rate: 5.0000e-04\n",
      "Epoch 10/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 37.7982 - mae: 37.1833\n",
      "Epoch 10: val_mae did not improve from 32.90109\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 37.7982 - mae: 37.1833 - val_loss: 33.5992 - val_mae: 32.9723 - learning_rate: 5.0000e-04\n",
      "Epoch 11/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 37.1485 - mae: 36.5223\n",
      "Epoch 11: val_mae did not improve from 32.90109\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 37.1501 - mae: 36.5239 - val_loss: 34.3245 - val_mae: 33.7041 - learning_rate: 5.0000e-04\n",
      "Epoch 12/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 36.9873 - mae: 36.3708\n",
      "Epoch 12: val_mae did not improve from 32.90109\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 36.9870 - mae: 36.3706 - val_loss: 43.2436 - val_mae: 42.6258 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 36.6410 - mae: 36.0194\n",
      "Epoch 13: val_mae improved from 32.90109 to 32.62646, saving model to model_fold1.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 36.6405 - mae: 36.0189 - val_loss: 33.2562 - val_mae: 32.6265 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 37.0691 - mae: 36.4361\n",
      "Epoch 14: val_mae did not improve from 32.62646\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 52ms/step - loss: 37.0680 - mae: 36.4349 - val_loss: 39.2293 - val_mae: 38.5863 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 35.0599 - mae: 34.4159\n",
      "Epoch 15: val_mae did not improve from 32.62646\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 35.0583 - mae: 34.4143 - val_loss: 33.3825 - val_mae: 32.7473 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 33.8887 - mae: 33.2561\n",
      "Epoch 16: val_mae did not improve from 32.62646\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 33.8884 - mae: 33.2558 - val_loss: 40.4629 - val_mae: 39.8331 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 33.5051 - mae: 32.8765\n",
      "Epoch 17: val_mae improved from 32.62646 to 31.49274, saving model to model_fold1.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 33.5052 - mae: 32.8766 - val_loss: 32.1174 - val_mae: 31.4927 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 32.4049 - mae: 31.7836\n",
      "Epoch 18: val_mae did not improve from 31.49274\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 32.4044 - mae: 31.7831 - val_loss: 38.8962 - val_mae: 38.2853 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 30.9775 - mae: 30.3685\n",
      "Epoch 19: val_mae did not improve from 31.49274\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 52ms/step - loss: 30.9757 - mae: 30.3668 - val_loss: 49.7326 - val_mae: 49.1253 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 29.2146 - mae: 28.6062\n",
      "Epoch 20: val_mae did not improve from 31.49274\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 29.2133 - mae: 28.6049 - val_loss: 40.3356 - val_mae: 39.7212 - learning_rate: 5.0000e-04\n",
      "Epoch 21/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 27.4846 - mae: 26.8684\n",
      "Epoch 21: val_mae did not improve from 31.49274\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 27.4831 - mae: 26.8669 - val_loss: 55.1200 - val_mae: 54.5003 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 26.5896 - mae: 25.9691\n",
      "Epoch 22: val_mae did not improve from 31.49274\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 26.5875 - mae: 25.9669 - val_loss: 33.5183 - val_mae: 32.8944 - learning_rate: 5.0000e-04\n",
      "Epoch 23/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 24.4258 - mae: 23.8028\n",
      "Epoch 23: val_mae improved from 31.49274 to 25.87661, saving model to model_fold1.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 24.4242 - mae: 23.8012 - val_loss: 26.4965 - val_mae: 25.8766 - learning_rate: 2.5000e-04\n",
      "Epoch 24/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 23.4100 - mae: 22.7912\n",
      "Epoch 24: val_mae did not improve from 25.87661\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 23.4104 - mae: 22.7915 - val_loss: 29.5715 - val_mae: 28.9553 - learning_rate: 2.5000e-04\n",
      "Epoch 25/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 22.9231 - mae: 22.3084\n",
      "Epoch 25: val_mae did not improve from 25.87661\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 22.9230 - mae: 22.3083 - val_loss: 37.6114 - val_mae: 36.9997 - learning_rate: 2.5000e-04\n",
      "Epoch 26/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 22.7615 - mae: 22.1506\n",
      "Epoch 26: val_mae did not improve from 25.87661\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 22.7602 - mae: 22.1494 - val_loss: 32.5026 - val_mae: 31.8958 - learning_rate: 2.5000e-04\n",
      "Epoch 27/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 21.8717 - mae: 21.2644\n",
      "Epoch 27: val_mae improved from 25.87661 to 20.26556, saving model to model_fold1.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 55ms/step - loss: 21.8712 - mae: 21.2639 - val_loss: 20.8706 - val_mae: 20.2656 - learning_rate: 2.5000e-04\n",
      "Epoch 28/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 21.6011 - mae: 20.9974\n",
      "Epoch 28: val_mae did not improve from 20.26556\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 21.6011 - mae: 20.9974 - val_loss: 30.7057 - val_mae: 30.1057 - learning_rate: 2.5000e-04\n",
      "Epoch 29/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 21.1850 - mae: 20.5859\n",
      "Epoch 29: val_mae improved from 20.26556 to 19.73838, saving model to model_fold1.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 21.1856 - mae: 20.5866 - val_loss: 20.3342 - val_mae: 19.7384 - learning_rate: 2.5000e-04\n",
      "Epoch 30/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 20.8234 - mae: 20.2284\n",
      "Epoch 30: val_mae did not improve from 19.73838\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 20.8239 - mae: 20.2289 - val_loss: 27.9980 - val_mae: 27.4057 - learning_rate: 2.5000e-04\n",
      "Epoch 31/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 21.0425 - mae: 20.4495\n",
      "Epoch 31: val_mae did not improve from 19.73838\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 21.0419 - mae: 20.4489 - val_loss: 27.8628 - val_mae: 27.2697 - learning_rate: 2.5000e-04\n",
      "Epoch 32/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 20.5453 - mae: 19.9534\n",
      "Epoch 32: val_mae did not improve from 19.73838\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 20.5450 - mae: 19.9531 - val_loss: 30.3117 - val_mae: 29.7192 - learning_rate: 2.5000e-04\n",
      "Epoch 33/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 20.6764 - mae: 20.0844\n",
      "Epoch 33: val_mae did not improve from 19.73838\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 20.6756 - mae: 20.0836 - val_loss: 30.6731 - val_mae: 30.0840 - learning_rate: 2.5000e-04\n",
      "Epoch 34/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 20.3541 - mae: 19.7661\n",
      "Epoch 34: val_mae did not improve from 19.73838\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 20.3540 - mae: 19.7660 - val_loss: 25.4152 - val_mae: 24.8293 - learning_rate: 2.5000e-04\n",
      "Epoch 35/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 19.7834 - mae: 19.1983\n",
      "Epoch 35: val_mae improved from 19.73838 to 17.70222, saving model to model_fold1.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 19.7828 - mae: 19.1977 - val_loss: 18.2845 - val_mae: 17.7022 - learning_rate: 1.2500e-04\n",
      "Epoch 36/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 19.4711 - mae: 18.8897\n",
      "Epoch 36: val_mae did not improve from 17.70222\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 19.4707 - mae: 18.8894 - val_loss: 22.4202 - val_mae: 21.8413 - learning_rate: 1.2500e-04\n",
      "Epoch 37/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 19.1464 - mae: 18.5686\n",
      "Epoch 37: val_mae did not improve from 17.70222\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 52ms/step - loss: 19.1461 - mae: 18.5683 - val_loss: 21.5731 - val_mae: 20.9986 - learning_rate: 1.2500e-04\n",
      "Epoch 38/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 19.3037 - mae: 18.7304\n",
      "Epoch 38: val_mae did not improve from 17.70222\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 19.3032 - mae: 18.7299 - val_loss: 26.4083 - val_mae: 25.8382 - learning_rate: 1.2500e-04\n",
      "Epoch 39/50\n",
      "\u001B[1m300/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 19.0413 - mae: 18.4718\n",
      "Epoch 39: val_mae improved from 17.70222 to 17.29903, saving model to model_fold1.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 19.0410 - mae: 18.4715 - val_loss: 17.8668 - val_mae: 17.2990 - learning_rate: 1.2500e-04\n",
      "Epoch 40/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 18.8551 - mae: 18.2881\n",
      "Epoch 40: val_mae did not improve from 17.29903\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 18.8550 - mae: 18.2880 - val_loss: 29.8149 - val_mae: 29.2500 - learning_rate: 1.2500e-04\n",
      "Epoch 41/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 18.2545 - mae: 17.6904\n",
      "Epoch 41: val_mae did not improve from 17.29903\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 18.2553 - mae: 17.6912 - val_loss: 23.5286 - val_mae: 22.9671 - learning_rate: 1.2500e-04\n",
      "Epoch 42/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 18.9163 - mae: 18.3553\n",
      "Epoch 42: val_mae did not improve from 17.29903\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 18.9161 - mae: 18.3550 - val_loss: 19.9632 - val_mae: 19.4027 - learning_rate: 1.2500e-04\n",
      "Epoch 43/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 18.4778 - mae: 17.9178\n",
      "Epoch 43: val_mae did not improve from 17.29903\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 18.4781 - mae: 17.9181 - val_loss: 22.5721 - val_mae: 22.0136 - learning_rate: 1.2500e-04\n",
      "Epoch 44/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 18.5797 - mae: 18.0214\n",
      "Epoch 44: val_mae improved from 17.29903 to 17.20469, saving model to model_fold1.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 55ms/step - loss: 18.5797 - mae: 18.0213 - val_loss: 17.7631 - val_mae: 17.2047 - learning_rate: 1.2500e-04\n",
      "Epoch 45/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 18.4546 - mae: 17.8972\n",
      "Epoch 45: val_mae improved from 17.20469 to 15.95920, saving model to model_fold1.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 18.4543 - mae: 17.8969 - val_loss: 16.5143 - val_mae: 15.9592 - learning_rate: 1.2500e-04\n",
      "Epoch 46/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 18.3227 - mae: 17.7682\n",
      "Epoch 46: val_mae did not improve from 15.95920\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 18.3225 - mae: 17.7680 - val_loss: 18.0768 - val_mae: 17.5241 - learning_rate: 1.2500e-04\n",
      "Epoch 47/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 18.3389 - mae: 17.7871\n",
      "Epoch 47: val_mae did not improve from 15.95920\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 18.3390 - mae: 17.7871 - val_loss: 16.8704 - val_mae: 16.3205 - learning_rate: 1.2500e-04\n",
      "Epoch 48/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 18.2125 - mae: 17.6633\n",
      "Epoch 48: val_mae did not improve from 15.95920\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 18.2124 - mae: 17.6631 - val_loss: 16.8855 - val_mae: 16.3381 - learning_rate: 1.2500e-04\n",
      "Epoch 49/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 18.4027 - mae: 17.8555\n",
      "Epoch 49: val_mae did not improve from 15.95920\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 18.4024 - mae: 17.8552 - val_loss: 16.6187 - val_mae: 16.0728 - learning_rate: 1.2500e-04\n",
      "Epoch 50/50\n",
      "\u001B[1m300/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 17.9154 - mae: 17.3699\n",
      "Epoch 50: val_mae did not improve from 15.95920\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 17.9157 - mae: 17.3703 - val_loss: 18.1277 - val_mae: 17.5838 - learning_rate: 1.2500e-04\n",
      "Restoring model weights from the end of the best epoch: 45.\n",
      "\n",
      ">>> Fold 2/5\n",
      "Epoch 1/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 118.8209 - mae: 118.5319\n",
      "Epoch 1: val_mae improved from inf to 125.99750, saving model to model_fold2.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m48s\u001B[0m 62ms/step - loss: 118.8042 - mae: 118.5152 - val_loss: 126.2933 - val_mae: 125.9975 - learning_rate: 5.0000e-04\n",
      "Epoch 2/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 97.3722 - mae: 97.0738\n",
      "Epoch 2: val_mae improved from 125.99750 to 78.74725, saving model to model_fold2.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 97.3524 - mae: 97.0540 - val_loss: 79.0575 - val_mae: 78.7472 - learning_rate: 5.0000e-04\n",
      "Epoch 3/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 72.8216 - mae: 72.5079\n",
      "Epoch 3: val_mae improved from 78.74725 to 40.43538, saving model to model_fold2.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 72.8050 - mae: 72.4913 - val_loss: 40.7629 - val_mae: 40.4354 - learning_rate: 5.0000e-04\n",
      "Epoch 4/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 55.1164 - mae: 54.7811\n",
      "Epoch 4: val_mae did not improve from 40.43538\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 55.1071 - mae: 54.7717 - val_loss: 43.9356 - val_mae: 43.5713 - learning_rate: 5.0000e-04\n",
      "Epoch 5/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 46.5748 - mae: 46.2041\n",
      "Epoch 5: val_mae did not improve from 40.43538\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 46.5720 - mae: 46.2013 - val_loss: 95.7191 - val_mae: 95.3316 - learning_rate: 5.0000e-04\n",
      "Epoch 6/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 42.7685 - mae: 42.3664\n",
      "Epoch 6: val_mae improved from 40.43538 to 37.93621, saving model to model_fold2.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 42.7657 - mae: 42.3635 - val_loss: 38.3790 - val_mae: 37.9362 - learning_rate: 5.0000e-04\n",
      "Epoch 7/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 39.1633 - mae: 38.7022\n",
      "Epoch 7: val_mae improved from 37.93621 to 34.66224, saving model to model_fold2.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 39.1619 - mae: 38.7006 - val_loss: 35.2138 - val_mae: 34.6622 - learning_rate: 5.0000e-04\n",
      "Epoch 8/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 36.8083 - mae: 36.2388\n",
      "Epoch 8: val_mae did not improve from 34.66224\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m19s\u001B[0m 65ms/step - loss: 36.8066 - mae: 36.2371 - val_loss: 35.4977 - val_mae: 34.8689 - learning_rate: 5.0000e-04\n",
      "Epoch 9/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 35.3018 - mae: 34.6642\n",
      "Epoch 9: val_mae did not improve from 34.66224\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 35.3032 - mae: 34.6656 - val_loss: 45.7382 - val_mae: 45.0613 - learning_rate: 5.0000e-04\n",
      "Epoch 10/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 35.5407 - mae: 34.8449\n",
      "Epoch 10: val_mae did not improve from 34.66224\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 35.5416 - mae: 34.8457 - val_loss: 36.8192 - val_mae: 36.0767 - learning_rate: 5.0000e-04\n",
      "Epoch 11/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 35.8541 - mae: 35.1013\n",
      "Epoch 11: val_mae improved from 34.66224 to 30.79208, saving model to model_fold2.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 55ms/step - loss: 35.8514 - mae: 35.0986 - val_loss: 31.5654 - val_mae: 30.7921 - learning_rate: 5.0000e-04\n",
      "Epoch 12/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 33.9938 - mae: 33.2189\n",
      "Epoch 12: val_mae did not improve from 30.79208\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 33.9950 - mae: 33.2200 - val_loss: 32.9000 - val_mae: 32.1114 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 34.1992 - mae: 33.4067\n",
      "Epoch 13: val_mae did not improve from 30.79208\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 34.1988 - mae: 33.4063 - val_loss: 49.9113 - val_mae: 49.0974 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 34.1455 - mae: 33.3263\n",
      "Epoch 14: val_mae did not improve from 30.79208\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 34.1460 - mae: 33.3268 - val_loss: 32.3196 - val_mae: 31.4796 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 34.0093 - mae: 33.1661\n",
      "Epoch 15: val_mae did not improve from 30.79208\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 34.0088 - mae: 33.1657 - val_loss: 71.2685 - val_mae: 70.4275 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 33.7918 - mae: 32.9496\n",
      "Epoch 16: val_mae did not improve from 30.79208\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 33.7903 - mae: 32.9481 - val_loss: 45.3176 - val_mae: 44.4660 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 32.9727 - mae: 32.1225\n",
      "Epoch 17: val_mae did not improve from 30.79208\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 32.9733 - mae: 32.1231 - val_loss: 35.0256 - val_mae: 34.1796 - learning_rate: 2.5000e-04\n",
      "Epoch 18/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 33.6851 - mae: 32.8331\n",
      "Epoch 18: val_mae improved from 30.79208 to 30.44388, saving model to model_fold2.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 33.6845 - mae: 32.8325 - val_loss: 31.2971 - val_mae: 30.4439 - learning_rate: 2.5000e-04\n",
      "Epoch 19/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 32.9363 - mae: 32.0854\n",
      "Epoch 19: val_mae improved from 30.44388 to 29.43322, saving model to model_fold2.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 32.9365 - mae: 32.0856 - val_loss: 30.2773 - val_mae: 29.4332 - learning_rate: 2.5000e-04\n",
      "Epoch 20/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 32.2127 - mae: 31.3709\n",
      "Epoch 20: val_mae did not improve from 29.43322\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 32.2137 - mae: 31.3719 - val_loss: 41.1967 - val_mae: 40.3626 - learning_rate: 2.5000e-04\n",
      "Epoch 21/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 32.4280 - mae: 31.5967\n",
      "Epoch 21: val_mae did not improve from 29.43322\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 32.4292 - mae: 31.5978 - val_loss: 101.6696 - val_mae: 100.8450 - learning_rate: 2.5000e-04\n",
      "Epoch 22/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 32.7598 - mae: 31.9349\n",
      "Epoch 22: val_mae did not improve from 29.43322\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 32.7595 - mae: 31.9346 - val_loss: 33.9999 - val_mae: 33.1779 - learning_rate: 2.5000e-04\n",
      "Epoch 23/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 32.0565 - mae: 31.2328\n",
      "Epoch 23: val_mae did not improve from 29.43322\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 32.0567 - mae: 31.2330 - val_loss: 31.8783 - val_mae: 31.0554 - learning_rate: 2.5000e-04\n",
      "Epoch 24/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 32.6154 - mae: 31.7926\n",
      "Epoch 24: val_mae did not improve from 29.43322\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 32.6142 - mae: 31.7913 - val_loss: 31.3633 - val_mae: 30.5475 - learning_rate: 2.5000e-04\n",
      "Epoch 25/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 32.3109 - mae: 31.4967\n",
      "Epoch 25: val_mae did not improve from 29.43322\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 32.3095 - mae: 31.4953 - val_loss: 30.7096 - val_mae: 29.9006 - learning_rate: 1.2500e-04\n",
      "Epoch 26/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 31.3508 - mae: 30.5437\n",
      "Epoch 26: val_mae improved from 29.43322 to 28.33471, saving model to model_fold2.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 31.3521 - mae: 30.5450 - val_loss: 29.1363 - val_mae: 28.3347 - learning_rate: 1.2500e-04\n",
      "Epoch 27/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 31.6375 - mae: 30.8375\n",
      "Epoch 27: val_mae did not improve from 28.33471\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 31.6382 - mae: 30.8382 - val_loss: 32.6541 - val_mae: 31.8592 - learning_rate: 1.2500e-04\n",
      "Epoch 28/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 31.3995 - mae: 30.6057\n",
      "Epoch 28: val_mae did not improve from 28.33471\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 31.4000 - mae: 30.6063 - val_loss: 33.0519 - val_mae: 32.2631 - learning_rate: 1.2500e-04\n",
      "Epoch 29/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 31.3573 - mae: 30.5697\n",
      "Epoch 29: val_mae did not improve from 28.33471\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 31.3575 - mae: 30.5700 - val_loss: 32.3960 - val_mae: 31.6120 - learning_rate: 1.2500e-04\n",
      "Epoch 30/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 31.5369 - mae: 30.7543\n",
      "Epoch 30: val_mae did not improve from 28.33471\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 31.5359 - mae: 30.7533 - val_loss: 30.4542 - val_mae: 29.6772 - learning_rate: 1.2500e-04\n",
      "Epoch 31/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 31.4217 - mae: 30.6455\n",
      "Epoch 31: val_mae did not improve from 28.33471\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 31.4205 - mae: 30.6443 - val_loss: 32.1362 - val_mae: 31.3646 - learning_rate: 1.2500e-04\n",
      "Epoch 32/50\n",
      "\u001B[1m300/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 30.9510 - mae: 30.1802\n",
      "Epoch 32: val_mae did not improve from 28.33471\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 30.9506 - mae: 30.1798 - val_loss: 29.5087 - val_mae: 28.7402 - learning_rate: 6.2500e-05\n",
      "Epoch 33/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 30.3914 - mae: 29.6238\n",
      "Epoch 33: val_mae did not improve from 28.33471\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 30.3908 - mae: 29.6232 - val_loss: 33.8823 - val_mae: 33.1178 - learning_rate: 6.2500e-05\n",
      "Epoch 34/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 30.6724 - mae: 29.9089\n",
      "Epoch 34: val_mae improved from 28.33471 to 26.84322, saving model to model_fold2.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 30.6724 - mae: 29.9089 - val_loss: 27.6038 - val_mae: 26.8432 - learning_rate: 6.2500e-05\n",
      "Epoch 35/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 30.4439 - mae: 29.6844\n",
      "Epoch 35: val_mae improved from 26.84322 to 26.52155, saving model to model_fold2.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 30.4435 - mae: 29.6840 - val_loss: 27.2775 - val_mae: 26.5216 - learning_rate: 6.2500e-05\n",
      "Epoch 36/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 29.6551 - mae: 28.9006\n",
      "Epoch 36: val_mae did not improve from 26.52155\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 29.6558 - mae: 28.9014 - val_loss: 28.4472 - val_mae: 27.6966 - learning_rate: 6.2500e-05\n",
      "Epoch 37/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 30.6792 - mae: 29.9297\n",
      "Epoch 37: val_mae did not improve from 26.52155\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 30.6790 - mae: 29.9295 - val_loss: 30.0470 - val_mae: 29.3010 - learning_rate: 6.2500e-05\n",
      "Epoch 38/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 30.7014 - mae: 29.9564\n",
      "Epoch 38: val_mae did not improve from 26.52155\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 30.7002 - mae: 29.9553 - val_loss: 27.3202 - val_mae: 26.5784 - learning_rate: 6.2500e-05\n",
      "Epoch 39/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 29.7750 - mae: 29.0342\n",
      "Epoch 39: val_mae improved from 26.52155 to 26.24622, saving model to model_fold2.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 29.7755 - mae: 29.0347 - val_loss: 26.9840 - val_mae: 26.2462 - learning_rate: 6.2500e-05\n",
      "Epoch 40/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 30.0472 - mae: 29.3105\n",
      "Epoch 40: val_mae improved from 26.24622 to 25.86551, saving model to model_fold2.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 30.0465 - mae: 29.3098 - val_loss: 26.5992 - val_mae: 25.8655 - learning_rate: 6.2500e-05\n",
      "Epoch 41/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 30.0388 - mae: 29.3063\n",
      "Epoch 41: val_mae did not improve from 25.86551\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 30.0372 - mae: 29.3047 - val_loss: 26.8391 - val_mae: 26.1100 - learning_rate: 6.2500e-05\n",
      "Epoch 42/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 30.0429 - mae: 29.3149\n",
      "Epoch 42: val_mae did not improve from 25.86551\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 30.0426 - mae: 29.3145 - val_loss: 33.1156 - val_mae: 32.3897 - learning_rate: 6.2500e-05\n",
      "Epoch 43/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 29.0108 - mae: 28.2858\n",
      "Epoch 43: val_mae did not improve from 25.86551\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 29.0107 - mae: 28.2857 - val_loss: 27.0303 - val_mae: 26.3083 - learning_rate: 6.2500e-05\n",
      "Epoch 44/50\n",
      "\u001B[1m300/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 28.9522 - mae: 28.2312\n",
      "Epoch 44: val_mae did not improve from 25.86551\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 28.9526 - mae: 28.2316 - val_loss: 27.5679 - val_mae: 26.8499 - learning_rate: 6.2500e-05\n",
      "Epoch 45/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 28.4574 - mae: 27.7403\n",
      "Epoch 45: val_mae did not improve from 25.86551\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 28.4586 - mae: 27.7414 - val_loss: 29.8987 - val_mae: 29.1842 - learning_rate: 6.2500e-05\n",
      "Epoch 46/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 28.5418 - mae: 27.8279\n",
      "Epoch 46: val_mae improved from 25.86551 to 25.05817, saving model to model_fold2.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 28.5419 - mae: 27.8280 - val_loss: 25.7704 - val_mae: 25.0582 - learning_rate: 3.1250e-05\n",
      "Epoch 47/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 28.6041 - mae: 27.8924\n",
      "Epoch 47: val_mae improved from 25.05817 to 24.86152, saving model to model_fold2.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 28.6036 - mae: 27.8918 - val_loss: 25.5716 - val_mae: 24.8615 - learning_rate: 3.1250e-05\n",
      "Epoch 48/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 28.7372 - mae: 28.0276\n",
      "Epoch 48: val_mae did not improve from 24.86152\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 28.7363 - mae: 28.0268 - val_loss: 27.8248 - val_mae: 27.1165 - learning_rate: 3.1250e-05\n",
      "Epoch 49/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 28.5678 - mae: 27.8600\n",
      "Epoch 49: val_mae did not improve from 24.86152\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 28.5666 - mae: 27.8587 - val_loss: 25.9962 - val_mae: 25.2898 - learning_rate: 3.1250e-05\n",
      "Epoch 50/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 28.0651 - mae: 27.3592\n",
      "Epoch 50: val_mae improved from 24.86152 to 24.67362, saving model to model_fold2.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 28.0642 - mae: 27.3583 - val_loss: 25.3778 - val_mae: 24.6736 - learning_rate: 3.1250e-05\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "\n",
      ">>> Fold 3/5\n",
      "Epoch 1/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 119.7035 - mae: 119.4141\n",
      "Epoch 1: val_mae improved from inf to 126.25094, saving model to model_fold3.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m48s\u001B[0m 63ms/step - loss: 119.6850 - mae: 119.3956 - val_loss: 126.5499 - val_mae: 126.2509 - learning_rate: 5.0000e-04\n",
      "Epoch 2/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 98.9080 - mae: 98.6016\n",
      "Epoch 2: val_mae improved from 126.25094 to 61.93256, saving model to model_fold3.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 98.8872 - mae: 98.5807 - val_loss: 62.2591 - val_mae: 61.9326 - learning_rate: 5.0000e-04\n",
      "Epoch 3/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step - loss: 74.3747 - mae: 74.0432\n",
      "Epoch 3: val_mae improved from 61.93256 to 54.75406, saving model to model_fold3.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 57ms/step - loss: 74.3579 - mae: 74.0264 - val_loss: 55.0974 - val_mae: 54.7541 - learning_rate: 5.0000e-04\n",
      "Epoch 4/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step - loss: 57.5126 - mae: 57.1668\n",
      "Epoch 4: val_mae improved from 54.75406 to 41.24221, saving model to model_fold3.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 57.5027 - mae: 57.1568 - val_loss: 41.5992 - val_mae: 41.2422 - learning_rate: 5.0000e-04\n",
      "Epoch 5/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 46.6510 - mae: 46.2879\n",
      "Epoch 5: val_mae improved from 41.24221 to 40.55831, saving model to model_fold3.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 46.6469 - mae: 46.2838 - val_loss: 40.9330 - val_mae: 40.5583 - learning_rate: 5.0000e-04\n",
      "Epoch 6/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 42.8437 - mae: 42.4659\n",
      "Epoch 6: val_mae did not improve from 40.55831\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 42.8401 - mae: 42.4622 - val_loss: 61.3103 - val_mae: 60.9183 - learning_rate: 5.0000e-04\n",
      "Epoch 7/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 39.0453 - mae: 38.6446\n",
      "Epoch 7: val_mae improved from 40.55831 to 40.08315, saving model to model_fold3.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 39.0447 - mae: 38.6441 - val_loss: 40.5017 - val_mae: 40.0831 - learning_rate: 5.0000e-04\n",
      "Epoch 8/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 36.9289 - mae: 36.4848\n",
      "Epoch 8: val_mae did not improve from 40.08315\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 36.9283 - mae: 36.4842 - val_loss: 56.9489 - val_mae: 56.4776 - learning_rate: 5.0000e-04\n",
      "Epoch 9/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 40.2258 - mae: 39.7485\n",
      "Epoch 9: val_mae improved from 40.08315 to 34.14969, saving model to model_fold3.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 40.2246 - mae: 39.7473 - val_loss: 34.6464 - val_mae: 34.1497 - learning_rate: 5.0000e-04\n",
      "Epoch 10/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 39.3875 - mae: 38.8685\n",
      "Epoch 10: val_mae did not improve from 34.14969\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 39.3854 - mae: 38.8662 - val_loss: 35.5059 - val_mae: 34.8304 - learning_rate: 5.0000e-04\n",
      "Epoch 11/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 35.6767 - mae: 34.9880\n",
      "Epoch 11: val_mae did not improve from 34.14969\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 35.6822 - mae: 34.9934 - val_loss: 35.8615 - val_mae: 35.1010 - learning_rate: 5.0000e-04\n",
      "Epoch 12/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 39.0761 - mae: 38.2898\n",
      "Epoch 12: val_mae did not improve from 34.14969\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 39.0755 - mae: 38.2891 - val_loss: 37.1767 - val_mae: 36.3562 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 38.1828 - mae: 37.3534\n",
      "Epoch 13: val_mae improved from 34.14969 to 33.38327, saving model to model_fold3.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 38.1817 - mae: 37.3522 - val_loss: 34.2394 - val_mae: 33.3833 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 35.3786 - mae: 34.5159\n",
      "Epoch 14: val_mae improved from 33.38327 to 31.59493, saving model to model_fold3.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 35.3783 - mae: 34.5156 - val_loss: 32.4691 - val_mae: 31.5949 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 33.7637 - mae: 32.8883\n",
      "Epoch 15: val_mae did not improve from 31.59493\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 33.7633 - mae: 32.8879 - val_loss: 40.3181 - val_mae: 39.4373 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 31.7638 - mae: 30.8802\n",
      "Epoch 16: val_mae did not improve from 31.59493\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 31.7616 - mae: 30.8780 - val_loss: 35.9225 - val_mae: 35.0357 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 29.0885 - mae: 28.2019\n",
      "Epoch 17: val_mae did not improve from 31.59493\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 29.0874 - mae: 28.2008 - val_loss: 93.7522 - val_mae: 92.8623 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 30.8037 - mae: 29.9075\n",
      "Epoch 18: val_mae did not improve from 31.59493\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 30.8002 - mae: 29.9040 - val_loss: 51.5464 - val_mae: 50.6445 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 27.3492 - mae: 26.4494\n",
      "Epoch 19: val_mae improved from 31.59493 to 24.69197, saving model to model_fold3.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 27.3470 - mae: 26.4473 - val_loss: 25.5830 - val_mae: 24.6920 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 25.4330 - mae: 24.5458\n",
      "Epoch 20: val_mae improved from 24.69197 to 24.18442, saving model to model_fold3.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 25.4330 - mae: 24.5459 - val_loss: 25.0640 - val_mae: 24.1844 - learning_rate: 5.0000e-04\n",
      "Epoch 21/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 25.3347 - mae: 24.4581\n",
      "Epoch 21: val_mae did not improve from 24.18442\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 54ms/step - loss: 25.3346 - mae: 24.4581 - val_loss: 43.2206 - val_mae: 42.3473 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 25.0442 - mae: 24.1758\n",
      "Epoch 22: val_mae did not improve from 24.18442\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 25.0428 - mae: 24.1744 - val_loss: 30.2592 - val_mae: 29.4036 - learning_rate: 5.0000e-04\n",
      "Epoch 23/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 24.9493 - mae: 24.0878\n",
      "Epoch 23: val_mae did not improve from 24.18442\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 24.9479 - mae: 24.0863 - val_loss: 68.5211 - val_mae: 67.6587 - learning_rate: 5.0000e-04\n",
      "Epoch 24/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 26.6488 - mae: 25.7816\n",
      "Epoch 24: val_mae did not improve from 24.18442\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m20s\u001B[0m 68ms/step - loss: 26.6442 - mae: 25.7771 - val_loss: 33.2652 - val_mae: 32.4014 - learning_rate: 5.0000e-04\n",
      "Epoch 25/50\n",
      "\u001B[1m300/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 23.7813 - mae: 22.9143\n",
      "Epoch 25: val_mae did not improve from 24.18442\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 23.7816 - mae: 22.9146 - val_loss: 60.2827 - val_mae: 59.4222 - learning_rate: 5.0000e-04\n",
      "Epoch 26/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 23.5709 - mae: 22.7144\n",
      "Epoch 26: val_mae improved from 24.18442 to 22.30128, saving model to model_fold3.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 23.5698 - mae: 22.7133 - val_loss: 23.1457 - val_mae: 22.3013 - learning_rate: 2.5000e-04\n",
      "Epoch 27/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 22.9922 - mae: 22.1520\n",
      "Epoch 27: val_mae did not improve from 22.30128\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 22.9915 - mae: 22.1513 - val_loss: 68.4914 - val_mae: 67.6630 - learning_rate: 2.5000e-04\n",
      "Epoch 28/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 22.3443 - mae: 21.5193\n",
      "Epoch 28: val_mae improved from 22.30128 to 22.25443, saving model to model_fold3.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 22.3439 - mae: 21.5190 - val_loss: 23.0681 - val_mae: 22.2544 - learning_rate: 2.5000e-04\n",
      "Epoch 29/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 22.7849 - mae: 21.9747\n",
      "Epoch 29: val_mae did not improve from 22.25443\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 22.7842 - mae: 21.9740 - val_loss: 26.7154 - val_mae: 25.9170 - learning_rate: 2.5000e-04\n",
      "Epoch 30/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 21.8275 - mae: 21.0324\n",
      "Epoch 30: val_mae did not improve from 22.25443\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 21.8280 - mae: 21.0329 - val_loss: 23.8415 - val_mae: 23.0551 - learning_rate: 2.5000e-04\n",
      "Epoch 31/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 21.9181 - mae: 21.1345\n",
      "Epoch 31: val_mae did not improve from 22.25443\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 21.9180 - mae: 21.1344 - val_loss: 31.7538 - val_mae: 30.9792 - learning_rate: 2.5000e-04\n",
      "Epoch 32/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 21.4027 - mae: 20.6309\n",
      "Epoch 32: val_mae did not improve from 22.25443\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 21.4024 - mae: 20.6306 - val_loss: 77.9389 - val_mae: 77.1746 - learning_rate: 2.5000e-04\n",
      "Epoch 33/50\n",
      "\u001B[1m300/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 21.0920 - mae: 20.3301\n",
      "Epoch 33: val_mae did not improve from 22.25443\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 21.0921 - mae: 20.3303 - val_loss: 39.8403 - val_mae: 39.0864 - learning_rate: 2.5000e-04\n",
      "Epoch 34/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 20.9654 - mae: 20.2134\n",
      "Epoch 34: val_mae improved from 22.25443 to 19.05515, saving model to model_fold3.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 20.9646 - mae: 20.2126 - val_loss: 19.8011 - val_mae: 19.0552 - learning_rate: 1.2500e-04\n",
      "Epoch 35/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 20.4856 - mae: 19.7416\n",
      "Epoch 35: val_mae improved from 19.05515 to 18.01890, saving model to model_fold3.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 20.4848 - mae: 19.7408 - val_loss: 18.7562 - val_mae: 18.0189 - learning_rate: 1.2500e-04\n",
      "Epoch 36/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 20.4093 - mae: 19.6730\n",
      "Epoch 36: val_mae did not improve from 18.01890\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 20.4091 - mae: 19.6728 - val_loss: 20.1112 - val_mae: 19.3794 - learning_rate: 1.2500e-04\n",
      "Epoch 37/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 19.9328 - mae: 19.2025\n",
      "Epoch 37: val_mae did not improve from 18.01890\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 19.9330 - mae: 19.2027 - val_loss: 34.2119 - val_mae: 33.4869 - learning_rate: 1.2500e-04\n",
      "Epoch 38/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 20.2110 - mae: 19.4878\n",
      "Epoch 38: val_mae improved from 18.01890 to 17.90180, saving model to model_fold3.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 20.2105 - mae: 19.4873 - val_loss: 18.6197 - val_mae: 17.9018 - learning_rate: 1.2500e-04\n",
      "Epoch 39/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 20.1985 - mae: 19.4814\n",
      "Epoch 39: val_mae did not improve from 17.90180\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 20.1987 - mae: 19.4816 - val_loss: 26.9417 - val_mae: 26.2310 - learning_rate: 1.2500e-04\n",
      "Epoch 40/50\n",
      "\u001B[1m300/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 19.3902 - mae: 18.6810\n",
      "Epoch 40: val_mae did not improve from 17.90180\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 19.3908 - mae: 18.6817 - val_loss: 29.6924 - val_mae: 28.9890 - learning_rate: 1.2500e-04\n",
      "Epoch 41/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 19.4262 - mae: 18.7245\n",
      "Epoch 41: val_mae did not improve from 17.90180\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 19.4267 - mae: 18.7250 - val_loss: 40.9591 - val_mae: 40.2620 - learning_rate: 1.2500e-04\n",
      "Epoch 42/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 19.4523 - mae: 18.7570\n",
      "Epoch 42: val_mae did not improve from 17.90180\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 19.4524 - mae: 18.7571 - val_loss: 25.6665 - val_mae: 24.9760 - learning_rate: 1.2500e-04\n",
      "Epoch 43/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 19.3832 - mae: 18.6943\n",
      "Epoch 43: val_mae improved from 17.90180 to 17.77038, saving model to model_fold3.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 57ms/step - loss: 19.3839 - mae: 18.6950 - val_loss: 18.4568 - val_mae: 17.7704 - learning_rate: 1.2500e-04\n",
      "Epoch 44/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 19.0863 - mae: 18.4009\n",
      "Epoch 44: val_mae did not improve from 17.77038\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 19.0867 - mae: 18.4013 - val_loss: 19.5841 - val_mae: 18.9018 - learning_rate: 1.2500e-04\n",
      "Epoch 45/50\n",
      "\u001B[1m300/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 19.4527 - mae: 18.7721\n",
      "Epoch 45: val_mae did not improve from 17.77038\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 19.4522 - mae: 18.7715 - val_loss: 21.8625 - val_mae: 21.1869 - learning_rate: 1.2500e-04\n",
      "Epoch 46/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 19.2559 - mae: 18.5816\n",
      "Epoch 46: val_mae improved from 17.77038 to 16.76178, saving model to model_fold3.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 19.2554 - mae: 18.5811 - val_loss: 17.4317 - val_mae: 16.7618 - learning_rate: 1.2500e-04\n",
      "Epoch 47/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 19.2777 - mae: 18.6090\n",
      "Epoch 47: val_mae did not improve from 16.76178\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 19.2767 - mae: 18.6081 - val_loss: 24.3928 - val_mae: 23.7280 - learning_rate: 1.2500e-04\n",
      "Epoch 48/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 18.8665 - mae: 18.2032\n",
      "Epoch 48: val_mae did not improve from 16.76178\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 18.8667 - mae: 18.2035 - val_loss: 25.6047 - val_mae: 24.9450 - learning_rate: 1.2500e-04\n",
      "Epoch 49/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 18.7948 - mae: 18.1366\n",
      "Epoch 49: val_mae did not improve from 16.76178\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 18.7952 - mae: 18.1371 - val_loss: 25.9133 - val_mae: 25.2589 - learning_rate: 1.2500e-04\n",
      "Epoch 50/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 18.7529 - mae: 18.0996\n",
      "Epoch 50: val_mae did not improve from 16.76178\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m20s\u001B[0m 68ms/step - loss: 18.7532 - mae: 18.1000 - val_loss: 21.3610 - val_mae: 20.7109 - learning_rate: 1.2500e-04\n",
      "Restoring model weights from the end of the best epoch: 46.\n",
      "\n",
      ">>> Fold 4/5\n",
      "Epoch 1/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 119.1466 - mae: 118.8595\n",
      "Epoch 1: val_mae improved from inf to 77.24465, saving model to model_fold4.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m47s\u001B[0m 62ms/step - loss: 119.1307 - mae: 118.8435 - val_loss: 77.5335 - val_mae: 77.2447 - learning_rate: 5.0000e-04\n",
      "Epoch 2/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 98.5084 - mae: 98.2123\n",
      "Epoch 2: val_mae improved from 77.24465 to 76.95058, saving model to model_fold4.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 98.4895 - mae: 98.1934 - val_loss: 77.2652 - val_mae: 76.9506 - learning_rate: 5.0000e-04\n",
      "Epoch 3/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step - loss: 76.2119 - mae: 75.8817\n",
      "Epoch 3: val_mae improved from 76.95058 to 58.78055, saving model to model_fold4.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m20s\u001B[0m 68ms/step - loss: 76.1941 - mae: 75.8638 - val_loss: 59.1614 - val_mae: 58.7806 - learning_rate: 5.0000e-04\n",
      "Epoch 4/50\n",
      "\u001B[1m300/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 58.7546 - mae: 58.3658\n",
      "Epoch 4: val_mae did not improve from 58.78055\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 58.7386 - mae: 58.3497 - val_loss: 73.6813 - val_mae: 73.2247 - learning_rate: 5.0000e-04\n",
      "Epoch 5/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 49.9520 - mae: 49.4732\n",
      "Epoch 5: val_mae did not improve from 58.78055\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 49.9480 - mae: 49.4691 - val_loss: 67.8855 - val_mae: 67.3036 - learning_rate: 5.0000e-04\n",
      "Epoch 6/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 44.9295 - mae: 44.3228\n",
      "Epoch 6: val_mae improved from 58.78055 to 52.23931, saving model to model_fold4.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 44.9264 - mae: 44.3196 - val_loss: 52.9142 - val_mae: 52.2393 - learning_rate: 5.0000e-04\n",
      "Epoch 7/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 40.1507 - mae: 39.4630\n",
      "Epoch 7: val_mae improved from 52.23931 to 30.73193, saving model to model_fold4.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 40.1462 - mae: 39.4585 - val_loss: 31.4728 - val_mae: 30.7319 - learning_rate: 5.0000e-04\n",
      "Epoch 8/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 35.9861 - mae: 35.2294\n",
      "Epoch 8: val_mae did not improve from 30.73193\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 35.9879 - mae: 35.2311 - val_loss: 35.5186 - val_mae: 34.7034 - learning_rate: 5.0000e-04\n",
      "Epoch 9/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 36.3993 - mae: 35.5733\n",
      "Epoch 9: val_mae did not improve from 30.73193\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 36.3979 - mae: 35.5719 - val_loss: 58.1990 - val_mae: 57.3430 - learning_rate: 5.0000e-04\n",
      "Epoch 10/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 34.6739 - mae: 33.8114\n",
      "Epoch 10: val_mae improved from 30.73193 to 30.22432, saving model to model_fold4.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 34.6725 - mae: 33.8100 - val_loss: 31.1035 - val_mae: 30.2243 - learning_rate: 5.0000e-04\n",
      "Epoch 11/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 32.3747 - mae: 31.4913\n",
      "Epoch 11: val_mae improved from 30.22432 to 29.22299, saving model to model_fold4.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 32.3724 - mae: 31.4890 - val_loss: 30.1191 - val_mae: 29.2230 - learning_rate: 5.0000e-04\n",
      "Epoch 12/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 30.2483 - mae: 29.3496\n",
      "Epoch 12: val_mae improved from 29.22299 to 27.82823, saving model to model_fold4.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 30.2490 - mae: 29.3503 - val_loss: 28.7264 - val_mae: 27.8282 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 29.6274 - mae: 28.7308\n",
      "Epoch 13: val_mae did not improve from 27.82823\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 29.6278 - mae: 28.7313 - val_loss: 28.7238 - val_mae: 27.8333 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 29.5312 - mae: 28.6428\n",
      "Epoch 14: val_mae did not improve from 27.82823\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 29.5309 - mae: 28.6425 - val_loss: 29.0120 - val_mae: 28.1308 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001B[1m300/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 28.8697 - mae: 27.9878\n",
      "Epoch 15: val_mae did not improve from 27.82823\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 28.8716 - mae: 27.9897 - val_loss: 29.5331 - val_mae: 28.6533 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 28.5815 - mae: 27.7032\n",
      "Epoch 16: val_mae improved from 27.82823 to 26.33534, saving model to model_fold4.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m22s\u001B[0m 72ms/step - loss: 28.5820 - mae: 27.7038 - val_loss: 27.2110 - val_mae: 26.3353 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 28.5183 - mae: 27.6430\n",
      "Epoch 17: val_mae did not improve from 26.33534\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 28.5185 - mae: 27.6432 - val_loss: 36.1989 - val_mae: 35.3250 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 28.0309 - mae: 27.1567\n",
      "Epoch 18: val_mae did not improve from 26.33534\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 28.0315 - mae: 27.1573 - val_loss: 28.8576 - val_mae: 27.9771 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 27.2049 - mae: 26.3237\n",
      "Epoch 19: val_mae improved from 26.33534 to 25.54146, saving model to model_fold4.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 27.2056 - mae: 26.3245 - val_loss: 26.4274 - val_mae: 25.5415 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 27.7043 - mae: 26.8151\n",
      "Epoch 20: val_mae did not improve from 25.54146\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 27.7036 - mae: 26.8143 - val_loss: 34.6383 - val_mae: 33.7278 - learning_rate: 5.0000e-04\n",
      "Epoch 21/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 26.6345 - mae: 25.7221\n",
      "Epoch 21: val_mae did not improve from 25.54146\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 26.6348 - mae: 25.7224 - val_loss: 34.6068 - val_mae: 33.6821 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 26.3703 - mae: 25.4431\n",
      "Epoch 22: val_mae did not improve from 25.54146\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 26.3697 - mae: 25.4425 - val_loss: 26.9645 - val_mae: 26.0251 - learning_rate: 5.0000e-04\n",
      "Epoch 23/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 25.5419 - mae: 24.5989\n",
      "Epoch 23: val_mae did not improve from 25.54146\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 25.5445 - mae: 24.6015 - val_loss: 34.8390 - val_mae: 33.8414 - learning_rate: 5.0000e-04\n",
      "Epoch 24/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 27.7544 - mae: 26.7557\n",
      "Epoch 24: val_mae did not improve from 25.54146\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 27.7514 - mae: 26.7527 - val_loss: 34.0631 - val_mae: 33.0607 - learning_rate: 5.0000e-04\n",
      "Epoch 25/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 24.6243 - mae: 23.6231\n",
      "Epoch 25: val_mae did not improve from 25.54146\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m20s\u001B[0m 68ms/step - loss: 24.6239 - mae: 23.6227 - val_loss: 32.5110 - val_mae: 31.5134 - learning_rate: 2.5000e-04\n",
      "Epoch 26/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 23.5701 - mae: 22.5745\n",
      "Epoch 26: val_mae did not improve from 25.54146\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 23.5703 - mae: 22.5747 - val_loss: 28.2519 - val_mae: 27.2621 - learning_rate: 2.5000e-04\n",
      "Epoch 27/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 23.3492 - mae: 22.3615\n",
      "Epoch 27: val_mae did not improve from 25.54146\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 23.3495 - mae: 22.3618 - val_loss: 30.3252 - val_mae: 29.3398 - learning_rate: 2.5000e-04\n",
      "Epoch 28/50\n",
      "\u001B[1m300/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 22.9596 - mae: 21.9757\n",
      "Epoch 28: val_mae did not improve from 25.54146\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 22.9593 - mae: 21.9755 - val_loss: 97.7439 - val_mae: 96.7648 - learning_rate: 2.5000e-04\n",
      "Epoch 29/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 23.0094 - mae: 22.0320\n",
      "Epoch 29: val_mae did not improve from 25.54146\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 23.0091 - mae: 22.0317 - val_loss: 33.4291 - val_mae: 32.4563 - learning_rate: 2.5000e-04\n",
      "Epoch 29: early stopping\n",
      "Restoring model weights from the end of the best epoch: 19.\n",
      "\n",
      ">>> Fold 5/5\n",
      "Epoch 1/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 119.4870 - mae: 119.1969\n",
      "Epoch 1: val_mae improved from inf to 117.36304, saving model to model_fold5.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m48s\u001B[0m 61ms/step - loss: 119.4703 - mae: 119.1802 - val_loss: 117.6586 - val_mae: 117.3630 - learning_rate: 5.0000e-04\n",
      "Epoch 2/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 98.2912 - mae: 97.9926\n",
      "Epoch 2: val_mae improved from 117.36304 to 57.70887, saving model to model_fold5.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 98.2735 - mae: 97.9750 - val_loss: 58.0285 - val_mae: 57.7089 - learning_rate: 5.0000e-04\n",
      "Epoch 3/50\n",
      "\u001B[1m300/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 44ms/step - loss: 75.7623 - mae: 75.4351\n",
      "Epoch 3: val_mae improved from 57.70887 to 51.34655, saving model to model_fold5.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 57ms/step - loss: 75.7278 - mae: 75.4006 - val_loss: 51.7052 - val_mae: 51.3466 - learning_rate: 5.0000e-04\n",
      "Epoch 4/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step - loss: 58.2299 - mae: 57.8569\n",
      "Epoch 4: val_mae did not improve from 51.34655\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 54ms/step - loss: 58.2185 - mae: 57.8455 - val_loss: 114.2252 - val_mae: 113.8088 - learning_rate: 5.0000e-04\n",
      "Epoch 5/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 49.7559 - mae: 49.3162\n",
      "Epoch 5: val_mae improved from 51.34655 to 49.87999, saving model to model_fold5.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 49.7505 - mae: 49.3107 - val_loss: 50.3757 - val_mae: 49.8800 - learning_rate: 5.0000e-04\n",
      "Epoch 6/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 43.8191 - mae: 43.3111\n",
      "Epoch 6: val_mae improved from 49.87999 to 43.77650, saving model to model_fold5.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 43.8196 - mae: 43.3115 - val_loss: 44.3497 - val_mae: 43.7765 - learning_rate: 5.0000e-04\n",
      "Epoch 7/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 42.6028 - mae: 42.0197\n",
      "Epoch 7: val_mae improved from 43.77650 to 37.09678, saving model to model_fold5.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 42.6002 - mae: 42.0171 - val_loss: 37.7097 - val_mae: 37.0968 - learning_rate: 5.0000e-04\n",
      "Epoch 8/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 39.0961 - mae: 38.4747\n",
      "Epoch 8: val_mae improved from 37.09678 to 33.25401, saving model to model_fold5.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 39.0968 - mae: 38.4754 - val_loss: 33.9029 - val_mae: 33.2540 - learning_rate: 5.0000e-04\n",
      "Epoch 9/50\n",
      "\u001B[1m300/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 37.0734 - mae: 36.4207\n",
      "Epoch 9: val_mae did not improve from 33.25401\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 37.0712 - mae: 36.4185 - val_loss: 35.0725 - val_mae: 34.4053 - learning_rate: 5.0000e-04\n",
      "Epoch 10/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 34.8387 - mae: 34.1658\n",
      "Epoch 10: val_mae did not improve from 33.25401\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 34.8393 - mae: 34.1663 - val_loss: 34.0180 - val_mae: 33.3498 - learning_rate: 5.0000e-04\n",
      "Epoch 11/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 34.4141 - mae: 33.7471\n",
      "Epoch 11: val_mae improved from 33.25401 to 32.38133, saving model to model_fold5.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 34.4136 - mae: 33.7466 - val_loss: 33.0410 - val_mae: 32.3813 - learning_rate: 5.0000e-04\n",
      "Epoch 12/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 33.3438 - mae: 32.6839\n",
      "Epoch 12: val_mae did not improve from 32.38133\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 33.3457 - mae: 32.6858 - val_loss: 46.0017 - val_mae: 45.3383 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 32.5974 - mae: 31.9354\n",
      "Epoch 13: val_mae did not improve from 32.38133\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 32.5996 - mae: 31.9376 - val_loss: 45.7035 - val_mae: 45.0347 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 32.5701 - mae: 31.8966\n",
      "Epoch 14: val_mae did not improve from 32.38133\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m20s\u001B[0m 68ms/step - loss: 32.5690 - mae: 31.8954 - val_loss: 43.4893 - val_mae: 42.8070 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 31.8285 - mae: 31.1465\n",
      "Epoch 15: val_mae improved from 32.38133 to 29.35118, saving model to model_fold5.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m21s\u001B[0m 71ms/step - loss: 31.8283 - mae: 31.1463 - val_loss: 30.0300 - val_mae: 29.3512 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 31.3386 - mae: 30.6594\n",
      "Epoch 16: val_mae improved from 29.35118 to 28.76507, saving model to model_fold5.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 31.3371 - mae: 30.6579 - val_loss: 29.4487 - val_mae: 28.7651 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 30.1572 - mae: 29.4696\n",
      "Epoch 17: val_mae did not improve from 28.76507\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 30.1565 - mae: 29.4689 - val_loss: 34.8725 - val_mae: 34.1741 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 29.8306 - mae: 29.1234\n",
      "Epoch 18: val_mae improved from 28.76507 to 27.45963, saving model to model_fold5.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 29.8292 - mae: 29.1220 - val_loss: 28.1804 - val_mae: 27.4596 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 29.1082 - mae: 28.3829\n",
      "Epoch 19: val_mae did not improve from 27.45963\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 29.1076 - mae: 28.3824 - val_loss: 33.3851 - val_mae: 32.6486 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 27.0942 - mae: 26.3551\n",
      "Epoch 20: val_mae did not improve from 27.45963\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 27.0952 - mae: 26.3561 - val_loss: 32.1331 - val_mae: 31.3662 - learning_rate: 5.0000e-04\n",
      "Epoch 21/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 26.7091 - mae: 25.9347\n",
      "Epoch 21: val_mae did not improve from 27.45963\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 26.7089 - mae: 25.9344 - val_loss: 100.2486 - val_mae: 99.4613 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 26.4580 - mae: 25.6679\n",
      "Epoch 22: val_mae did not improve from 27.45963\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 26.4556 - mae: 25.6655 - val_loss: 47.4903 - val_mae: 46.6924 - learning_rate: 5.0000e-04\n",
      "Epoch 23/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 24.9604 - mae: 24.1587\n",
      "Epoch 23: val_mae did not improve from 27.45963\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 24.9592 - mae: 24.1575 - val_loss: 30.4783 - val_mae: 29.6643 - learning_rate: 5.0000e-04\n",
      "Epoch 24/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 23.2664 - mae: 22.4525\n",
      "Epoch 24: val_mae improved from 27.45963 to 23.37170, saving model to model_fold5.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 23.2662 - mae: 22.4523 - val_loss: 24.1835 - val_mae: 23.3717 - learning_rate: 2.5000e-04\n",
      "Epoch 25/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 22.5803 - mae: 21.7699\n",
      "Epoch 25: val_mae did not improve from 23.37170\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 22.5808 - mae: 21.7704 - val_loss: 29.7352 - val_mae: 28.9268 - learning_rate: 2.5000e-04\n",
      "Epoch 26/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 22.1704 - mae: 21.3625\n",
      "Epoch 26: val_mae did not improve from 23.37170\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 22.1697 - mae: 21.3618 - val_loss: 64.3732 - val_mae: 63.5681 - learning_rate: 2.5000e-04\n",
      "Epoch 27/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 21.6509 - mae: 20.8471\n",
      "Epoch 27: val_mae did not improve from 23.37170\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 21.6519 - mae: 20.8481 - val_loss: 29.7005 - val_mae: 28.8967 - learning_rate: 2.5000e-04\n",
      "Epoch 28/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 21.8703 - mae: 21.0663\n",
      "Epoch 28: val_mae improved from 23.37170 to 20.77659, saving model to model_fold5.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 21.8695 - mae: 21.0655 - val_loss: 21.5794 - val_mae: 20.7766 - learning_rate: 2.5000e-04\n",
      "Epoch 29/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 21.6164 - mae: 20.8114\n",
      "Epoch 29: val_mae did not improve from 20.77659\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 21.6165 - mae: 20.8115 - val_loss: 35.0269 - val_mae: 34.2203 - learning_rate: 2.5000e-04\n",
      "Epoch 30/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 21.0395 - mae: 20.2325\n",
      "Epoch 30: val_mae improved from 20.77659 to 18.17049, saving model to model_fold5.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 21.0388 - mae: 20.2319 - val_loss: 18.9787 - val_mae: 18.1705 - learning_rate: 2.5000e-04\n",
      "Epoch 31/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 20.7900 - mae: 19.9825\n",
      "Epoch 31: val_mae did not improve from 18.17049\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 20.7897 - mae: 19.9822 - val_loss: 19.0639 - val_mae: 18.2595 - learning_rate: 2.5000e-04\n",
      "Epoch 32/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 20.2432 - mae: 19.4390\n",
      "Epoch 32: val_mae did not improve from 18.17049\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 20.2437 - mae: 19.4395 - val_loss: 21.3544 - val_mae: 20.5532 - learning_rate: 2.5000e-04\n",
      "Epoch 33/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 20.2391 - mae: 19.4377\n",
      "Epoch 33: val_mae did not improve from 18.17049\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 20.2390 - mae: 19.4376 - val_loss: 21.7117 - val_mae: 20.9105 - learning_rate: 2.5000e-04\n",
      "Epoch 34/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 19.5352 - mae: 18.7342\n",
      "Epoch 34: val_mae did not improve from 18.17049\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 19.5375 - mae: 18.7365 - val_loss: 20.8544 - val_mae: 20.0494 - learning_rate: 2.5000e-04\n",
      "Epoch 35/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 20.0838 - mae: 19.2799\n",
      "Epoch 35: val_mae did not improve from 18.17049\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 20.0832 - mae: 19.2794 - val_loss: 26.5235 - val_mae: 25.7216 - learning_rate: 2.5000e-04\n",
      "Epoch 36/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 19.5151 - mae: 18.7142\n",
      "Epoch 36: val_mae improved from 18.17049 to 16.78478, saving model to model_fold5.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 19.5141 - mae: 18.7132 - val_loss: 17.5836 - val_mae: 16.7848 - learning_rate: 1.2500e-04\n",
      "Epoch 37/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 18.8346 - mae: 18.0367\n",
      "Epoch 37: val_mae did not improve from 16.78478\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 18.8350 - mae: 18.0372 - val_loss: 18.3981 - val_mae: 17.6035 - learning_rate: 1.2500e-04\n",
      "Epoch 38/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 18.4316 - mae: 17.6379\n",
      "Epoch 38: val_mae did not improve from 16.78478\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 18.4319 - mae: 17.6382 - val_loss: 17.9433 - val_mae: 17.1524 - learning_rate: 1.2500e-04\n",
      "Epoch 39/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 18.5245 - mae: 17.7345\n",
      "Epoch 39: val_mae did not improve from 16.78478\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 18.5247 - mae: 17.7348 - val_loss: 20.3914 - val_mae: 19.6031 - learning_rate: 1.2500e-04\n",
      "Epoch 40/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 18.3388 - mae: 17.5516\n",
      "Epoch 40: val_mae did not improve from 16.78478\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 18.3388 - mae: 17.5517 - val_loss: 22.3321 - val_mae: 21.5481 - learning_rate: 1.2500e-04\n",
      "Epoch 41/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 18.2198 - mae: 17.4370\n",
      "Epoch 41: val_mae improved from 16.78478 to 15.70695, saving model to model_fold5.keras\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 56ms/step - loss: 18.2198 - mae: 17.4369 - val_loss: 16.4863 - val_mae: 15.7069 - learning_rate: 1.2500e-04\n",
      "Epoch 42/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 18.0383 - mae: 17.2599\n",
      "Epoch 42: val_mae did not improve from 15.70695\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 18.0383 - mae: 17.2599 - val_loss: 17.8740 - val_mae: 17.0988 - learning_rate: 1.2500e-04\n",
      "Epoch 43/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 18.1680 - mae: 17.3939\n",
      "Epoch 43: val_mae did not improve from 15.70695\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 18.1672 - mae: 17.3932 - val_loss: 18.3449 - val_mae: 17.5740 - learning_rate: 1.2500e-04\n",
      "Epoch 44/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 17.8034 - mae: 17.0334\n",
      "Epoch 44: val_mae did not improve from 15.70695\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 17.8036 - mae: 17.0336 - val_loss: 19.9006 - val_mae: 19.1331 - learning_rate: 1.2500e-04\n",
      "Epoch 45/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 17.4453 - mae: 16.6788\n",
      "Epoch 45: val_mae did not improve from 15.70695\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 17.4458 - mae: 16.6793 - val_loss: 16.7692 - val_mae: 16.0060 - learning_rate: 1.2500e-04\n",
      "Epoch 46/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 17.4898 - mae: 16.7275\n",
      "Epoch 46: val_mae did not improve from 15.70695\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 17.4897 - mae: 16.7274 - val_loss: 26.2052 - val_mae: 25.4461 - learning_rate: 1.2500e-04\n",
      "Epoch 47/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 17.6575 - mae: 16.8991\n",
      "Epoch 47: val_mae did not improve from 15.70695\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 17.6565 - mae: 16.8982 - val_loss: 17.4589 - val_mae: 16.7026 - learning_rate: 6.2500e-05\n",
      "Epoch 48/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 17.4985 - mae: 16.7429\n",
      "Epoch 48: val_mae did not improve from 15.70695\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 17.4982 - mae: 16.7426 - val_loss: 17.7254 - val_mae: 16.9720 - learning_rate: 6.2500e-05\n",
      "Epoch 49/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 17.1510 - mae: 16.3980\n",
      "Epoch 49: val_mae did not improve from 15.70695\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 17.1510 - mae: 16.3980 - val_loss: 18.4991 - val_mae: 17.7482 - learning_rate: 6.2500e-05\n",
      "Epoch 50/50\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 17.1633 - mae: 16.4131\n",
      "Epoch 50: val_mae did not improve from 15.70695\n",
      "\u001B[1m301/301\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 53ms/step - loss: 17.1631 - mae: 16.4129 - val_loss: 16.8046 - val_mae: 16.0564 - learning_rate: 6.2500e-05\n",
      "Restoring model weights from the end of the best epoch: 41.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# after CV, average your metric, e.g.:\n",
    "val_maes = np.array([h[\"val_mae\"][-1] for h in all_hist])\n",
    "print(\"Per-fold final val MAE:\", val_maes)\n",
    "print(\"CV mean val MAE:    \", val_maes.mean())\n",
    "print(\"CV std  val MAE:    \", val_maes.std())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fuBrrDFFOyLC",
    "outputId": "a6fba050-73a9-44e9-fc31-27721f58e934"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Per-fold final val MAE: [17.58379745 24.67361832 20.71093178 32.45626068 16.05643463]\n",
      "CV mean val MAE:     22.296208572387695\n",
      "CV std  val MAE:     5.874239284129035\n"
     ]
    }
   ]
  }
 ]
}
