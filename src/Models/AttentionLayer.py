from keras import layers
import tensorflow as tf


class SpatialAttention(layers.Layer):
    """
    A custom Keras layer implementing a more attentive spatial attention mechanism.
    It now includes an intermediate convolutional layer to learn richer attention features.
    """

    def __init__(self, kernel_size=7, **kwargs):
        super(SpatialAttention, self).__init__(**kwargs)
        self.kernel_size = kernel_size

        # Intermediate conv layer to learn more complex features from concatenated avg/max pools
        # More filters (e.g., 8 or 16) for more capacity, smaller kernel for local patterns.
        self.intermediate_conv = layers.Conv2D(
            filters=32,  # Increased filters for more "intelligence"
            kernel_size=5,  # Small kernel for local feature extraction within attention
            padding='same',
            activation='relu',  # Added ReLU activation for non-linearity
            name='attention_intermediate_conv'
        )
        # Final conv layer to produce the single-channel attention map
        self.final_conv = layers.Conv2D(
            filters=1,
            kernel_size=self.kernel_size,  # Use the initial kernel_size for the final aggregation
            padding='same',
            activation='sigmoid',  # Sigmoid to output values between 0 and 1
            use_bias=False,
            name='attention_final_conv'
        )

    def build(self, input_shape):
        # Keras will automatically build internal layers like self.intermediate_conv and self.final_conv
        super(SpatialAttention, self).build(input_shape)

    def call(self, inputs):
        """
        Applies spatial attention to the input feature map.
        The attention map is now generated by a slightly deeper sub-network.
        """
        # Compute average and max pool along the channel axis
        avg_pool = tf.reduce_mean(inputs, axis=-1, keepdims=True)
        max_pool = tf.reduce_max(inputs, axis=-1, keepdims=True)
        concat = layers.concatenate([avg_pool, max_pool], axis=-1, name='attention_concat_pools')

        # Process through intermediate conv layer
        x = self.intermediate_conv(concat)
        # Apply final conv to get the attention map
        attention_map = self.final_conv(x)

        return inputs * attention_map

    def get_config(self):
        config = super(SpatialAttention, self).get_config()
        config.update({"kernel_size": self.kernel_size})
        return config