{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "",
   "id": "2b054293ea22fd32"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Edge Aware CNN",
   "id": "18e382e78c11a063"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "fb45424d61190efd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Preprocess Image",
   "id": "9ca7029fbfb428bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "from tqdm import tqdm  # For progress bar\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# --- Constants ---\n",
    "OUTPUT_SIZE: int = 500\n",
    "\"\"\"\n",
    "int: Default target dimension for resizing images (height and width).\n",
    "     Images will be resized to (OUTPUT_SIZE, OUTPUT_SIZE) pixels.\n",
    "\"\"\"\n",
    "OFFSET_PERCENT: int = 5\n",
    "\"\"\"\n",
    "int: Percentage by which to expand the detected hand's bounding box\n",
    "     during cropping, providing a margin around the hand.\n",
    "\"\"\"\n",
    "BLUR_KERNEL_SIZE: int = 7\n",
    "\"\"\"\n",
    "int: Kernel size for the Gaussian Blur applied to images during hand detection.\n",
    "     Must be an odd integer.\n",
    "\"\"\"\n",
    "MIN_CONTOUR_AREA_FACTOR: float = 0.05\n",
    "\"\"\"\n",
    "float: Minimum contour area as a factor of the total image area.\n",
    "       Contours smaller than this threshold are ignored during hand detection,\n",
    "       to filter out small artifacts.\n",
    "\"\"\"\n",
    "\n",
    "# --- Functions ---\n",
    "def apply_clahe(image_gray: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies Contrast Limited Adaptive Histogram Equalization (CLAHE) to a grayscale image.\n",
    "\n",
    "    CLAHE is used to enhance the contrast of the image, especially in regions\n",
    "    where contrast might be low, while limiting noise amplification. This is\n",
    "    particularly useful for medical images like X-rays.\n",
    "\n",
    "    Args:\n",
    "        image_gray (np.ndarray): The input grayscale image (NumPy array).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The contrast-enhanced grayscale image.\n",
    "    \"\"\"\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    return clahe.apply(image_gray)\n",
    "\n",
    "\n",
    "def detect_and_crop_hand_cv(\n",
    "        image_gray: np.ndarray,\n",
    "        offset_percent: int = OFFSET_PERCENT,\n",
    "        blur_ksize: int = BLUR_KERNEL_SIZE,\n",
    "        min_area_factor: float = MIN_CONTOUR_AREA_FACTOR) -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Detects the main hand region in a grayscale X-ray image and crops it.\n",
    "\n",
    "    This function uses OpenCV's contour detection to find the largest connected\n",
    "    component (assumed to be the hand) and crops the image around its bounding box,\n",
    "    applying an optional offset.\n",
    "\n",
    "    Args:\n",
    "        image_gray (np.ndarray): The input grayscale image.\n",
    "        offset_percent (int, optional): Percentage to expand the bounding box. Defaults to OFFSET_PERCENT.\n",
    "        blur_ksize (int, optional): Kernel size for initial Gaussian blur. Defaults to BLUR_KERNEL_SIZE.\n",
    "        min_area_factor (float, optional): Minimum contour area relative to image area. Defaults to MIN_CONTOUR_AREA_FACTOR.\n",
    "\n",
    "    Returns:\n",
    "        Optional[np.ndarray]: The cropped image containing the hand, or None if\n",
    "                              no suitable hand contour is found or cropping fails.\n",
    "    \"\"\"\n",
    "    h, w = image_gray.shape\n",
    "\n",
    "    # Apply Gaussian blur to reduce noise and smooth the image, aiding contour detection.\n",
    "    blurred = cv2.GaussianBlur(image_gray, (blur_ksize, blur_ksize), 0)\n",
    "\n",
    "    # Apply Otsu's thresholding to convert the grayscale image to a binary image.\n",
    "    # This automatically determines an optimal threshold value.\n",
    "    _, thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Find contours in the binary image. RETR_EXTERNAL retrieves only the outer contours.\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # If no contours are found, return None as hand detection failed.\n",
    "    if not contours:\n",
    "        return None\n",
    "\n",
    "    # Find the largest contour by area, assuming it corresponds to the hand.\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    area = cv2.contourArea(largest_contour)\n",
    "    min_area = h * w * min_area_factor\n",
    "\n",
    "    # If the largest contour's area is too small, it's likely noise, so return None.\n",
    "    if area < min_area:\n",
    "        return None\n",
    "\n",
    "    # Get the bounding box coordinates (x, y, width, height) of the largest contour.\n",
    "    x, y, wb, hb = cv2.boundingRect(largest_contour)\n",
    "\n",
    "    # Calculate the offset for the bounding box based on the specified percentage,\n",
    "    # ensuring the cropped region includes a margin around the hand.\n",
    "    offset_h = int((h * offset_percent) / 100)\n",
    "    offset_w = int((w * offset_percent) / 100)\n",
    "\n",
    "    # Calculate the final coordinates for cropping, ensuring they stay within image boundaries.\n",
    "    y_min = max(0, y - offset_h)\n",
    "    y_max = min(h, y + hb + offset_h)\n",
    "    x_min = max(0, x - offset_w)\n",
    "    x_max = min(w, x + wb + offset_w)\n",
    "\n",
    "    # Crop the original grayscale image using the calculated bounding box.\n",
    "    # Ensure the cropping dimensions are valid (max > min).\n",
    "    if y_max > y_min and x_max > x_min:\n",
    "        return image_gray[y_min:y_max, x_min:x_max]\n",
    "    else:  # If calculated box is invalid (e.g., negative or zero dimensions)\n",
    "        return None\n",
    "\n",
    "\n",
    "def resize(image_gray: np.ndarray, output_size: int = OUTPUT_SIZE) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Resizes a grayscale image to a square output_size, preserving aspect ratio by padding.\n",
    "\n",
    "    The image is first padded to a square shape based on its maximum dimension,\n",
    "    using the mean pixel value as padding color. Then, it's resized to the final\n",
    "    desired output size.\n",
    "\n",
    "    Args:\n",
    "        image_gray (np.ndarray): The input grayscale image.\n",
    "        output_size (int, optional): The target square dimension (e.g., 500x500). Defaults to OUTPUT_SIZE.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The resized and padded grayscale image.\n",
    "    \"\"\"\n",
    "    h, w = image_gray.shape\n",
    "    max_dim = max(h, w)\n",
    "\n",
    "    # Calculate padding needed to make the image square.\n",
    "    top_pad = (max_dim - h) // 2\n",
    "    bottom_pad = max_dim - h - top_pad\n",
    "    left_pad = (max_dim - w) // 2\n",
    "    right_pad = max_dim - w - left_pad\n",
    "\n",
    "    # Pad the image with the mean pixel value of the original image.\n",
    "    padding = int(np.mean(image_gray))\n",
    "    padded_img = cv2.copyMakeBorder(image_gray, top_pad, bottom_pad, left_pad, right_pad,\n",
    "                                    cv2.BORDER_CONSTANT, value=padding)\n",
    "\n",
    "    # Resize the padded image to the final output size using area interpolation (good for shrinking).\n",
    "    resized_img = cv2.resize(padded_img, (output_size, output_size), interpolation=cv2.INTER_AREA)\n",
    "    return resized_img\n",
    "\n",
    "\n",
    "def aug(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies a set of random augmentations to a grayscale image.\n",
    "\n",
    "    These augmentations are typically used during training to increase the diversity\n",
    "    of the dataset and improve the model's generalization capabilities.\n",
    "    Applied augmentations include: random horizontal flip, random 90/180/270 degree rotations,\n",
    "    random inversion (negative image), random brightness adjustment, and random contrast adjustment.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): The input grayscale image (NumPy array).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The augmented image.\n",
    "    \"\"\"\n",
    "    # Random horizontal flip: Flips the image left-right with 50% probability.\n",
    "    if random.random() > 0.5:\n",
    "        image = cv2.flip(image, 1)\n",
    "\n",
    "    # Random 90/180/270 degree rotation: Rotates the image with 75% probability.\n",
    "    # None means no rotation.\n",
    "    rot_code = random.choice([None, cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_180, cv2.ROTATE_90_COUNTERCLOCKWISE])\n",
    "    if rot_code is not None:\n",
    "        image = cv2.rotate(image, rot_code)\n",
    "\n",
    "    # Random inversion (negative image): Inverts pixel values (255-x) with 50% probability.\n",
    "    if random.random() > 0.5:\n",
    "        image = 255 - image  # Invert pixel values for 8-bit grayscale (0-255 range)\n",
    "\n",
    "    # Random brightness adjustment: Adjusts brightness by a random factor (0.7 to 1.3) with 50% probability.\n",
    "    # `cv2.convertScaleAbs` handles clipping to 0-255 and converts to absolute values.\n",
    "    if random.random() > 0.5:\n",
    "        alpha = random.uniform(0.7, 1.3)  # Factor for brightness (1.0 is no change)\n",
    "        image = cv2.convertScaleAbs(image, alpha=alpha, beta=0)  # beta is offset, set to 0\n",
    "\n",
    "    # Random contrast adjustment: Adjusts contrast by a random factor (0.7 to 1.3) with 50% probability.\n",
    "    if random.random() > 0.5:\n",
    "        alpha = random.uniform(0.7, 1.3)  # Factor for contrast\n",
    "        # `cv2.convertScaleAbs` (alpha, beta) performs: output = alpha * input + beta\n",
    "        image = cv2.convertScaleAbs(image, alpha=alpha, beta=0)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def calculate_mean_std(image_folder: str, img_size: Tuple[int, int]) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Calculates the mean and standard deviation of pixel values across all images\n",
    "    within a specified folder.\n",
    "\n",
    "    This function iterates through all PNG images in the given directory,\n",
    "    resizes them (if necessary), flattens their pixel values, and then computes\n",
    "    the overall mean and standard deviation. It includes detailed logging for\n",
    "    files that cannot be read or processed. These calculated values are crucial\n",
    "    for standardizing the dataset for neural network input.\n",
    "\n",
    "    Args:\n",
    "        image_folder (str): The path to the directory containing the images (e.g., 'data/Train/prep_images').\n",
    "        img_size (Tuple[int, int]): The target size (height, width) to which images should be\n",
    "                                     resized before calculating pixel values. This ensures\n",
    "                                     consistency in the calculation.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: A tuple containing the mean and standard deviation of all\n",
    "                             processed pixel values. Returns (0.0, 1.0) if no valid\n",
    "                             pixel values are found to prevent division by zero errors.\n",
    "    \"\"\"\n",
    "    pixel_values = []\n",
    "    print(f\"\\nCalculating mean and standard deviation for images in: {image_folder}\")\n",
    "    # List all PNG files in the specified folder.\n",
    "    image_files = [f for f in os.listdir(image_folder) if f.endswith('.png')]\n",
    "\n",
    "    total_files = len(image_files)\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "\n",
    "    # Iterate through each image file with a progress bar.\n",
    "    for img_file in tqdm(image_files, desc=\"Processing images for mean/std\"):\n",
    "        img_path = os.path.join(image_folder, img_file)\n",
    "        try:\n",
    "            # Preliminary check for empty or corrupted files by checking file size.\n",
    "            if os.path.getsize(img_path) == 0:\n",
    "                print(f\"Warning: Skipped empty file {img_file}\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            # Read the image as grayscale directly using OpenCV.\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is None:\n",
    "                # cv2.imread returns None if it fails to read the file (e.g., corrupted, wrong format).\n",
    "                print(f\"Warning: Could not read or process {img_file}. Skipping.\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            # Resize the image to the target size if its dimensions don't match.\n",
    "            # This ensures all images contribute equally to the mean/std calculation,\n",
    "            # regardless of their initial cropped size variability.\n",
    "            if img.shape[0] != img_size[0] or img.shape[1] != img_size[1]:\n",
    "                img = cv2.resize(img, img_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            # Flatten the 2D image array into a 1D array of pixel values and extend the list.\n",
    "            pixel_values.extend(img.flatten())\n",
    "            processed_count += 1\n",
    "        except:\n",
    "            # Catch any unexpected errors during image processing and log a warning.\n",
    "            skipped_count += 1\n",
    "\n",
    "    # Convert the list of pixel values to a NumPy array for efficient calculation.\n",
    "    pixel_values = np.array(pixel_values, dtype=np.float32)\n",
    "\n",
    "    # Print a summary of the mean/std calculation process.\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Mean/Std Calculation Summary for {image_folder}:\")\n",
    "    print(f\"Total files found: {total_files}\")\n",
    "    print(f\"Successfully processed files: {processed_count}\")\n",
    "    print(f\"Skipped files: {skipped_count}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # If no valid pixel values were found (e.g., all files skipped), return default\n",
    "    # values to prevent division by zero errors in standardization.\n",
    "    if len(pixel_values) == 0:\n",
    "        print(\"No valid pixel values found for mean/std calculation. Returning default values (0.0, 1.0).\")\n",
    "        return 0.0, 1.0\n",
    "\n",
    "    # Calculate the mean and standard deviation of the collected pixel values.\n",
    "    mean_val = np.mean(pixel_values)\n",
    "    std_val = np.std(pixel_values)\n",
    "\n",
    "    # Safeguard against zero standard deviation (e.g., if all pixels have the same value).\n",
    "    # In such cases, replace std_val with 1.0 to prevent division by zero during standardization.\n",
    "    if std_val < 1e-7:  # Using a small epsilon to check for near-zero std\n",
    "        std_val = 1.0\n",
    "\n",
    "    print(f\"Mean pixel value of the training set: {mean_val:.4f}\")\n",
    "    print(f\"Standard deviation of the training set pixel values: {std_val:.4f}\")\n",
    "    return mean_val, std_val"
   ],
   "id": "376540a33c39c502"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Preprocessing pipeline",
   "id": "b1adb273246b990d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import specific preprocessing functions from PreprocessImage module\n",
    "from src.preprocessing.PreprocessImage import detect_and_crop_hand_cv, apply_clahe, aug, resize\n",
    "\n",
    "# --- Constants for Directory Names ---\n",
    "valid_sample_dir = 'validation_samples'\n",
    "\"\"\"str: Subdirectory name for validation raw images.\"\"\"\n",
    "test_sample_dir = 'test_samples'\n",
    "\"\"\"str: Subdirectory name for test raw images.\"\"\"\n",
    "train_sample_dir = 'train_samples'\n",
    "\"\"\"str: Subdirectory name for training raw images.\"\"\"\n",
    "prep_img_dir = 'prep_images'\n",
    "\"\"\"str: Subdirectory name where preprocessed images will be saved.\"\"\"\n",
    "\n",
    "def preprocess_pipeline(folder: str, Train: bool = False, Test: bool = False, Val: bool = True):\n",
    "    \"\"\"\n",
    "    Orchestrates the preprocessing of image datasets based on the specified set type.\n",
    "\n",
    "    This function determines which input subdirectory to use (train, validation, or test)\n",
    "    and calls the `preprocess_images` function accordingly, enabling augmentation\n",
    "    only for the training set.\n",
    "\n",
    "    Args:\n",
    "        folder (str): The base directory containing the raw image subfolders (e.g., 'data/Train').\n",
    "        Train (bool, optional): If True, preprocesses the training samples and applies augmentation. Defaults to False.\n",
    "        Test (bool, optional): If True, preprocesses the test samples (no augmentation). Defaults to False.\n",
    "        Val (bool, optional): If True, preprocesses the validation samples (no augmentation). Defaults to True.\n",
    "                              Note: Only one of Train, Test, or Val should typically be True.\n",
    "    \"\"\"\n",
    "    if Val:  # Preprocess validation samples\n",
    "        subdir_input = os.path.join(folder, valid_sample_dir)\n",
    "        subdir_output = os.path.join(folder, prep_img_dir)\n",
    "        preprocess_images(subdir_input, subdir_output, augment=False)  # No augmentation for validation\n",
    "    elif Train:  # Preprocess training samples\n",
    "        subdir_input = os.path.join(folder, train_sample_dir)\n",
    "        subdir_output = os.path.join(folder, prep_img_dir)\n",
    "        preprocess_images(subdir_input, subdir_output, augment=True)  # Apply augmentation for training\n",
    "    elif Test:  # Preprocess test samples\n",
    "        subdir_input = os.path.join(folder, test_sample_dir)\n",
    "        subdir_output = os.path.join(folder, prep_img_dir)\n",
    "        preprocess_images(subdir_input, subdir_output, augment=False)  # No augmentation for test\n",
    "    else:\n",
    "        print(\"Warning: No dataset type (Train, Test, Val) specified for preprocessing. No action taken.\")\n",
    "\n",
    "\n",
    "def preprocess_images(input_path: str, output_path: str, output_size: int = 256, augment: bool = False):\n",
    "    \"\"\"\n",
    "    Processes images from an input directory and saves the processed versions\n",
    "    to an output directory.\n",
    "\n",
    "    Each image undergoes hand detection, cropping, CLAHE enhancement, resizing,\n",
    "    and optional data augmentation. Provides progress updates and error logging.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): The directory containing the raw input images.\n",
    "        output_path (str): The directory where processed images will be saved.\n",
    "                           This directory will be created if it does not exist.\n",
    "        output_size (int, optional): The target side length for resized square images. Defaults to 256.\n",
    "        augment (bool, optional): If True, applies random data augmentation to images\n",
    "                                  before saving. Typically True for training sets only. Defaults to False.\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    print(f\"Input directory:  {input_path}\")\n",
    "    print(f\"Output directory: {output_path}\")\n",
    "    print(f\"Output size:      {output_size}x{output_size} pixels\")\n",
    "    print(f\"Augmentation:     {'Disabled' if not augment else 'Enabled'}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # List all PNG files in the input directory.\n",
    "    # Using a set initially for potential future deduplication if needed, though list comprehension is often fine.\n",
    "    image_files = {img for img in os.listdir(input_path) if img.lower().endswith('.png')}\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"No PNG images found in {input_path}. Exiting preprocessing for this folder.\")\n",
    "        # sys.exit(0) # Do not exit the whole script if one folder is empty.\n",
    "        return  # Just return from the function\n",
    "\n",
    "    total_images = len(image_files)\n",
    "    processed_count = 0  # Count of images successfully processed and saved\n",
    "    success_count = 0  # Count of images where hand detection was successful\n",
    "    skipped_count = 0  # Count of images skipped due to read errors or detection failure\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate through each image file with a progress bar.\n",
    "    for img_file in tqdm(image_files, desc=\"Processing Images\", unit=\"img\"):\n",
    "        input_img_path = os.path.join(input_path, img_file)\n",
    "        try:\n",
    "            # Read the image as grayscale. cv2.IMREAD_GRAYSCALE ensures 1 channel.\n",
    "            img_gray = cv2.imread(str(input_img_path), cv2.IMREAD_GRAYSCALE)\n",
    "            if img_gray is None:  # Check if image reading failed (e.g., corrupted file)\n",
    "                print(f\"Warning: Could not read image {img_file}. Skipping.\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            # Detect and crop the hand region from the grayscale image.\n",
    "            img_cropped = detect_and_crop_hand_cv(img_gray)\n",
    "\n",
    "            if img_cropped is None:  # Check if hand detection failed for this image\n",
    "                print(f\"Warning: Hand detection failed for {img_file}. Skipping.\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            else:\n",
    "                success_count += 1  # Increment count for successful hand detections\n",
    "\n",
    "            # Apply CLAHE to the cropped image to enhance contrast.\n",
    "            img_clahe = apply_clahe(img_cropped)\n",
    "\n",
    "            # Resize the image to the target output_size, preserving aspect ratio with padding.\n",
    "            img_resized = resize(img_clahe, output_size=output_size)\n",
    "\n",
    "            # Apply data augmentation if `augment` flag is True (typically for training set).\n",
    "            img_final = img_resized\n",
    "            if augment:\n",
    "                img_final = aug(img_resized)\n",
    "\n",
    "            # Construct the output filename and save the final processed image.\n",
    "            output_filename = os.path.join(output_path, img_file)\n",
    "            cv2.imwrite(str(output_filename), img_final)\n",
    "            processed_count += 1  # Increment count for successfully processed and saved images\n",
    "\n",
    "        except Exception as e:\n",
    "            # Catch any unexpected errors during the processing of a single image.\n",
    "            print(f\"Error processing {img_file}: {e}. Skipping.\")\n",
    "            skipped_count += 1\n",
    "            continue  # Continue to the next image even if an error occurs\n",
    "\n",
    "    # --- Summarize processing results ---\n",
    "    elapsed_time = time.time() - start_time\n",
    "    elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "    print(\"\\n\" + \"-\" * 30)\n",
    "    print(\"Image preprocessing finished.\")\n",
    "    print(f\"Total images found in input directory: {total_images}\")\n",
    "    print(f\"Successfully processed and saved: {processed_count}\")\n",
    "    print(\n",
    "        f\"Hand detection successful: {success_count}\")  # Note: success_count refers to hand detection, not file saving.\n",
    "    print(f\"Images skipped (read/detection error): {skipped_count}\")\n",
    "    print(f\"Total time elapsed: {elapsed_str}\")\n",
    "    if total_images > 0:\n",
    "        # Calculate hand detection success rate based on files attempted to process.\n",
    "        detection_rate = (success_count / (processed_count + skipped_count) * 100) if (\n",
    "                                                                                                  processed_count + skipped_count) > 0 else 0\n",
    "        print(f\"Hand Detection Success Rate (of processed/skipped): {detection_rate:.1f}%\")\n",
    "    print(\"-\" * 30)"
   ],
   "id": "7116c55f81b52972"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model",
   "id": "50fb97e074e58808"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dataset",
   "id": "b7f2ecfb7ce996f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# src/dataset/radiograph_dataset_builder.py\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# --- RadiographDatasetBuilder ---\n",
    "class RadiographDatasetBuilder:\n",
    "    \"\"\"\n",
    "    Builds a TensorFlow Dataset for radiograph images, handling raw and\n",
    "    preprocessed image inputs along with age labels, applying pixel standardization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 base_dir,\n",
    "                 label_csv,\n",
    "                 img_subfolder=\"prep_images\",\n",
    "                 prepimg_subfolder=\"prep_images\",\n",
    "                 img_size=(128, 128),\n",
    "                 batch_size=16,\n",
    "                 mean_pixel_value: float = 0.0,  # Nuovo parametro per la media\n",
    "                 std_pixel_value: float = 1.0):  # Nuovo parametro per la deviazione standard\n",
    "        \"\"\"\n",
    "        Initializes the dataset builder.\n",
    "\n",
    "        Args:\n",
    "            base_dir (str): The base directory containing image subfolders.\n",
    "            label_csv (str): The name of the CSV file containing image IDs and bone ages,\n",
    "                             relative to base_dir.\n",
    "            img_subfolder (str): Subfolder containing the main images to list files from.\n",
    "            prepimg_subfolder (str): Subfolder containing the preprocessed images.\n",
    "            img_size (tuple): Desired image size (height, width).\n",
    "            batch_size (int): Batch size for the dataset.\n",
    "            mean_pixel_value (float): Mean pixel value for standardizing image data.\n",
    "            std_pixel_value (float): Standard deviation of pixel values for standardizing image data.\n",
    "        \"\"\"\n",
    "        self.base_dir = base_dir\n",
    "        self.img_subfolder = os.path.join(base_dir, img_subfolder)\n",
    "        self.prepimg_subfolder = os.path.join(base_dir, prepimg_subfolder)\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mean_pixel_value = mean_pixel_value\n",
    "        self.std_pixel_value = std_pixel_value\n",
    "\n",
    "        # Read CSV and prepare age_map\n",
    "        # Corretto: label_csv dovrebbe essere relativo a base_dir\n",
    "        df = pd.read_csv(label_csv)\n",
    "        try:\n",
    "            df['filename'] = df['id'].astype(str)\n",
    "            df['age_months'] = df['boneage']\n",
    "        except KeyError:  # Handle cases where column names might be different or delimiter is ';'\n",
    "            df = pd.read_csv(label_csv, delimiter=\";\")  # Corretto path\n",
    "            df['filename'] = df['id'].astype(str)\n",
    "            df['age_months'] = df['boneage'].astype(str).str.replace(',', '.').astype(float)\n",
    "\n",
    "        self.age_map = dict(zip(df['filename'], df['age_months']))\n",
    "\n",
    "    def _parse_function(self, filepath):\n",
    "        \"\"\"\n",
    "        Parses a single image file and its corresponding preprocessed images\n",
    "        and age label, applying pixel standardization.\n",
    "        This function runs in Python context via tf.py_function.\n",
    "\n",
    "        Args:\n",
    "            filepath (tf.Tensor): The TensorFlow string tensor representing the image file path.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (raw_img, prep_img, age_months) as TensorFlow tensors,\n",
    "                   with images standardized.\n",
    "        \"\"\"\n",
    "\n",
    "        # 2) Extract filename stem to find corresponding preprocessed images\n",
    "        fname_tensor = tf.strings.split(filepath, os.sep)[-1]\n",
    "        stem_tensor = tf.strings.split(fname_tensor, '.')[0]\n",
    "        fname = stem_tensor.numpy().decode('utf-8')  # Python string for dict lookup and path construction\n",
    "\n",
    "        # Construct path for preprocessed image\n",
    "        prep_path = os.path.join(self.prepimg_subfolder, fname + \".png\")\n",
    "\n",
    "        # Read and resize preprocessed image\n",
    "        prep_img = tf.io.read_file(prep_path)\n",
    "        prep_img = tf.image.decode_png(prep_img, channels=1)  # Decode as grayscale\n",
    "        prep_img = tf.image.resize(prep_img, self.img_size)\n",
    "\n",
    "        # Convert to float32 BEFORE standardization\n",
    "        prep_img = tf.cast(prep_img, tf.float32)\n",
    "\n",
    "        # standardization\n",
    "        std_val_safe = self.std_pixel_value if self.std_pixel_value > 1e-7 else 1.0\n",
    "\n",
    "        prep_img = (prep_img - self.mean_pixel_value) / std_val_safe\n",
    "\n",
    "        # Get age from map, handling potential comma decimal separator\n",
    "        try:\n",
    "            age_months = float(self.age_map[fname])\n",
    "        except ValueError:\n",
    "            age_months = float(str(self.age_map[fname]).replace(',', '.'))\n",
    "\n",
    "        # Return all three items. The build method's _tf_parse will then select what to pass.\n",
    "        return prep_img, age_months\n",
    "\n",
    "    def build(self, train=True):\n",
    "        \"\"\"\n",
    "        Builds and returns a TensorFlow Dataset.\n",
    "\n",
    "        Args:\n",
    "            train (bool): If True, shuffles the dataset.\n",
    "\n",
    "        Returns:\n",
    "            tf.data.Dataset: A TensorFlow dataset yielding\n",
    "                             (prep_img, age_months) tuples, as per the model's current input.\n",
    "        \"\"\"\n",
    "        pattern = os.path.join(self.img_subfolder, \"*.png\")\n",
    "        ds = tf.data.Dataset.list_files(pattern, shuffle=train)\n",
    "\n",
    "        def _tf_parse(fp):\n",
    "            \"\"\"\n",
    "            TensorFlow-graph compatible wrapper for _parse_function.\n",
    "            \"\"\"\n",
    "            # _parse_function restituisce (raw_img, prep_img, age_months)\n",
    "            img_prep, age_months = tf.py_function(\n",
    "                func=self._parse_function,\n",
    "                inp=[fp],  # Pass the filepath tensor to the py_function\n",
    "                Tout=[tf.float32, tf.float32]  # Tipi di output di _parse_function\n",
    "            )\n",
    "\n",
    "            # Set static shapes for the output tensors\n",
    "            img_prep.set_shape((*self.img_size, 1))\n",
    "            age_months.set_shape(())\n",
    "\n",
    "            return img_prep, age_months  # Restituisce solo l'immagine preprocessata e l'età\n",
    "\n",
    "        # Map the parsing function over the dataset\n",
    "        ds = ds.map(_tf_parse, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "        # Batch and prefetch for performance\n",
    "        ds = ds.batch(self.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        return ds\n"
   ],
   "id": "738e3fb1584a76a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Attention Layer",
   "id": "f678cdbc387acda6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "class SpatialAttention(layers.Layer):\n",
    "    \"\"\"\n",
    "    A custom Keras layer implementing a more attentive spatial attention mechanism.\n",
    "    It now includes an intermediate convolutional layer to learn richer attention features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size=7, **kwargs):\n",
    "        super(SpatialAttention, self).__init__(**kwargs)\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        # Intermediate conv layer to learn more complex features from concatenated avg/max pools\n",
    "        # More filters (e.g., 8 or 16) for more capacity, smaller kernel for local patterns.\n",
    "        self.intermediate_conv = layers.Conv2D(\n",
    "            filters=32,  # Increased filters for more \"intelligence\"\n",
    "            kernel_size=5,  # Small kernel for local feature extraction within attention\n",
    "            padding='same',\n",
    "            activation='relu',  # Added ReLU activation for non-linearity\n",
    "            name='attention_intermediate_conv'\n",
    "        )\n",
    "        # Final conv layer to produce the single-channel attention map\n",
    "        self.final_conv = layers.Conv2D(\n",
    "            filters=1,\n",
    "            kernel_size=self.kernel_size,  # Use the initial kernel_size for the final aggregation\n",
    "            padding='same',\n",
    "            activation='sigmoid',  # Sigmoid to output values between 0 and 1\n",
    "            use_bias=False,\n",
    "            name='attention_final_conv'\n",
    "        )\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Keras will automatically build internal layers like self.intermediate_conv and self.final_conv\n",
    "        super(SpatialAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Applies spatial attention to the input feature map.\n",
    "        The attention map is now generated by a slightly deeper sub-network.\n",
    "        \"\"\"\n",
    "        # Compute average and max pool along the channel axis\n",
    "        avg_pool = tf.reduce_mean(inputs, axis=-1, keepdims=True)\n",
    "        max_pool = tf.reduce_max(inputs, axis=-1, keepdims=True)\n",
    "        concat = layers.concatenate([avg_pool, max_pool], axis=-1, name='attention_concat_pools')\n",
    "\n",
    "        # Process through intermediate conv layer\n",
    "        x = self.intermediate_conv(concat)\n",
    "        # Apply final conv to get the attention map\n",
    "        attention_map = self.final_conv(x)\n",
    "\n",
    "        return inputs * attention_map\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(SpatialAttention, self).get_config()\n",
    "        config.update({\"kernel_size\": self.kernel_size})\n",
    "        return config"
   ],
   "id": "5a12f9b3a5f426bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### AgrePredictionModel",
   "id": "13bff061cb05744f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers, regularizers, models\n",
    "\n",
    "from src.Models.AttentionLayer import \\\n",
    "    SpatialAttention  # Assuming this import path is correct and SpatialAttention is defined elsewhere\n",
    "\n",
    "# --- Age Prediction Model with Attention ---\n",
    "class AgePredictionModel:\n",
    "    \"\"\"\n",
    "    Represents a deep learning model for automated bone age prediction from hand radiographs.\n",
    "\n",
    "    This model employs a multi-layered Convolutional Neural Network (CNN) backbone\n",
    "    for feature extraction, integrated with a custom spatial attention mechanism,\n",
    "    and a robust fully connected regression head. It is designed to be gender-agnostic,\n",
    "    relying solely on image-derived features for prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=(128, 128)):\n",
    "        \"\"\"\n",
    "        Initializes the AgePredictionModel.\n",
    "\n",
    "        Args:\n",
    "            img_size (tuple, optional): The target dimensions (height, width) for input images.\n",
    "                                        Defaults to (128, 128). This should match the size\n",
    "                                        used in preprocessing.\n",
    "        \"\"\"\n",
    "        self.img_size = img_size\n",
    "        # Build the Keras model graph immediately upon initialization\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _create_cnn_branch(self, input_tensor: tf.Tensor, name_prefix: str) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Constructs a single Convolutional Neural Network (CNN) branch for feature extraction.\n",
    "\n",
    "        This branch consists of five sequential blocks, each typically comprising two\n",
    "        convolutional layers, Batch Normalization, ReLU activation, and MaxPooling.\n",
    "        The number of filters progressively increases with depth.\n",
    "\n",
    "        Args:\n",
    "            input_tensor (tf.Tensor): The input tensor to the CNN branch (e.g., image input).\n",
    "            name_prefix (str): A prefix for naming the layers within this branch\n",
    "                               to ensure uniqueness (e.g., 'prep', 'raw', 'extr').\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The output tensor from the final MaxPooling layer of this CNN branch,\n",
    "                       representing extracted spatial features.\n",
    "        \"\"\"\n",
    "        # Block 1: Conv -> BN -> ReLU -> Conv -> BN -> ReLU -> MaxPool\n",
    "        # Captures initial low-level features\n",
    "        x = layers.Conv2D(32, (3, 3), padding='same',\n",
    "                          kernel_regularizer=regularizers.l2(1e-4), name=f'{name_prefix}_conv1a')(input_tensor)\n",
    "        x = layers.BatchNormalization(name=f'{name_prefix}_bn1a')(x)\n",
    "        x = layers.Activation('relu', name=f'{name_prefix}_relu1a')(x)\n",
    "        x = layers.Conv2D(32, (3, 3), padding='same',\n",
    "                          kernel_regularizer=regularizers.l2(1e-4), name=f'{name_prefix}_conv1b')(x)\n",
    "        x = layers.BatchNormalization(name=f'{name_prefix}_bn1b')(x)\n",
    "        x = layers.Activation('relu', name=f'{name_prefix}_relu1b')(x)\n",
    "        x = layers.MaxPooling2D((2, 2), strides=(2, 2), name=f'{name_prefix}_pool1')(x)\n",
    "\n",
    "        # Block 2: Increase filters, capture more complex features\n",
    "        x = layers.Conv2D(64, (3, 3), padding='same',\n",
    "                          kernel_regularizer=regularizers.l2(1e-4), name=f'{name_prefix}_conv2a')(x)\n",
    "        x = layers.BatchNormalization(name=f'{name_prefix}_bn2a')(x)\n",
    "        x = layers.Activation('relu', name=f'{name_prefix}_relu2a')(x)\n",
    "        x = layers.Conv2D(64, (3, 3), padding='same',\n",
    "                          kernel_regularizer=regularizers.l2(1e-4), name=f'{name_prefix}_conv2b')(x)\n",
    "        x = layers.BatchNormalization(name=f'{name_prefix}_bn2b')(x)\n",
    "        x = layers.Activation('relu', name=f'{name_prefix}_relu2b')(x)\n",
    "        x = layers.MaxPooling2D((2, 2), strides=(2, 2), name=f'{name_prefix}_pool2')(x)\n",
    "\n",
    "        # Block 3: Further increase filters for higher-level feature abstraction\n",
    "        x = layers.Conv2D(128, (3, 3), padding='same',\n",
    "                          kernel_regularizer=regularizers.l2(1e-4), name=f'{name_prefix}_conv3a')(x)\n",
    "        x = layers.BatchNormalization(name=f'{name_prefix}_bn3a')(x)\n",
    "        x = layers.Activation('relu', name=f'{name_prefix}_relu3a')(x)\n",
    "        x = layers.Conv2D(128, (3, 3), padding='same',\n",
    "                          kernel_regularizer=regularizers.l2(1e-4), name=f'{name_prefix}_conv3b')(x)\n",
    "        x = layers.BatchNormalization(name=f'{name_prefix}_bn3b')(x)\n",
    "        x = layers.Activation('relu', name=f'{name_prefix}_relu3b')(x)\n",
    "        x = layers.MaxPooling2D((2, 2), strides=(2, 2), name=f'{name_prefix}_pool3')(x)\n",
    "\n",
    "        # Block 4: Continue increasing filter depth\n",
    "        x = layers.Conv2D(256, (3, 3), padding='same',\n",
    "                          kernel_regularizer=regularizers.l2(1e-4), name=f'{name_prefix}_conv4a')(x)\n",
    "        x = layers.BatchNormalization(name=f'{name_prefix}_bn4a')(x)\n",
    "        x = layers.Activation('relu', name=f'{name_prefix}_relu4a')(x)\n",
    "        x = layers.Conv2D(256, (3, 3), padding='same',\n",
    "                          kernel_regularizer=regularizers.l2(1e-4), name=f'{name_prefix}_conv4b')(x)\n",
    "        x = layers.BatchNormalization(name=f'{name_prefix}_bn4b')(x)\n",
    "        x = layers.Activation('relu', name=f'{name_prefix}_relu4b')(x)\n",
    "        x = layers.MaxPooling2D((2, 2), strides=(2, 2), name=f'{name_prefix}_pool4')(x)\n",
    "\n",
    "        # Block 5: Final convolutional block in the backbone\n",
    "        # Note: Filter count adjusted to 512 in the theoretical write-up for deeper abstraction,\n",
    "        # but kept at 256 here based on provided code's last working state.\n",
    "        # If input size is 256x256, after 5 pools, spatial dim becomes 8x8.\n",
    "        x = layers.Conv2D(256, (3, 3), padding='same', name='conv5a',\n",
    "                          kernel_regularizer=regularizers.l2(1e-4))(x)  # Added L2 regularization for consistency\n",
    "        x = layers.BatchNormalization(name='bn5a')(x)\n",
    "        x = layers.Activation('relu', name='relu5a')(x)\n",
    "        x = layers.Conv2D(256, (3, 3), padding='same', name='conv5b',\n",
    "                          kernel_regularizer=regularizers.l2(1e-4))(x)  # Added L2 regularization for consistency\n",
    "        x = layers.BatchNormalization(name='bn5b')(x)\n",
    "        x = layers.Activation('relu', name='relu5b')(x)\n",
    "        x = layers.MaxPooling2D((2, 2), name='pool5')(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _build_model(self) -> models.Model:\n",
    "        \"\"\"\n",
    "        Constructs the complete Keras model for bone age prediction.\n",
    "\n",
    "        The model integrates a CNN backbone for feature extraction,\n",
    "        a Spatial Attention layer, and a multi-layered regression head.\n",
    "\n",
    "        Returns:\n",
    "            tf.keras.Model: The compiled Keras Model instance.\n",
    "        \"\"\"\n",
    "        # Define the input layer for preprocessed images.\n",
    "        # The input shape is (height, width, channels), where channels=1 for grayscale.\n",
    "        prep_input = layers.Input(shape=(*self.img_size, 1), name='prep_input')\n",
    "\n",
    "        # Create the CNN branch for feature extraction from the preprocessed input.\n",
    "        prep_features = self._create_cnn_branch(prep_input, 'prep')\n",
    "\n",
    "        # Apply the custom Spatial Attention mechanism to the extracted features.\n",
    "        # This layer selectively re-weights spatial regions, focusing on diagnostically\n",
    "        # relevant areas of the radiograph.\n",
    "        attended_prep_features = SpatialAttention(name='attention_prep')(prep_features)\n",
    "\n",
    "        # Flatten the attended features to prepare for the fully connected layers.\n",
    "        x = layers.Flatten(name='flatten_features')(attended_prep_features)\n",
    "\n",
    "        # --- Regression Head: Fully Connected Layers for Age Prediction ---\n",
    "        # Dense Layer 1: Processes the flattened features.\n",
    "        # Followed by Batch Normalization and Dropout for regularization.\n",
    "        x = layers.Dense(512, activation='relu', name='dense1',\n",
    "                         kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "        x = layers.BatchNormalization(name='bn_dense1')(x)\n",
    "        x = layers.Dropout(0.4, name='dropout1')(x)\n",
    "\n",
    "        # Dense Layer 2: Further refines the features.\n",
    "        # Includes Batch Normalization and Dropout.\n",
    "        x = layers.Dense(256, activation='relu', name='dense2',\n",
    "                         kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "        x = layers.BatchNormalization(name='bn_dense2')(x)\n",
    "        x = layers.Dropout(0.4, name='dropout2')(x)\n",
    "\n",
    "        # Dense Layer 3: Adds more complexity to the mapping.\n",
    "        # Also includes Batch Normalization and Dropout.\n",
    "        x = layers.Dense(128, activation='relu', name='dense3',\n",
    "                         kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "        x = layers.BatchNormalization(name='bn_dense3')(x)\n",
    "        x = layers.Dropout(0.3, name='dropout3')(x)\n",
    "\n",
    "        # Final Output Layer: Predicts the bone age in months.\n",
    "        # 'linear' activation allows for any real value output.\n",
    "        # A subsequent 'relu' activation is applied to ensure predictions are non-negative.\n",
    "        output_linear = layers.Dense(1, name='age_output_linear',\n",
    "                                     kernel_regularizer=regularizers.l2(1e-4))(x)  # Added L2 for consistency\n",
    "        # Force predictions to be non-negative (bone age cannot be < 0)\n",
    "        output = layers.Activation('relu', name='age_output_relu')(output_linear)\n",
    "\n",
    "        # Create the Keras Model instance, defining its inputs and outputs.\n",
    "        model = models.Model(inputs=prep_input, outputs=output, name='AgePredictionModel')\n",
    "        return model\n",
    "\n",
    "    def compile_model(self, learning_rate: float = 0.0005):\n",
    "        \"\"\"\n",
    "        Compiles the Keras model with a specified optimizer, loss function, and metrics.\n",
    "\n",
    "        Args:\n",
    "            learning_rate (float, optional): The initial learning rate for the Adam optimizer.\n",
    "                                             Defaults to 0.0005.\n",
    "        \"\"\"\n",
    "        # Use Adam optimizer for efficient training with adaptive learning rates.\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "        # Compile the model specifying Mean Absolute Error (MAE) as the loss function\n",
    "        # (directly interpretable as error in months) and also track it as a metric.\n",
    "        self.model.compile(optimizer=optimizer, loss='mae', metrics=['mae'])"
   ],
   "id": "f7e442d16366a692"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Utils",
   "id": "698a764f12dfb48"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load Model",
   "id": "4d4ec2e4aafd16fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "from typing import Optional  # Import for Optional type hint\n",
    "\n",
    "# Assuming correct import path for your custom AttentionLayer\n",
    "from src.Models.AttentionLayer import SpatialAttention\n",
    "\n",
    "def load_trained_model(model_save_path: str) -> Optional[keras.Model]:\n",
    "    \"\"\"\n",
    "    Attempts to load a pre-trained Keras model from a specified path.\n",
    "\n",
    "    This utility function checks for the existence of the model file and\n",
    "    handles potential errors during the loading process. It's designed\n",
    "    to facilitate resuming training from a checkpoint or using a saved model\n",
    "    for evaluation.\n",
    "\n",
    "    Args:\n",
    "        model_save_path (str): The file path to the saved Keras model\n",
    "                               (e.g., '.keras', '.h5', or SavedModel directory).\n",
    "\n",
    "    Returns:\n",
    "        Optional[tf.keras.Model]: The loaded Keras model instance if successful.\n",
    "                                  Returns None if the model file does not exist\n",
    "                                  or if an error occurs during loading.\n",
    "    \"\"\"\n",
    "    # Check if the model file exists at the specified path.\n",
    "    if os.path.exists(model_save_path):\n",
    "        print(f\"\\nLoading existing model from '{model_save_path}' to continue training...\")\n",
    "        try:\n",
    "            # Load the Keras model.\n",
    "            # `custom_objects` is crucial for loading models that use custom layers,\n",
    "            # like your SpatialAttention layer.\n",
    "            loaded_model = load_model(\n",
    "                model_save_path, custom_objects={'SpatialAttention': SpatialAttention}\n",
    "            )\n",
    "            print(\"Model loaded successfully.\")\n",
    "            return loaded_model\n",
    "        except Exception as e:\n",
    "            # Catch any exceptions that occur during the model loading process.\n",
    "            # This can happen if the file is corrupted, the custom object is not found, etc.\n",
    "            print(f\"Error during model loading: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()  # Print the full stack trace for debugging purposes\n",
    "\n",
    "            # If loading fails, treat it as if no model was found, and suggest starting anew.\n",
    "            print(\"Model loading failed. Proceeding with training a new model.\")\n",
    "            return None  # Indicate failure to load by returning None\n",
    "    else:\n",
    "        # Inform the user if the model file does not exist.\n",
    "        print(f\"\\nModel not found at '{model_save_path}'. Starting training from scratch.\")\n",
    "        return None  # Indicate that no existing model was found"
   ],
   "id": "af90a6523604a1c8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Interpret pipeline",
   "id": "70d5529fd9300cd0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras import Model\n",
    "import cv2  # Per il resizing e la gestione delle immagini\n",
    "import os\n",
    "\n",
    "from src.plot.LIME import explain_prediction_lime\n",
    "from src.plot.PlotActivationLayer import visualize_layer_activations\n",
    "from src.plot.PlotFilters import visualize_filters\n",
    "from src.plot.PlotHeatmapOverlay import visualize_attention_map\n",
    "from src.plot.PlotShap import explain_prediction_shap\n",
    "\n",
    "\n",
    "# --- Main orchestrator for interpretability plots ---\n",
    "def run_all_interpretability_plots(\n",
    "        model,\n",
    "        preprocessed_image_tensor,\n",
    "        true_age,\n",
    "        mean_val,\n",
    "        std_val,\n",
    "        output_dir='analysis_plots/interpretability'\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs all interpretability visualizations for a single image sample.\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): The trained Keras model.\n",
    "        preprocessed_image_tensor (tf.Tensor): The preprocessed input image tensor (batch_size=1, H, W, C).\n",
    "        true_age (float): The true age of the patient for the given image.\n",
    "        mean_val (float): Mean value used for image denormalization.\n",
    "        std_val (float): Standard deviation used for image denormalization.\n",
    "        output_dir (str): Directory to save the generated plots.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Denormalize the original image for LIME/SHAP display purposes (0-1 range)\n",
    "    # This is the image that will be shown as background for explanation masks.\n",
    "    original_display_image = (preprocessed_image_tensor[0].numpy() * std_val) + mean_val\n",
    "    original_display_image = (original_display_image - original_display_image.min()) / \\\n",
    "                             (original_display_image.max() - original_display_image.min() + 1e-8)\n",
    "\n",
    "    print(\"\\n--- Starting Model Interpretability Visualizations ---\")\n",
    "\n",
    "    # 1. Spatial Attention Map\n",
    "    print(\"Generating Spatial Attention Map...\")\n",
    "    visualize_attention_map(\n",
    "        model,\n",
    "        preprocessed_image_tensor,\n",
    "        true_age,\n",
    "        std_val,\n",
    "        mean_val,\n",
    "        save_path=os.path.join(output_dir, 'spatial_attention_map.png')\n",
    "    )\n",
    "\n",
    "    # 2. Layer Activations\n",
    "    print(\"Generating Layer Activations Visualization...\")\n",
    "    # Adjust layer_names based on your model's architecture\n",
    "    # You can get a list of layer names by doing `model.summary()`\n",
    "    visualize_layer_activations(\n",
    "        model,\n",
    "        preprocessed_image_tensor,\n",
    "        layer_names=['prep_conv1a', 'prep_conv3a', 'conv5a'],  # EXAMPLE NAMES - ADJUST FOR YOUR MODEL\n",
    "        num_filters_to_show=8,\n",
    "        save_path=os.path.join(output_dir, 'layer_activations.png')\n",
    "    )\n",
    "\n",
    "    # 3. Filter Visualization (if you want to include it, ensure layer_name is correct)\n",
    "    #print(\"Generating Filter Visualizations...\")\n",
    "    #visualize_filters(\n",
    "    #     model,\n",
    "    #     layer_names=['prep_conv1a', 'prep_conv3a', 'conv5a'],\n",
    "    #     save_path=os.path.join(output_dir, 'filter_visualization.png')\n",
    "    # )\n",
    "\n",
    "    # 4. LIME Explanation\n",
    "    print(\"Generating LIME Explanation...\")\n",
    "    explain_prediction_lime(\n",
    "        model,\n",
    "        preprocessed_image_tensor,\n",
    "        original_display_image,\n",
    "        std_val,\n",
    "        mean_val,\n",
    "        save_path=os.path.join(output_dir, 'lime_explanation.png')\n",
    "    )\n",
    "\n",
    "    # 5. SHAP Explanation\n",
    "    #print(\"Generating SHAP Explanation...\")\n",
    "    #explain_prediction_shap(\n",
    "    #    model,\n",
    "    #    preprocessed_image_tensor,\n",
    "    #    original_display_image,\n",
    "    #    save_path=os.path.join(output_dir, 'shap_explanation.png')\n",
    "    #)\n",
    "\n",
    "    print(\"All interpretability visualizations complete.\")"
   ],
   "id": "40d69041cdbb36dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "6a22fd4d9c75ea0c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Calculate std and mean",
   "id": "8346e3d31236e5b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "from tqdm import tqdm  # For progress bar\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# --- Constants ---\n",
    "OUTPUT_SIZE: int = 500\n",
    "\"\"\"\n",
    "int: Default target dimension for resizing images (height and width).\n",
    "     Images will be resized to (OUTPUT_SIZE, OUTPUT_SIZE) pixels.\n",
    "\"\"\"\n",
    "OFFSET_PERCENT: int = 5\n",
    "\"\"\"\n",
    "int: Percentage by which to expand the detected hand's bounding box\n",
    "     during cropping, providing a margin around the hand.\n",
    "\"\"\"\n",
    "BLUR_KERNEL_SIZE: int = 7\n",
    "\"\"\"\n",
    "int: Kernel size for the Gaussian Blur applied to images during hand detection.\n",
    "     Must be an odd integer.\n",
    "\"\"\"\n",
    "MIN_CONTOUR_AREA_FACTOR: float = 0.05\n",
    "\"\"\"\n",
    "float: Minimum contour area as a factor of the total image area.\n",
    "       Contours smaller than this threshold are ignored during hand detection,\n",
    "       to filter out small artifacts.\n",
    "\"\"\"\n",
    "\n",
    "# --- Functions ---\n",
    "def apply_clahe(image_gray: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies Contrast Limited Adaptive Histogram Equalization (CLAHE) to a grayscale image.\n",
    "\n",
    "    CLAHE is used to enhance the contrast of the image, especially in regions\n",
    "    where contrast might be low, while limiting noise amplification. This is\n",
    "    particularly useful for medical images like X-rays.\n",
    "\n",
    "    Args:\n",
    "        image_gray (np.ndarray): The input grayscale image (NumPy array).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The contrast-enhanced grayscale image.\n",
    "    \"\"\"\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    return clahe.apply(image_gray)\n",
    "\n",
    "\n",
    "def detect_and_crop_hand_cv(\n",
    "        image_gray: np.ndarray,\n",
    "        offset_percent: int = OFFSET_PERCENT,\n",
    "        blur_ksize: int = BLUR_KERNEL_SIZE,\n",
    "        min_area_factor: float = MIN_CONTOUR_AREA_FACTOR) -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Detects the main hand region in a grayscale X-ray image and crops it.\n",
    "\n",
    "    This function uses OpenCV's contour detection to find the largest connected\n",
    "    component (assumed to be the hand) and crops the image around its bounding box,\n",
    "    applying an optional offset.\n",
    "\n",
    "    Args:\n",
    "        image_gray (np.ndarray): The input grayscale image.\n",
    "        offset_percent (int, optional): Percentage to expand the bounding box. Defaults to OFFSET_PERCENT.\n",
    "        blur_ksize (int, optional): Kernel size for initial Gaussian blur. Defaults to BLUR_KERNEL_SIZE.\n",
    "        min_area_factor (float, optional): Minimum contour area relative to image area. Defaults to MIN_CONTOUR_AREA_FACTOR.\n",
    "\n",
    "    Returns:\n",
    "        Optional[np.ndarray]: The cropped image containing the hand, or None if\n",
    "                              no suitable hand contour is found or cropping fails.\n",
    "    \"\"\"\n",
    "    h, w = image_gray.shape\n",
    "\n",
    "    # Apply Gaussian blur to reduce noise and smooth the image, aiding contour detection.\n",
    "    blurred = cv2.GaussianBlur(image_gray, (blur_ksize, blur_ksize), 0)\n",
    "\n",
    "    # Apply Otsu's thresholding to convert the grayscale image to a binary image.\n",
    "    # This automatically determines an optimal threshold value.\n",
    "    _, thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Find contours in the binary image. RETR_EXTERNAL retrieves only the outer contours.\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # If no contours are found, return None as hand detection failed.\n",
    "    if not contours:\n",
    "        return None\n",
    "\n",
    "    # Find the largest contour by area, assuming it corresponds to the hand.\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    area = cv2.contourArea(largest_contour)\n",
    "    min_area = h * w * min_area_factor\n",
    "\n",
    "    # If the largest contour's area is too small, it's likely noise, so return None.\n",
    "    if area < min_area:\n",
    "        return None\n",
    "\n",
    "    # Get the bounding box coordinates (x, y, width, height) of the largest contour.\n",
    "    x, y, wb, hb = cv2.boundingRect(largest_contour)\n",
    "\n",
    "    # Calculate the offset for the bounding box based on the specified percentage,\n",
    "    # ensuring the cropped region includes a margin around the hand.\n",
    "    offset_h = int((h * offset_percent) / 100)\n",
    "    offset_w = int((w * offset_percent) / 100)\n",
    "\n",
    "    # Calculate the final coordinates for cropping, ensuring they stay within image boundaries.\n",
    "    y_min = max(0, y - offset_h)\n",
    "    y_max = min(h, y + hb + offset_h)\n",
    "    x_min = max(0, x - offset_w)\n",
    "    x_max = min(w, x + wb + offset_w)\n",
    "\n",
    "    # Crop the original grayscale image using the calculated bounding box.\n",
    "    # Ensure the cropping dimensions are valid (max > min).\n",
    "    if y_max > y_min and x_max > x_min:\n",
    "        return image_gray[y_min:y_max, x_min:x_max]\n",
    "    else:  # If calculated box is invalid (e.g., negative or zero dimensions)\n",
    "        return None\n",
    "\n",
    "\n",
    "def resize(image_gray: np.ndarray, output_size: int = OUTPUT_SIZE) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Resizes a grayscale image to a square output_size, preserving aspect ratio by padding.\n",
    "\n",
    "    The image is first padded to a square shape based on its maximum dimension,\n",
    "    using the mean pixel value as padding color. Then, it's resized to the final\n",
    "    desired output size.\n",
    "\n",
    "    Args:\n",
    "        image_gray (np.ndarray): The input grayscale image.\n",
    "        output_size (int, optional): The target square dimension (e.g., 500x500). Defaults to OUTPUT_SIZE.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The resized and padded grayscale image.\n",
    "    \"\"\"\n",
    "    h, w = image_gray.shape\n",
    "    max_dim = max(h, w)\n",
    "\n",
    "    # Calculate padding needed to make the image square.\n",
    "    top_pad = (max_dim - h) // 2\n",
    "    bottom_pad = max_dim - h - top_pad\n",
    "    left_pad = (max_dim - w) // 2\n",
    "    right_pad = max_dim - w - left_pad\n",
    "\n",
    "    # Pad the image with the mean pixel value of the original image.\n",
    "    padding = int(np.mean(image_gray))\n",
    "    padded_img = cv2.copyMakeBorder(image_gray, top_pad, bottom_pad, left_pad, right_pad,\n",
    "                                    cv2.BORDER_CONSTANT, value=padding)\n",
    "\n",
    "    # Resize the padded image to the final output size using area interpolation (good for shrinking).\n",
    "    resized_img = cv2.resize(padded_img, (output_size, output_size), interpolation=cv2.INTER_AREA)\n",
    "    return resized_img\n",
    "\n",
    "\n",
    "def aug(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies a set of random augmentations to a grayscale image.\n",
    "\n",
    "    These augmentations are typically used during training to increase the diversity\n",
    "    of the dataset and improve the model's generalization capabilities.\n",
    "    Applied augmentations include: random horizontal flip, random 90/180/270 degree rotations,\n",
    "    random inversion (negative image), random brightness adjustment, and random contrast adjustment.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): The input grayscale image (NumPy array).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The augmented image.\n",
    "    \"\"\"\n",
    "    # Random horizontal flip: Flips the image left-right with 50% probability.\n",
    "    if random.random() > 0.5:\n",
    "        image = cv2.flip(image, 1)\n",
    "\n",
    "    # Random 90/180/270 degree rotation: Rotates the image with 75% probability.\n",
    "    # None means no rotation.\n",
    "    rot_code = random.choice([None, cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_180, cv2.ROTATE_90_COUNTERCLOCKWISE])\n",
    "    if rot_code is not None:\n",
    "        image = cv2.rotate(image, rot_code)\n",
    "\n",
    "    # Random inversion (negative image): Inverts pixel values (255-x) with 50% probability.\n",
    "    if random.random() > 0.5:\n",
    "        image = 255 - image  # Invert pixel values for 8-bit grayscale (0-255 range)\n",
    "\n",
    "    # Random brightness adjustment: Adjusts brightness by a random factor (0.7 to 1.3) with 50% probability.\n",
    "    # `cv2.convertScaleAbs` handles clipping to 0-255 and converts to absolute values.\n",
    "    if random.random() > 0.5:\n",
    "        alpha = random.uniform(0.7, 1.3)  # Factor for brightness (1.0 is no change)\n",
    "        image = cv2.convertScaleAbs(image, alpha=alpha, beta=0)  # beta is offset, set to 0\n",
    "\n",
    "    # Random contrast adjustment: Adjusts contrast by a random factor (0.7 to 1.3) with 50% probability.\n",
    "    if random.random() > 0.5:\n",
    "        alpha = random.uniform(0.7, 1.3)  # Factor for contrast\n",
    "        # `cv2.convertScaleAbs` (alpha, beta) performs: output = alpha * input + beta\n",
    "        image = cv2.convertScaleAbs(image, alpha=alpha, beta=0)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def calculate_mean_std(image_folder: str, img_size: Tuple[int, int]) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Calculates the mean and standard deviation of pixel values across all images\n",
    "    within a specified folder.\n",
    "\n",
    "    This function iterates through all PNG images in the given directory,\n",
    "    resizes them (if necessary), flattens their pixel values, and then computes\n",
    "    the overall mean and standard deviation. It includes detailed logging for\n",
    "    files that cannot be read or processed. These calculated values are crucial\n",
    "    for standardizing the dataset for neural network input.\n",
    "\n",
    "    Args:\n",
    "        image_folder (str): The path to the directory containing the images (e.g., 'data/Train/prep_images').\n",
    "        img_size (Tuple[int, int]): The target size (height, width) to which images should be\n",
    "                                     resized before calculating pixel values. This ensures\n",
    "                                     consistency in the calculation.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: A tuple containing the mean and standard deviation of all\n",
    "                             processed pixel values. Returns (0.0, 1.0) if no valid\n",
    "                             pixel values are found to prevent division by zero errors.\n",
    "    \"\"\"\n",
    "    pixel_values = []\n",
    "    print(f\"\\nCalculating mean and standard deviation for images in: {image_folder}\")\n",
    "    # List all PNG files in the specified folder.\n",
    "    image_files = [f for f in os.listdir(image_folder) if f.endswith('.png')]\n",
    "\n",
    "    total_files = len(image_files)\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "\n",
    "    # Iterate through each image file with a progress bar.\n",
    "    for img_file in tqdm(image_files, desc=\"Processing images for mean/std\"):\n",
    "        img_path = os.path.join(image_folder, img_file)\n",
    "        try:\n",
    "            # Preliminary check for empty or corrupted files by checking file size.\n",
    "            if os.path.getsize(img_path) == 0:\n",
    "                print(f\"Warning: Skipped empty file {img_file}\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            # Read the image as grayscale directly using OpenCV.\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is None:\n",
    "                # cv2.imread returns None if it fails to read the file (e.g., corrupted, wrong format).\n",
    "                print(f\"Warning: Could not read or process {img_file}. Skipping.\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            # Resize the image to the target size if its dimensions don't match.\n",
    "            # This ensures all images contribute equally to the mean/std calculation,\n",
    "            # regardless of their initial cropped size variability.\n",
    "            if img.shape[0] != img_size[0] or img.shape[1] != img_size[1]:\n",
    "                img = cv2.resize(img, img_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            # Flatten the 2D image array into a 1D array of pixel values and extend the list.\n",
    "            pixel_values.extend(img.flatten())\n",
    "            processed_count += 1\n",
    "        except:\n",
    "            # Catch any unexpected errors during image processing and log a warning.\n",
    "            skipped_count += 1\n",
    "\n",
    "    # Convert the list of pixel values to a NumPy array for efficient calculation.\n",
    "    pixel_values = np.array(pixel_values, dtype=np.float32)\n",
    "\n",
    "    # Print a summary of the mean/std calculation process.\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Mean/Std Calculation Summary for {image_folder}:\")\n",
    "    print(f\"Total files found: {total_files}\")\n",
    "    print(f\"Successfully processed files: {processed_count}\")\n",
    "    print(f\"Skipped files: {skipped_count}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # If no valid pixel values were found (e.g., all files skipped), return default\n",
    "    # values to prevent division by zero errors in standardization.\n",
    "    if len(pixel_values) == 0:\n",
    "        print(\"No valid pixel values found for mean/std calculation. Returning default values (0.0, 1.0).\")\n",
    "        return 0.0, 1.0\n",
    "\n",
    "    # Calculate the mean and standard deviation of the collected pixel values.\n",
    "    mean_val = np.mean(pixel_values)\n",
    "    std_val = np.std(pixel_values)\n",
    "\n",
    "    # Safeguard against zero standard deviation (e.g., if all pixels have the same value).\n",
    "    # In such cases, replace std_val with 1.0 to prevent division by zero during standardization.\n",
    "    if std_val < 1e-7:  # Using a small epsilon to check for near-zero std\n",
    "        std_val = 1.0\n",
    "\n",
    "    print(f\"Mean pixel value of the training set: {mean_val:.4f}\")\n",
    "    print(f\"Standard deviation of the training set pixel values: {std_val:.4f}\")\n",
    "    return mean_val, std_val"
   ],
   "id": "351233dc439901ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load trained model",
   "id": "b4c200ed46db2afa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "from typing import Optional  # Import for Optional type hint\n",
    "\n",
    "# Assuming correct import path for your custom AttentionLayer\n",
    "from src.Models.AttentionLayer import SpatialAttention\n",
    "\n",
    "def load_trained_model(model_save_path: str) -> Optional[keras.Model]:\n",
    "    \"\"\"\n",
    "    Attempts to load a pre-trained Keras model from a specified path.\n",
    "\n",
    "    This utility function checks for the existence of the model file and\n",
    "    handles potential errors during the loading process. It's designed\n",
    "    to facilitate resuming training from a checkpoint or using a saved model\n",
    "    for evaluation.\n",
    "\n",
    "    Args:\n",
    "        model_save_path (str): The file path to the saved Keras model\n",
    "                               (e.g., '.keras', '.h5', or SavedModel directory).\n",
    "\n",
    "    Returns:\n",
    "        Optional[tf.keras.Model]: The loaded Keras model instance if successful.\n",
    "                                  Returns None if the model file does not exist\n",
    "                                  or if an error occurs during loading.\n",
    "    \"\"\"\n",
    "    # Check if the model file exists at the specified path.\n",
    "    if os.path.exists(model_save_path):\n",
    "        print(f\"\\nLoading existing model from '{model_save_path}' to continue training...\")\n",
    "        try:\n",
    "            # Load the Keras model.\n",
    "            # `custom_objects` is crucial for loading models that use custom layers,\n",
    "            # like your SpatialAttention layer.\n",
    "            loaded_model = load_model(\n",
    "                model_save_path, custom_objects={'SpatialAttention': SpatialAttention}\n",
    "            )\n",
    "            print(\"Model loaded successfully.\")\n",
    "            return loaded_model\n",
    "        except Exception as e:\n",
    "            # Catch any exceptions that occur during the model loading process.\n",
    "            # This can happen if the file is corrupted, the custom object is not found, etc.\n",
    "            print(f\"Error during model loading: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()  # Print the full stack trace for debugging purposes\n",
    "\n",
    "            # If loading fails, treat it as if no model was found, and suggest starting anew.\n",
    "            print(\"Model loading failed. Proceeding with training a new model.\")\n",
    "            return None  # Indicate failure to load by returning None\n",
    "    else:\n",
    "        # Inform the user if the model file does not exist.\n",
    "        print(f\"\\nModel not found at '{model_save_path}'. Starting training from scratch.\")\n",
    "        return None  # Indicate that no existing model was found"
   ],
   "id": "78bae0d28581f478"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training pipeline",
   "id": "10a9c035d841e3bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from typing import Optional, Tuple  # Import for type hinting\n",
    "\n",
    "\n",
    "def train_model(model: tf.keras.Model,\n",
    "                train_dataset: tf.data.Dataset,\n",
    "                epochs: int = 100,\n",
    "                validation_dataset: Optional[tf.data.Dataset] = None,\n",
    "                model_save_path: str = 'best_age_prediction_model.keras') -> Tuple[tf.keras.Model, tf.keras.callbacks.History]:\n",
    "    \"\"\"\n",
    "    Trains the provided Keras model using specified datasets and callbacks.\n",
    "\n",
    "    This function orchestrates the training process, including saving the best model,\n",
    "    implementing early stopping to prevent overfitting, and dynamically adjusting\n",
    "    the learning rate during training based on validation performance.\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): The already compiled Keras model to be trained.\n",
    "        train_dataset (tf.data.Dataset): The TensorFlow Dataset for training.\n",
    "                                         It should yield (input_data, labels) tuples,\n",
    "                                         where input_data matches the model's input\n",
    "                                         (e.g., preprocessed images).\n",
    "        epochs (int, optional): The maximum number of training epochs. Training\n",
    "                                may stop earlier due to EarlyStopping. Defaults to 50.\n",
    "        validation_dataset (tf.data.Dataset, optional): The TensorFlow Dataset for validation.\n",
    "                                                        Used to monitor performance and guide\n",
    "                                                        EarlyStopping and ReduceLROnPlateau.\n",
    "                                                        Should have the same structure as `train_dataset`.\n",
    "                                                        Defaults to None.\n",
    "        model_save_path (str, optional): The file path where the best performing model\n",
    "                                         (based on validation MAE) will be saved.\n",
    "                                         Defaults to 'best_age_prediction_model.keras'.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[tf.keras.Model, tf.keras.callbacks.History]: A tuple containing:\n",
    "            - tf.keras.Model: The trained model instance. Its weights will be restored\n",
    "                              to the best observed performance during training if EarlyStopping\n",
    "                              with `restore_best_weights=True` is used.\n",
    "            - tf.keras.callbacks.History: An object containing the history of loss and\n",
    "                                           metric values during training.\n",
    "    \"\"\"\n",
    "    print(f\"\\nStarting model training for {epochs} epochs...\")\n",
    "\n",
    "    # Define Keras Callbacks for enhanced training control and regularization.\n",
    "    callbacks = [\n",
    "        # ModelCheckpoint: Saves the best model weights observed during training.\n",
    "        # It monitors 'val_mae' (Mean Absolute Error on validation set) and saves\n",
    "        # only if the monitored metric improves (mode='min' for MAE).\n",
    "        ModelCheckpoint(\n",
    "            filepath=model_save_path,\n",
    "            monitor='val_mae' if validation_dataset else 'mae', # Monitor validation MAE if validation set exists, else training MAE\n",
    "            save_best_only=True, # Only save the model when validation MAE improves\n",
    "            mode='min', # 'min' mode means lower is better for the monitored metric (MAE)\n",
    "            verbose=1 # Display messages when a better model is saved\n",
    "        ),\n",
    "        # EarlyStopping: Halts training if the monitored metric does not improve\n",
    "        # for a specified number of epochs (patience). This prevents overfitting.\n",
    "        EarlyStopping(\n",
    "            monitor='val_mae' if validation_dataset else 'mae', # Monitor validation MAE\n",
    "            patience=10, # Number of epochs with no improvement after which training will be stopped\n",
    "            mode='min', # 'min' mode for MAE\n",
    "            verbose=1, # Display messages when early stopping is triggered\n",
    "            restore_best_weights=True # Restore model weights from the epoch with the best monitored value\n",
    "        ),\n",
    "        # ReduceLROnPlateau: Dynamically reduces the learning rate when a metric\n",
    "        # has stopped improving. This helps the model to converge more finely.\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_mae' if validation_dataset else 'mae', # Monitor validation MAE\n",
    "            factor=0.5, # Factor by which the learning rate will be reduced (new_lr = lr * factor)\n",
    "            patience=5, # Number of epochs with no improvement after which the learning rate will be reduced\n",
    "            min_lr=1e-6, # Lower bound on the learning rate\n",
    "            mode='min', # 'min' mode for MAE\n",
    "            verbose=1 # Display messages when learning rate is reduced\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Start the model training process.\n",
    "    # The 'initial_epoch=5' parameter means training will start from epoch 5.\n",
    "    # This might be useful if resuming training and wanting to skip initial already-converged epochs\n",
    "    # or to allow callbacks to become active after a few epochs.\n",
    "    # If resuming from a loaded model, Keras automatically handles the initial epoch correctly.\n",
    "    # It's important to ensure this parameter aligns with the overall training strategy.\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_dataset,\n",
    "        callbacks=callbacks,\n",
    "        initial_epoch=0\n",
    "    )\n",
    "    print(\"\\nModel training completed.\")\n",
    "    return model, history"
   ],
   "id": "6a8ad741e27ace46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# src/pipelines/training_pipeline.py\n",
    "\n",
    "import tensorflow as tf\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# Assuming these imports are correctly resolved in your project structure\n",
    "from src.Models.AgePredictionModel import AgePredictionModel  # Model architecture definition\n",
    "from src.dataset.RadiographDataset import RadiographDatasetBuilder  # Dataset loading and preprocessing\n",
    "from src.plot.PlotHistory import plot_training_history  # Utility for plotting training history\n",
    "from src.training.RadiographTraining import train_model  # Core model training function\n",
    "\n",
    "def training_pipeline(base_dir_train: str, label_train: str,\n",
    "                      base_dir_val: str, label_val: str,\n",
    "                      img_sizes: int = 256,\n",
    "                      mean_pixel_value: float = 0.0, std_pixel_value: float = 1.0,\n",
    "                      existing_model: Optional[tf.keras.Model] = None,\n",
    "                      epochs: int = 100,\n",
    "                      model_save_path: str = 'best_age_prediction_model_standalone.keras',\n",
    "                      learning_rate: float = 0.0005) -> Optional[tf.keras.Model]:\n",
    "    \"\"\"\n",
    "    Orchestrates the entire model training process.\n",
    "\n",
    "    This pipeline handles the creation and preparation of training and validation datasets,\n",
    "    the initialization or loading of the deep learning model, and the execution of the\n",
    "    training loop with specified parameters and callbacks. It also plots the training history.\n",
    "\n",
    "    Args:\n",
    "        base_dir_train (str): The base directory for the training dataset, containing images and labels.\n",
    "        label_train (str): The filename of the CSV file containing labels for the training set,\n",
    "                           relative to `base_dir_train`.\n",
    "        base_dir_val (str): The base directory for the validation dataset, containing images and labels.\n",
    "        label_val (str): The filename of the CSV file containing labels for the validation set,\n",
    "                         relative to `base_dir_val`.\n",
    "        img_sizes (int, optional): The target side length for input images (e.g., 256 for 256x256).\n",
    "                                   Defaults to 256.\n",
    "        mean_pixel_value (float, optional): The mean pixel value calculated from the training set.\n",
    "                                            Used for standardizing both training and validation images.\n",
    "                                            Defaults to 0.0.\n",
    "        std_pixel_value (float, optional): The standard deviation of pixel values from the training set.\n",
    "                                           Used for standardizing both training and validation images.\n",
    "                                           Defaults to 1.0.\n",
    "        existing_model (tf.keras.Model, optional): An already initialized or loaded Keras model.\n",
    "                                                   If provided, training continues from this model's state.\n",
    "                                                   If None, a new `AgePredictionModel` is created.\n",
    "                                                   Defaults to None.\n",
    "        epochs (int, optional): The maximum number of epochs for which to train the model.\n",
    "                                Actual epochs may be fewer due to early stopping. Defaults to 50.\n",
    "        model_save_path (str, optional): The file path where the best performing model\n",
    "                                         (based on validation metrics) will be saved during training.\n",
    "                                         Defaults to 'best_age_prediction_model_standalone.keras'.\n",
    "        learning_rate (float, optional): The initial learning rate for the optimizer if a new model\n",
    "                                         is created and compiled. Defaults to 0.0005.\n",
    "\n",
    "    Returns:\n",
    "        Optional[tf.keras.Model]: The trained Keras model instance. Returns None if an error\n",
    "                                  occurs during the training process.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Dataset Preparation ---\n",
    "    # Initialize RadiographDatasetBuilder for the training set.\n",
    "    # It will load image paths and labels, and apply specified preprocessing (including standardization).\n",
    "    builder_train = RadiographDatasetBuilder(\n",
    "        base_dir=base_dir_train,\n",
    "        label_csv=label_train,  # Expects label_csv relative to base_dir\n",
    "        img_size=(img_sizes, img_sizes),\n",
    "        batch_size=16,\n",
    "        mean_pixel_value=mean_pixel_value,\n",
    "        std_pixel_value=std_pixel_value\n",
    "    )\n",
    "    # Build the TensorFlow Dataset for training. `train=True` implies shuffling.\n",
    "    train_dataset = builder_train.build(train=True)\n",
    "\n",
    "    # Initialize RadiographDatasetBuilder for the validation set.\n",
    "    # It uses the same standardization parameters as the training set for consistency.\n",
    "    builder_val = RadiographDatasetBuilder(\n",
    "        base_dir=base_dir_val,\n",
    "        label_csv=label_val,  # Expects label_csv relative to base_dir\n",
    "        img_size=(img_sizes, img_sizes),\n",
    "        batch_size=16,\n",
    "        mean_pixel_value=mean_pixel_value,\n",
    "        std_pixel_value=std_pixel_value\n",
    "    )\n",
    "    # Build the TensorFlow Dataset for validation. `train=False` implies no shuffling.\n",
    "    val_dataset = builder_val.build(train=False)\n",
    "\n",
    "    # Print the calculated dataset sizes in terms of batches.\n",
    "    print(f\"\\nTrain dataset size (batches): {tf.data.experimental.cardinality(train_dataset).numpy()}\")\n",
    "    print(f\"Validation dataset size (batches): {tf.data.experimental.cardinality(val_dataset).numpy()}\")\n",
    "\n",
    "    # --- Model Initialization or Loading ---\n",
    "    model_to_train: tf.keras.Model = None\n",
    "    if existing_model is not None:\n",
    "        # If an existing model is provided, use it directly.\n",
    "        # This allows resuming training or fine-tuning.\n",
    "        model_to_train = existing_model\n",
    "        print(\"\\nUsing existing model for training.\")\n",
    "        # Note: If you want to recompile the existing model with new learning_rate or metrics,\n",
    "        # you would do it here (e.g., model_to_train.compile(...)).\n",
    "        # Keras's load_model usually preserves compilation state, but recompiling can override.\n",
    "    else:\n",
    "        # If no existing model is provided, create a new AgePredictionModel instance.\n",
    "        model_builder = AgePredictionModel(img_size=(img_sizes, img_sizes))\n",
    "        # Compile the newly created model with the specified learning rate.\n",
    "        model_builder.compile_model(learning_rate=learning_rate)\n",
    "        model_to_train = model_builder.model\n",
    "        print(\"\\nCreating and compiling a new model for training.\")\n",
    "\n",
    "    # Display the summary of the model (whether new or existing).\n",
    "    print(\"\\nModel Summary:\")\n",
    "    model_to_train.summary()\n",
    "\n",
    "    # --- Model Training Execution ---\n",
    "    print(\"\\nStarting training:\")\n",
    "    try:\n",
    "        # Call the core `train_model` function to execute the training loop.\n",
    "        # This function handles epochs, validation, and callbacks (e.g., ModelCheckpoint, EarlyStopping).\n",
    "        trained_model, history = train_model(\n",
    "            model=model_to_train,  # The model to be trained\n",
    "            train_dataset=train_dataset,\n",
    "            validation_dataset=val_dataset,\n",
    "            epochs=epochs,\n",
    "            model_save_path=model_save_path\n",
    "        )\n",
    "        # Plot the training and validation loss/metrics history.\n",
    "        plot_training_history(history, save_path='training_history_plots.png')\n",
    "\n",
    "        print(f\"Training history keys: {history.history.keys()}\")\n",
    "        print(f\"Model trained and saved to '{model_save_path}'.\")\n",
    "        return trained_model  # Return the trained model instance\n",
    "    except Exception as e:\n",
    "        # Catch any exceptions during training and print a detailed traceback.\n",
    "        print(f\"Error during model training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()  # Print the full stack trace for debugging\n",
    "        return None  # Indicate that training failed by returning None"
   ],
   "id": "9db6983d51c2cf42"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Plot functions",
   "id": "54b1910895cd051"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### LIME",
   "id": "4862070b9d166e89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def explain_prediction_lime(model, image_tensor, original_image_for_display, std_val, mean_val, top_labels=5, save_path=None):\n",
    "    \"\"\"\n",
    "    Explains a single prediction using LIME (Local Interpretable Model-agnostic Explanations).\n",
    "    Requires 'lime' library to be installed (pip install lime).\n",
    "    This is a conceptual structure; actual implementation needs careful handling of\n",
    "    image preprocessing for LIME's internal perturbation.\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): The trained Keras model.\n",
    "        image_tensor (tf.Tensor): The preprocessed input image tensor (1, H, W, C) for the model.\n",
    "        original_image_for_display (np.array): The original (denormalized, 0-1) image for display.\n",
    "        std_val (float): Standard deviation used for internal image preprocessing by LIME.\n",
    "        mean_val (float): Mean value used for internal image preprocessing by LIME.\n",
    "        top_labels (int): Number of labels/segments to highlight (for regression, this might be adjusted).\n",
    "        save_path (str, optional): Path to save the plot.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from lime import lime_image\n",
    "    except ImportError:\n",
    "        print(\"LIME library not installed. Please run 'pip install lime'.\")\n",
    "        return\n",
    "\n",
    "    explainer = lime_image.LimeImageExplainer()\n",
    "\n",
    "    # Define a prediction function for LIME.\n",
    "    # LIME expects a function that takes numpy array of images (0-255 RGB)\n",
    "    # and returns predictions for each image.\n",
    "    # Your model expects preprocessed (standardized) grayscale images.\n",
    "    def predict_fn(images):\n",
    "        # images will be (N, H, W, 3) from LIME, need to convert to (N, H, W, 1) and preprocess\n",
    "        # Convert RGB to grayscale and then apply your model's preprocessing\n",
    "        grayscale_images = np.mean(images, axis=-1, keepdims=True) # Simple grayscale conversion\n",
    "        preprocessed_images = (grayscale_images - mean_val) / std_val\n",
    "        return model.predict(preprocessed_images)\n",
    "\n",
    "    # LIME works best for classification. For regression, you might need to\n",
    "    # treat it as a \"class\" representing the predicted value or bin predictions.\n",
    "    # For a simple regression, LIME will try to explain the single output value.\n",
    "    # The image should be 0-1 or 0-255 range for LIME explainer.\n",
    "    # original_image_for_display should be 0-1 here, scale to 0-255 if LIME expects it\n",
    "    lime_image_input = (original_image_for_display * 255).astype(np.uint8)\n",
    "    if len(lime_image_input.shape) == 2: # LIME expects 3 channels\n",
    "        lime_image_input = cv2.cvtColor(lime_image_input, cv2.COLOR_GRAY2RGB)\n",
    "    elif lime_image_input.shape[-1] == 1:\n",
    "        lime_image_input = cv2.cvtColor(lime_image_input, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    # For regression, LIME will try to explain the single output neuron.\n",
    "    # 'top_labels' might not be directly applicable as it is for classification.\n",
    "    # LIME's 'explanation' object for regression provides 'image_and_mask'.\n",
    "    try:\n",
    "        explanation = explainer.explain_instance(\n",
    "            lime_image_input, # Original image (denormalized)\n",
    "            predict_fn,\n",
    "            top_labels=1, # For regression, we care about the main output\n",
    "            hide_color=0, # Color to use for masked regions\n",
    "            num_samples=1000 # Number of perturbed samples\n",
    "        )\n",
    "\n",
    "        # Get image and mask for the top feature (which is the regression output importance)\n",
    "        temp, mask = explanation.get_image_and_mask(\n",
    "            explanation.top_labels[0], # The single regression output 'label'\n",
    "            positive_only=True, negative_only=False, num_features=top_labels, hide_rest=True\n",
    "        )\n",
    "\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.imshow(temp / 255.0) # Scale back to 0-1 for matplotlib\n",
    "        plt.imshow(mask, alpha=0.5, cmap='jet') # Overlay the mask\n",
    "        plt.title(f'LIME Explanation for Predicted Age')\n",
    "        plt.axis('off')\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300)\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during LIME explanation: {e}\")\n",
    "        print(\"Ensure 'original_image_for_display' is properly denormalized (0-1 or 0-255) and LIME's predict_fn is correct for your model's input/output.\")\n"
   ],
   "id": "de2a0a2fb2a79720"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Activation layer",
   "id": "c2dc0e3334df7a59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from keras import Model\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def visualize_layer_activations(model, image_tensor, layer_names=None, num_filters_to_show=8, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualizes the activations of specified layers for a given input image.\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): The trained Keras model.\n",
    "        image_tensor (tf.Tensor): The preprocessed input image tensor (batch_size, H, W, C).\n",
    "        layer_names (list, optional): A list of layer names whose activations should be visualized.\n",
    "                                      If None, attempts to find representative conv layers.\n",
    "        num_filters_to_show (int): Number of filters to display per layer.\n",
    "        save_path (str, optional): Path to save the plot. If None, displays the plot.\n",
    "    \"\"\"\n",
    "    if layer_names is None:\n",
    "        # Try to find some representative convolutional layers automatically\n",
    "        layer_names = []\n",
    "        for layer in model.layers:\n",
    "            if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.SeparableConv2D)) and \\\n",
    "                    'attention' not in layer.name and 'output' not in layer.name:  # Avoid attention map output itself\n",
    "                layer_names.append(layer.name)\n",
    "        # Select a few, e.g., first, middle, last few conv layers\n",
    "        if len(layer_names) > 3:\n",
    "            selected_layers = [layer_names[0],\n",
    "                               layer_names[len(layer_names) // 2],\n",
    "                               layer_names[-1]]\n",
    "            layer_names = selected_layers\n",
    "        elif len(layer_names) > 0:\n",
    "            layer_names = layer_names  # Use all if few\n",
    "        else:\n",
    "            print(\"No suitable convolutional layers found for activation visualization.\")\n",
    "            return\n",
    "\n",
    "    # Create a model that outputs the activations of the specified layers\n",
    "    outputs = [model.get_layer(name).output for name in layer_names]\n",
    "    activation_model = Model(inputs=model.input, outputs=outputs)\n",
    "\n",
    "    # Get activations\n",
    "    activations = activation_model.predict(image_tensor)\n",
    "\n",
    "    if not isinstance(activations, list):  # If only one layer was specified\n",
    "        activations = [activations]\n",
    "\n",
    "    plt.figure(figsize=(num_filters_to_show * 1.5, len(layer_names) * 1.5 + 2))  # Adjust figure size\n",
    "\n",
    "    for i, layer_activation in enumerate(activations):\n",
    "        layer_name = layer_names[i]\n",
    "        # activations are (1, H, W, Channels) -> take first image and convert to (H, W, Channels)\n",
    "        activation_map = layer_activation[0]\n",
    "\n",
    "        # Determine how many filters to show (max available or num_filters_to_show)\n",
    "        n_filters = min(activation_map.shape[-1], num_filters_to_show)\n",
    "\n",
    "        for f in range(n_filters):\n",
    "            ax = plt.subplot(len(layer_names), num_filters_to_show, i * num_filters_to_show + f + 1)\n",
    "            plt.imshow(activation_map[:, :, f], cmap='viridis')\n",
    "            plt.axis('off')\n",
    "            if f == 0:  # Only label the first filter's column\n",
    "                ax.set_title(f'Layer: {layer_name}\\nFilter {f + 1}', fontsize=8)\n",
    "            else:\n",
    "                ax.set_title(f'Filter {f + 1}', fontsize=8)\n",
    "\n",
    "    plt.suptitle('Layer Activations for Input Image', y=0.98, fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to prevent title overlap\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ],
   "id": "2e21a38c9e9613bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluation",
   "id": "12dd0695fea0d370"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# src/plot/plot_evaluation.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_eval(errors_list, mae_months, true_months_list, pred_months_list):\n",
    "    \"\"\"\n",
    "        Generates and saves a series of plots to analyze the performance of\n",
    "        the age prediction model.\n",
    "\n",
    "        Args:\n",
    "            errors_list (list/array): List of absolute prediction errors (months).\n",
    "            mae_months (float): Overall Mean Absolute Error (months).\n",
    "            true_months_list (list/array): List of true ages (months).\n",
    "            pred_months_list (list/array): List of predicted ages (months).\n",
    "        \"\"\"\n",
    "    plt.figure(figsize=(18, 12))\n",
    "\n",
    "    # 1. Distribution of absolute errors\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.histplot(errors_list, kde=True, bins=30)\n",
    "    plt.axvline(mae_months, color='r', linestyle='--', label=f'MAE: {mae_months:.2f} months')\n",
    "    plt.xlabel('Absolute Error (months)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Age Prediction Errors')\n",
    "    plt.legend()\n",
    "\n",
    "    # 2. Scatter plot: true age vs predicted age\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(true_months_list, pred_months_list, alpha=0.5)\n",
    "    # Ideal y=x line\n",
    "    min_val = min(min(true_months_list), min(pred_months_list))\n",
    "    max_val = max(max(true_months_list), max(pred_months_list))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='Ideal')\n",
    "    plt.xlabel('True Age (months)')\n",
    "    plt.ylabel('Predicted Age (months)')\n",
    "    plt.title('True Age vs Predicted Age')\n",
    "    plt.legend()\n",
    "\n",
    "    # 3. Box plot of errors by age group\n",
    "    plt.subplot(2, 2, 3)\n",
    "\n",
    "    # Base age bins in months up to 20 years\n",
    "    # These represent the lower bounds of the intervals for standard labels\n",
    "    standard_age_boundaries_months = [0, 36, 72, 120, 180]  # 0, 3, 6, 10, 15 years\n",
    "    standard_age_labels = ['0-3', '3-6', '6-10', '10-15']  # Labels for these intervals\n",
    "\n",
    "    # Calculate the maximum true age in months from the data\n",
    "    max_true_age_in_months = max(true_months_list)\n",
    "\n",
    "    current_age_bins = list(standard_age_boundaries_months)\n",
    "    current_age_labels = list(standard_age_labels)\n",
    "\n",
    "    # Determine the upper bound for the last interval\n",
    "    # The last standard boundary is 180 months (15 years)\n",
    "    # We want the next bin to start at 15 years (180 months) and end at max_true_age,\n",
    "    # or at 20 years if max_true_age is less than 20.\n",
    "\n",
    "    # Calculate the upper limit for the final bin, ensuring it covers the max age\n",
    "    # It should be at least 20 years (240 months) if data goes up to there,\n",
    "    # or the max_true_age + a small margin to ensure inclusion with right=False.\n",
    "    final_bin_upper_bound = max(240, int(max_true_age_in_months) + 1)  # Ensure it goes at least to 20 years + 1 month\n",
    "    # and covers max data age\n",
    "\n",
    "    if final_bin_upper_bound > current_age_bins[-1]:  # If max age is > 15 years (180 months)\n",
    "        current_age_bins.append(final_bin_upper_bound)\n",
    "\n",
    "        # Determine the start year for the last label (e.g., 15 for 15-X)\n",
    "        start_year_for_last_bin = int(current_age_bins[-2] / 12)\n",
    "        # Determine the end year for the last label (round up to nearest year for clarity)\n",
    "        end_year_for_last_bin = int(np.ceil(max_true_age_in_months / 12.0))\n",
    "\n",
    "        # Ensure the end year is at least 20 if the data goes up to 20 years\n",
    "        if max_true_age_in_months >= 240:  # If data includes 20 years or more\n",
    "            end_year_for_last_bin = max(20, end_year_for_last_bin)  # Ensure it's at least 20\n",
    "\n",
    "        # Create the label for the last bin\n",
    "        current_age_labels.append(f'{start_year_for_last_bin}-{end_year_for_last_bin}')\n",
    "\n",
    "    # Create a pandas Series for age\n",
    "    age_series = pd.Series(true_months_list)\n",
    "    # Using 'right=False' for [a, b) intervals.\n",
    "    binned_ages = pd.cut(age_series, bins=current_age_bins, labels=current_age_labels, right=False)\n",
    "\n",
    "    # Create a DataFrame with age and errors\n",
    "    error_df = pd.DataFrame({\n",
    "        'Age_Group': binned_ages,\n",
    "        'Absolute_Error': errors_list\n",
    "    })\n",
    "\n",
    "    # Filter out NaNs if any, which might occur if data falls outside of the determined bins\n",
    "    error_df = error_df.dropna(subset=['Age_Group'])\n",
    "\n",
    "    # Ensure all labels appear in the correct order, even if some bins are empty\n",
    "    error_df['Age_Group'] = pd.Categorical(error_df['Age_Group'], categories=current_age_labels, ordered=True)\n",
    "\n",
    "    # Box plot\n",
    "    sns.boxplot(x='Age_Group', y='Absolute_Error', data=error_df)\n",
    "    plt.xlabel('Age Group (years)')\n",
    "    plt.ylabel('Absolute Error (months)')\n",
    "    plt.title('Distribution of Errors by Age Group')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # 4. Heatmap: error based on true vs predicted age\n",
    "    plt.subplot(2, 2, 4)\n",
    "\n",
    "    # Convert months to years for readability\n",
    "    true_years_array = np.array(true_months_list) / 12\n",
    "    pred_years_array = np.array(pred_months_list) / 12\n",
    "\n",
    "    # Define base bins for years (e.g., 0-4, 4-8, 8-12, 12-16, 16-20)\n",
    "    base_bins_years = np.arange(0, 21, 4)  # Up to 20 years (20 is the upper bound of the last base bin)\n",
    "\n",
    "    # Determine the effective upper bound for bins based on actual max data\n",
    "    max_data_year_heatmap = max(np.max(true_years_array), np.max(pred_years_array))\n",
    "\n",
    "    # Ensure the last bin in bins_years covers up to max_data_year without a '+' label\n",
    "    current_bins_years = list(base_bins_years)\n",
    "\n",
    "    if max_data_year_heatmap > base_bins_years[-1]:\n",
    "        # Extend the last bin's upper limit to include max_data_year\n",
    "        # Add 1 to ensure pd.histogram2d includes values up to max_data_year (similar to right=False in pd.cut)\n",
    "        current_bins_years.append(max_data_year_heatmap + 1)\n",
    "\n",
    "    # Calculate 2D distributions with np.histogram2d\n",
    "    heatmap, xedges, yedges = np.histogram2d(true_years_array, pred_years_array,\n",
    "                                             bins=[current_bins_years, current_bins_years])\n",
    "\n",
    "    # Normalize by row to show the distribution of predictions for each true age group\n",
    "    row_sums = heatmap.sum(axis=1, keepdims=True)\n",
    "    heatmap_norm = np.divide(heatmap, row_sums, out=np.zeros_like(heatmap), where=row_sums != 0)\n",
    "\n",
    "    # Create labels for the heatmap axes dynamically, ensuring no '+' sign\n",
    "    x_labels = []\n",
    "    y_labels = []\n",
    "\n",
    "    for i in range(len(xedges) - 1):\n",
    "        lower_bound = int(xedges[i])\n",
    "        upper_bound = int(xedges[i + 1])\n",
    "        # For the last bin, if it was extended, label it with the actual range\n",
    "        if i == len(xedges) - 2 and upper_bound > base_bins_years[-1]:\n",
    "            x_labels.append(f'{lower_bound}-{int(np.floor(max_data_year_heatmap))}')\n",
    "        else:\n",
    "            x_labels.append(f'{lower_bound}-{upper_bound}')\n",
    "\n",
    "    for i in range(len(yedges) - 1):\n",
    "        lower_bound = int(yedges[i])\n",
    "        upper_bound = int(yedges[i + 1])\n",
    "        if i == len(yedges) - 2 and upper_bound > base_bins_years[-1]:\n",
    "            y_labels.append(f'{lower_bound}-{int(np.floor(max_data_year_heatmap))}')\n",
    "        else:\n",
    "            y_labels.append(f'{lower_bound}-{upper_bound}')\n",
    "\n",
    "    sns.heatmap(heatmap_norm, cmap='YlGnBu', annot=True, fmt=\".2f\",\n",
    "                xticklabels=x_labels,\n",
    "                yticklabels=y_labels)\n",
    "    plt.xlabel('Predicted Age (years)')\n",
    "    plt.ylabel('True Age (years)')\n",
    "    plt.title('Distribution of Predictions by Age Group (Row Normalized)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('age_prediction_analysis.png', dpi=300)\n",
    "    plt.show()"
   ],
   "id": "d025163decd987d8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Filters",
   "id": "1981861ff68e6d6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from keras import Model\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "def visualize_filters(model, layer_name, filter_indices=None, iterations=50, learning_rate=0.1, save_path=None):\n",
    "    \"\"\"\n",
    "    Generates and visualizes patterns that maximally activate specific filters in a given layer.\n",
    "    This is a simplified conceptual example; robust implementations often use dedicated libraries.\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): The trained Keras model.\n",
    "        layer_name (str): The name of the convolutional layer to visualize filters from.\n",
    "        filter_indices (list, optional): List of specific filter indices to visualize.\n",
    "                                        If None, visualize a few representative filters.\n",
    "        iterations (int): Number of gradient ascent steps.\n",
    "        learning_rate (float): Step size for gradient ascent.\n",
    "        save_path (str, optional): Path to save the plot.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        layer = model.get_layer(layer_name)\n",
    "        if not isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.SeparableConv2D)):\n",
    "            print(f\"Layer '{layer_name}' is not a convolutional layer. Cannot visualize filters.\")\n",
    "            return\n",
    "    except ValueError:\n",
    "        print(f\"Error: Layer '{layer_name}' not found in the model.\")\n",
    "        return\n",
    "\n",
    "    # Select filters to visualize\n",
    "    if filter_indices is None:\n",
    "        n_filters_total = layer.filters  # Number of filters in the layer\n",
    "        filter_indices = np.linspace(0, n_filters_total - 1, min(8, n_filters_total), dtype=int)\n",
    "\n",
    "    # Placeholder for generated images\n",
    "    generated_images = []\n",
    "\n",
    "    for filter_index in filter_indices:\n",
    "        # We want to maximize the activation of a specific filter\n",
    "        # at a specific spatial location (e.g., center) of the output feature map.\n",
    "        # This requires a 'feature_extractor' model that outputs the activations of the target layer.\n",
    "        feature_extractor = Model(inputs=model.input, outputs=layer.output)\n",
    "\n",
    "        # Initialize a random input image\n",
    "        # Needs to match your model's input shape (e.g., (1, H, W, C))\n",
    "        # Start with small random noise\n",
    "        img_height, img_width, img_channels = model.input_shape[1:]\n",
    "        input_img_data = tf.random.uniform(shape=(1, img_height, img_width, img_channels), minval=0.0, maxval=1.0)\n",
    "        input_img_data = tf.Variable(input_img_data)  # Make it a tf.Variable for gradient optimization\n",
    "\n",
    "        for _ in range(iterations):\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(input_img_data)\n",
    "                # Get the activations of the chosen layer\n",
    "                layer_activation = feature_extractor(input_img_data)\n",
    "                # Maximize the activation of the chosen filter\n",
    "                # We often take the mean of the filter's output for robustness\n",
    "                loss = tf.reduce_mean(layer_activation[:, :, :, filter_index])\n",
    "\n",
    "            # Compute gradients of the loss with respect to the input image\n",
    "            grads = tape.gradient(loss, input_img_data)\n",
    "            # Normalize the gradients\n",
    "            grads = grads / (tf.norm(grads) + 1e-8)  # Add epsilon to avoid div by zero\n",
    "\n",
    "            # Update the input image using gradient ascent\n",
    "            input_img_data.assign_add(grads * learning_rate)\n",
    "\n",
    "            # Optional: apply constraints (e.g., pixel values within 0-1 range)\n",
    "            input_img_data.assign(tf.clip_by_value(input_img_data, 0.0, 1.0))\n",
    "\n",
    "        # Post-process the generated image for display\n",
    "        generated_image = input_img_data.numpy().squeeze()  # Remove batch dim and channel if 1\n",
    "        generated_images.append(generated_image)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(len(filter_indices) * 2, 3))\n",
    "    plt.suptitle(f'Visualized Filters for Layer: {layer_name}', y=1.02, fontsize=14)\n",
    "    for i, img in enumerate(generated_images):\n",
    "        ax = plt.subplot(1, len(filter_indices), i + 1)\n",
    "        # Assuming grayscale images for your X-rays\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.title(f'Filter {filter_indices[i]}', fontsize=10)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n"
   ],
   "id": "84a1e29fa8ea5e1c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Heatmap Overlay",
   "id": "d05638a7645da76b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Function to overlay heatmap on an image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras import Model\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def overlay_heatmap(image, heatmap, alpha=0.5, cmap='jet'):\n",
    "    \"\"\"Overlays a heatmap on a grayscale image.\"\"\"\n",
    "    # Normalize image for visualization (if not already 0-1 or 0-255)\n",
    "    image = image - image.min()\n",
    "    image = image / (image.max() - image.min() + 1e-8)  # Add small epsilon to avoid div by zero\n",
    "\n",
    "    # Apply colormap to the heatmap\n",
    "    heatmap = 255 * heatmap  # Scale to 0-255 for colormap\n",
    "    heatmap = heatmap.astype(np.uint8)\n",
    "    heatmap_colored = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)  # For matplotlib\n",
    "\n",
    "    # Resize original image to 3 channels for overlay\n",
    "    if len(image.shape) == 2:  # If grayscale 2D\n",
    "        image_rgb = cv2.cvtColor((image * 255).astype(np.uint8), cv2.COLOR_GRAY2RGB)\n",
    "    elif image.shape[-1] == 1:  # If grayscale with 1 channel\n",
    "        image_rgb = cv2.cvtColor((image[:, :, 0] * 255).astype(np.uint8), cv2.COLOR_GRAY2RGB)\n",
    "    else:  # Already RGB\n",
    "        image_rgb = (image * 255).astype(np.uint8)\n",
    "\n",
    "    # Overlay the heatmap on the original image\n",
    "    # Ensure heatmap_colored and image_rgb have the same HxW dimensions\n",
    "    heatmap_resized = cv2.resize(heatmap_colored, (image_rgb.shape[1], image_rgb.shape[0]))\n",
    "\n",
    "    # Convert image_rgb and heatmap_resized to float before addWeighted\n",
    "    superimposed_img = cv2.addWeighted(image_rgb.astype(np.float32), 1 - alpha, heatmap_resized.astype(np.float32),\n",
    "                                       alpha, 0)\n",
    "\n",
    "    return superimposed_img / 255.0  # Return normalized for pyplot visualization\n",
    "\n",
    "\n",
    "def visualize_attention_map(model, preprocessed_image_tensor, true_age, std_val, mean_val, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualizes the spatial attention map generated by the model for a given preprocessed image.\n",
    "    Requires a model with a layer named 'attention_prep' (the output of SpatialAttention).\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): The trained Keras model.\n",
    "        preprocessed_image_tensor (tf.Tensor): The preprocessed input image tensor (batch_size=1, H, W, C).\n",
    "        true_age (float): The true age of the patient for the given image.\n",
    "        std_val (float): Standard deviation used for image denormalization.\n",
    "        mean_val (float): Mean value used for image denormalization.\n",
    "        save_path (str, optional): Path to save the plot. If None, displays the plot.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get the output of the attention_prep layer (which is the attention map)\n",
    "        attention_layer_output = model.get_layer('attention_prep').output\n",
    "        attention_model = Model(inputs=model.input, outputs=attention_layer_output)\n",
    "    except ValueError:\n",
    "        print(\"Error: 'attention_prep' layer not found in the model. Cannot visualize attention map.\")\n",
    "        print(\"Please ensure your SpatialAttention layer's output is named 'attention_prep' or adjust the code.\")\n",
    "        return\n",
    "\n",
    "    # Get the attention map\n",
    "    attention_map = attention_model.predict(preprocessed_image_tensor)[0, :, :, 0]  # Remove batch and channel dim\n",
    "\n",
    "    # Normalize attention map between 0 and 1\n",
    "    attention_map = (attention_map - attention_map.min()) / (attention_map.max() - attention_map.min() + 1e-8)\n",
    "\n",
    "    # Upsample attention map to original image size (model's input size)\n",
    "    img_height, img_width = preprocessed_image_tensor.shape[1:3]\n",
    "    upsampled_attention_map = cv2.resize(attention_map, (img_width, img_height), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # Denormalize the input image for display (assuming it was standardized)\n",
    "    display_image = (preprocessed_image_tensor[0].numpy() * std_val) + mean_val\n",
    "    display_image = (display_image - display_image.min()) / (display_image.max() - display_image.min() + 1e-8)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(display_image.squeeze(), cmap='gray')\n",
    "    plt.title(f'Original Image (True Age: {true_age:.2f} months)')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    overlayed_image = overlay_heatmap(display_image.squeeze(), upsampled_attention_map)\n",
    "    plt.imshow(overlayed_image)\n",
    "    plt.title('Spatial Attention Map')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ],
   "id": "3d74890ca37f0ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### History",
   "id": "ed6f35673d028456"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# src/plot/plot_training_history.py\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from keras.callbacks import History # For type hinting\n",
    "\n",
    "def plot_training_history(history: History, save_path: str = 'training_history_plots.png'):\n",
    "    \"\"\"\n",
    "    Generates and saves plots of the loss and MAE from the training and validation history.\n",
    "\n",
    "    Args:\n",
    "        history (tf.keras.callbacks.History): The History object returned by model.fit().\n",
    "        save_path (str): The full path where the plot will be saved.\n",
    "    \"\"\"\n",
    "    if not isinstance(history, History):\n",
    "        print(\"Error: 'history' must be an instance of tf.keras.callbacks.History.\")\n",
    "        return\n",
    "\n",
    "    hist = history.history\n",
    "    epochs = range(1, len(hist['loss']) + 1)\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    # Plot of Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, hist['loss'], label='Training Loss')\n",
    "    if 'val_loss' in hist:\n",
    "        plt.plot(epochs, hist['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss Curve During Training')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (MAE)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot of MAE\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, hist['mae'], label='Training MAE')\n",
    "    if 'val_mae' in hist:\n",
    "        plt.plot(epochs, hist['val_mae'], label='Validation MAE')\n",
    "    plt.title('MAE Curve During Training')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE (Months)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    print(f\"\\nTraining history plots saved as '{save_path}'.\")\n",
    "    plt.show() # Display the plot"
   ],
   "id": "38fda0656c796dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Shap",
   "id": "ce9904d6696150a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "def explain_prediction_shap(model, image_tensor, original_image_for_display, save_path=None):\n",
    "    \"\"\"\n",
    "    Explains a single prediction using SHAP (SHapley Additive exPlanations).\n",
    "    Requires 'shap' library to be installed (pip install shap).\n",
    "    This is a conceptual structure; SHAP for images (DeepExplainer) can be memory intensive.\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): The trained Keras model.\n",
    "        image_tensor (tf.Tensor): The preprocessed input image tensor (1, H, W, C) for the model.\n",
    "        original_image_for_display (np.array): The original (denormalized, 0-1) image for display.\n",
    "        save_path (str, optional): Path to save the plot.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import shap\n",
    "    except ImportError:\n",
    "        print(\"SHAP library not installed. Please run 'pip install shap'.\")\n",
    "        return\n",
    "\n",
    "    # SHAP DeepExplainer needs a background dataset for reference.\n",
    "    # This can be a small subset of your training data.\n",
    "    # For simplicity, we use a single blank image or averaged image as background.\n",
    "    # In a real scenario, use a sample of your training data (e.g., 10-50 images).\n",
    "    background_image = tf.zeros_like(image_tensor)  # Or use a mean image from your dataset\n",
    "    # If using a dataset, it might look like:\n",
    "    # background_data = next(iter(train_dataset.take(num_background_samples)))[0]\n",
    "    # background = background_data.numpy()\n",
    "\n",
    "    # The DeepExplainer takes (model, data). 'data' should be the background.\n",
    "    # For Keras models, it automatically detects the input layer.\n",
    "    explainer = shap.DeepExplainer(model, background_image.numpy())\n",
    "\n",
    "    # Calculate SHAP values for the input image\n",
    "    # Note: This can be computationally intensive for large images/models\n",
    "    shap_values = explainer.shap_values(image_tensor.numpy())\n",
    "\n",
    "    # SHAP values for images are usually per-channel. For grayscale, it's simpler.\n",
    "    # `shap_values` for regression will be a list with one array, matching output shape.\n",
    "    # `shap_values[0]` will have shape (1, H, W, C)\n",
    "\n",
    "    # Reshape for plotting. Assumes single output (regression).\n",
    "    if isinstance(shap_values, list) and len(shap_values) == 1:\n",
    "        shap_values = shap_values[0]  # Take the array for the single output\n",
    "\n",
    "    # Plot the SHAP values\n",
    "    # `shap.image_plot` expects image to be 0-1 or 0-255.\n",
    "    # original_image_for_display should be 0-1\n",
    "    shap_image_input = (original_image_for_display * 255).astype(np.uint8)\n",
    "    if len(shap_image_input.shape) == 2:  # SHAP expects 3 channels usually\n",
    "        shap_image_input = cv2.cvtColor(shap_image_input, cv2.COLOR_GRAY2RGB)\n",
    "    elif shap_image_input.shape[-1] == 1:\n",
    "        shap_image_input = cv2.cvtColor(shap_image_input, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    shap.image_plot(shap_values, shap_image_input, show=False)  # show=False to handle figure saving\n",
    "    plt.suptitle('SHAP Explanation for Predicted Age', y=1.02, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ],
   "id": "945453f88394060f",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluation",
   "id": "1e3a27bdedb85520"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import models\n",
    "from typing import Tuple, List, Optional\n",
    "\n",
    "from src.Models.AttentionLayer import SpatialAttention\n",
    "from src.dataset.RadiographDataset import RadiographDatasetBuilder\n",
    "from src.utils.interpretPipeline import run_all_interpretability_plots\n",
    "\n",
    "def evaluate_saved_model(model_path: str,\n",
    "                         test_base_dir: str,\n",
    "                         label_test_dataset_path: str,\n",
    "                         mean_pixel_value: float,\n",
    "                         std_pixel_value: float,\n",
    "                         img_sizes: int = 256) -> Tuple[\n",
    "    Optional[List[float]], Optional[np.ndarray], Optional[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Loads a saved Keras model, evaluates it on a test dataset, collects\n",
    "    the true labels and corresponding predictions, and performs interpretability visualizations.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): The file path to the saved Keras model\n",
    "                          (e.g., '.keras', '.h5', or SavedModel directory).\n",
    "        test_base_dir (str): The base directory for the test dataset,\n",
    "                             expected to contain a 'prep_images' subfolder\n",
    "                             and the label CSV.\n",
    "        label_test_dataset_path (str): The filename of the CSV file containing\n",
    "                                       ground truth labels for the test set,\n",
    "                                       relative to `test_base_dir`.\n",
    "        mean_pixel_value (float): The mean pixel value calculated from the TRAINING dataset,\n",
    "                                  used for standardizing image data. Crucial for consistency.\n",
    "        std_pixel_value (float): The standard deviation pixel value calculated from the TRAINING dataset,\n",
    "                                 used for standardizing image data. Crucial for consistency.\n",
    "        img_sizes (int, optional): The target square dimension (height and width)\n",
    "                                   for input images. This must match the size\n",
    "                                   used during model training and preprocessing.\n",
    "                                   Defaults to 256.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Optional[List[float]], Optional[np.ndarray], Optional[np.ndarray]]: A tuple containing:\n",
    "            - results (Optional[List[float]]): A list of evaluation results (e.g., loss, metrics).\n",
    "                                               Returns None if the model cannot be loaded or evaluated.\n",
    "            - true_months (Optional[np.ndarray]): A NumPy array of true bone ages in months.\n",
    "                                                  Returns None if the model cannot be loaded or evaluated.\n",
    "            - pred_months (Optional[np.ndarray]): A NumPy array of predicted bone ages in months.\n",
    "                                                  Returns None if the model cannot be loaded or evaluated.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the RadiographDatasetBuilder for the test set.\n",
    "    # It ensures images are loaded and standardized consistently using the provided mean/std\n",
    "    # values (which should come from the training set).\n",
    "    builder_test = RadiographDatasetBuilder(\n",
    "        base_dir=test_base_dir,\n",
    "        label_csv=label_test_dataset_path,\n",
    "        mean_pixel_value=mean_pixel_value,  # Pass the mean value for standardization\n",
    "        std_pixel_value=std_pixel_value,    # Pass the standard deviation for standardization\n",
    "        img_size=(img_sizes, img_sizes),\n",
    "        batch_size=1  # Set batch size to 1 to easily capture a single sample for visualizations\n",
    "    )\n",
    "    # Build the TensorFlow Dataset for testing. `shuffle=False` ensures consistent order.\n",
    "    test_dataset = builder_test.build(train=False)\n",
    "\n",
    "    print(f\"\\nLoading model from: {model_path}\")\n",
    "    try:\n",
    "        # Load the Keras model from the specified path.\n",
    "        # `custom_objects` is essential for correctly deserializing custom layers\n",
    "        # like `SpatialAttention` that are part of your model architecture.\n",
    "        loaded_model = models.load_model(model_path, custom_objects={'SpatialAttention': SpatialAttention})\n",
    "        print(\"Model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the model: {e}\")\n",
    "        import traceback  # Import traceback for detailed error logging\n",
    "        traceback.print_exc()  # Print full stack trace for debugging\n",
    "        return None, None, None  # Return None values to indicate failure\n",
    "\n",
    "    print(\"\\nEvaluating and collecting predictions on the Test Set:\")\n",
    "\n",
    "    true_months: List[float] = []  # List to store true bone ages\n",
    "    pred_months: List[float] = []  # List to store predicted bone ages\n",
    "\n",
    "    # Variables to capture a single sample for interpretability visualizations\n",
    "    sample_preprocessed_image_tensor = None\n",
    "    sample_true_age = None\n",
    "\n",
    "    try:\n",
    "        # Iterate over the test dataset to collect true labels and generate predictions.\n",
    "        # Batch size is 1, so each `inputs` and `labels` tensor represents a single sample.\n",
    "        for i, (inputs, labels) in enumerate(test_dataset):\n",
    "            # Capture the first sample for visualizations.\n",
    "            # `inputs` will already have a batch dimension of 1 due to `batch_size=1`.\n",
    "            if sample_preprocessed_image_tensor is None:\n",
    "                sample_preprocessed_image_tensor = inputs # This is already (1, H, W, C)\n",
    "                sample_true_age = labels.numpy()[0] # Extract scalar true age\n",
    "\n",
    "            # Generate prediction for the current single input.\n",
    "            # `verbose=0` suppresses output during prediction.\n",
    "            # `[0][0]` is used to extract the scalar prediction value from the output tensor.\n",
    "            prediction = loaded_model.predict(inputs, verbose=0)[0][0]\n",
    "\n",
    "            # Append the numpy value of the true label and the scalar prediction.\n",
    "            true_months.append(labels.numpy()[0]) # Extract scalar true age\n",
    "            pred_months.append(prediction)\n",
    "\n",
    "            # Optional: If you only need interpretability for the very first sample,\n",
    "            # you could break here to speed up evaluation for large datasets.\n",
    "            # However, typically you want to evaluate the whole test set.\n",
    "            # if i == 0:\n",
    "            #     # If you want to stop after processing just the first sample for visualization,\n",
    "            #     # you might need to adjust how the overall evaluation `loaded_model.evaluate` is called\n",
    "            #     # or ensure this loop processes the whole dataset.\n",
    "            #     pass\n",
    "            # else:\n",
    "            #     # If you uncomment this, only the first image will be processed for both eval and viz.\n",
    "            #     # For full dataset evaluation, remove this `break`.\n",
    "            #     # break\n",
    "            pass # Keep processing the full dataset for comprehensive evaluation\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"Dataset exhausted (end of iteration).\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction collection on the test set: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None  # Indicate failure\n",
    "\n",
    "    # Convert lists of true and predicted values to NumPy arrays for further analysis and plotting.\n",
    "    true_months_np = np.array(true_months)\n",
    "    pred_months_np = np.array(pred_months)\n",
    "\n",
    "    # Perform a formal evaluation of the model on the entire test dataset.\n",
    "    # It's important to build the test_dataset again or ensure its iterator is reset\n",
    "    # if it has already been consumed by the previous loop, to get a full evaluation.\n",
    "    test_dataset_for_eval = builder_test.build(train=False)  # Rebuild to ensure full evaluation\n",
    "    results = loaded_model.evaluate(test_dataset_for_eval, verbose=0)\n",
    "\n",
    "    print(\"Evaluation Results:\")\n",
    "    # Print each metric name and its corresponding value from the evaluation results.\n",
    "    for name, value in zip(loaded_model.metrics_names, results):\n",
    "        print(f\"{name}: {value:.4f}\")\n",
    "\n",
    "    # --- Execute interpretability visualizations on the captured sample ---\n",
    "    if sample_preprocessed_image_tensor is not None:\n",
    "        run_all_interpretability_plots(\n",
    "            loaded_model,\n",
    "            sample_preprocessed_image_tensor,\n",
    "            sample_true_age,\n",
    "            mean_pixel_value,  # Pass the mean value for denormalization\n",
    "            std_pixel_value    # Pass the std value for denormalization\n",
    "        )\n",
    "    else:\n",
    "        print(\"Could not retrieve a sample image for interpretability visualizations. Skipping.\")\n",
    "\n",
    "    return results, true_months_np, pred_months_np"
   ],
   "id": "52ad20f93087f6ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from typing import Tuple  # Import for type hinting\n",
    "\n",
    "# Assuming these imports are correctly resolved in your project structure\n",
    "from src.Models.AttentionLayer import SpatialAttention  # Ensure correct casing/path if different\n",
    "from src.plot.PlotEval import plot_eval  # Assuming plot_eval is a function for plotting evaluation results\n",
    "from keras import models  # Keras models module for loading saved models\n",
    "\n",
    "from src.testing.evaluate import evaluate_saved_model\n",
    "\n",
    "\n",
    "def evaluation_pipeline(model_save_path: str,\n",
    "                        test_path: str,\n",
    "                        label_path: str,\n",
    "                        img_sizes: int,  # Add img_sizes to signature as it's passed to evaluate_saved_model\n",
    "                        mean_val: float,\n",
    "                        std_val: float,\n",
    "                        ):\n",
    "    \"\"\"\n",
    "    Orchestrates the evaluation of a trained deep learning model on a test set\n",
    "    and generates visual plots of its performance.\n",
    "\n",
    "    This function loads a previously saved Keras model, evaluates it on the\n",
    "    specified test dataset (ensuring consistent data preprocessing), and then\n",
    "    visualizes the prediction errors and distributions.\n",
    "\n",
    "    Args:\n",
    "        model_save_path (str): The file path to the saved Keras model (e.g., '.keras', '.h5').\n",
    "        test_path (str): The base directory of the test dataset containing images.\n",
    "        label_path (str): The file path to the CSV containing ground truth labels for the test set.\n",
    "        img_sizes (int): The target square dimension (e.g., 256) of the images used by the model.\n",
    "                         This must match the size used during model training and preprocessing.\n",
    "        mean_val (float): The mean pixel value calculated from the TRAINING dataset,\n",
    "        std_val (float): The standard deviation pixel value calculated from the TRAINING dataset,\n",
    "\n",
    "    Returns:\n",
    "        None: This function does not return any value but prints evaluation results\n",
    "              and saves plots to disk.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Model Evaluation and Graph Generation ---\")\n",
    "\n",
    "    # Check if the saved model file exists before proceeding.\n",
    "    if os.path.exists(model_save_path):\n",
    "        # Evaluate the saved model on the test dataset.\n",
    "        # It's crucial to pass the mean/std values to ensure the test set is\n",
    "        # preprocessed identically to the training set.\n",
    "        evaluation_results, true_months_list, pred_months_list = evaluate_saved_model(\n",
    "            model_save_path,\n",
    "            test_path,\n",
    "            label_path,\n",
    "            mean_pixel_value=mean_val,\n",
    "            std_pixel_value=std_val,\n",
    "            img_sizes=img_sizes,  # Pass img_sizes\n",
    "        )\n",
    "\n",
    "        # Load the model again to access its metrics_names.\n",
    "        # This is redundant with the loading inside evaluate_saved_model\n",
    "        # but needed here to dynamically get the MAE index if it's not always 1.\n",
    "        try:\n",
    "            loaded_model_for_metrics = models.load_model(model_save_path,\n",
    "                                                         custom_objects={'SpatialAttention': SpatialAttention})\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model for metrics names: {e}\")\n",
    "            loaded_model_for_metrics = None\n",
    "\n",
    "        # Proceed with plotting if evaluation results were successfully obtained.\n",
    "        if evaluation_results is not None and loaded_model_for_metrics is not None:\n",
    "            # Dynamically find the index of 'mae' in the model's metrics names.\n",
    "            # Fallback to index 1 if 'mae' is not explicitly found (assuming loss is at 0, mae at 1).\n",
    "            mae_index = loaded_model_for_metrics.metrics_names.index(\n",
    "                'mae') if 'mae' in loaded_model_for_metrics.metrics_names else 1\n",
    "            mae_months = evaluation_results[mae_index] if mae_index is not None and mae_index < len(\n",
    "                evaluation_results) else np.nan\n",
    "\n",
    "            # Calculate absolute errors for plotting the error distribution.\n",
    "            errors_list = np.abs(true_months_list - pred_months_list)\n",
    "\n",
    "            # Call the external plotting function to visualize the evaluation results.\n",
    "            plot_eval(errors_list, mae_months, true_months_list, pred_months_list)\n",
    "            print(\"Evaluation graphs generated and saved as 'age_prediction_analysis.png'.\")\n",
    "        else:\n",
    "            print(\"\\nUnable to evaluate the saved model due to an error during loading or evaluation process.\")\n",
    "    else:\n",
    "        # Inform the user if the model file was not found.\n",
    "        print(f\"Error: The model was not found at the specified path: {model_save_path}\")"
   ],
   "id": "91a9a6a47e320b09"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Main",
   "id": "83de157ac802a2b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os.path\n",
    "\n",
    "# Imports for preprocessing utilities\n",
    "from src.preprocessing.PreprocessImage import calculate_mean_std  # Function to calculate mean/std for standardization\n",
    "from src.preprocessing.PreprocessPipeline import preprocess_pipeline  # Full preprocessing pipeline function\n",
    "\n",
    "# Imports for model evaluation\n",
    "from src.testing.EvaluationPipeline import evaluation_pipeline  # Orchestrates model evaluation\n",
    "\n",
    "# Imports for model training\n",
    "from src.training.training_pipeline import training_pipeline  # Orchestrates model training pipeline\n",
    "\n",
    "# Imports for utility functions\n",
    "from src.utils.load_model import load_trained_model  # Utility function to load a Keras model\n",
    "\n",
    "# --- Global Constants ---\n",
    "MODEL_PATH = 'best_age_prediction_model.keras'\n",
    "\"\"\"\n",
    "str: The default file path for saving and loading the best trained Keras model.\n",
    "\"\"\"\n",
    "TARGET_IMG_SIZE = 256\n",
    "\"\"\"\n",
    "int: The target size (height and width) in pixels for preprocessed images.\n",
    "     Images will be resized to (TARGET_IMG_SIZE, TARGET_IMG_SIZE).\n",
    "\"\"\"\n",
    "\n",
    "def radiograph_pipeline(preprocess: bool = False, training: bool = False, evaluate: bool = False):\n",
    "    \"\"\"\n",
    "    Main pipeline orchestrator for bone age assessment from radiographs.\n",
    "\n",
    "    This function controls the entire workflow, including data preprocessing,\n",
    "    model training, and model evaluation, based on the boolean flags provided.\n",
    "\n",
    "    Args:\n",
    "        preprocess (bool, optional): If True, executes the image preprocessing pipeline\n",
    "                                     for training, validation, and test datasets.\n",
    "                                     This creates standardized image files. Defaults to False.\n",
    "        training (bool, optional): If True, executes the model training pipeline.\n",
    "                                   It calculates mean/std for standardization, loads an\n",
    "                                   existing model (if available), and initiates training.\n",
    "                                   Defaults to False.\n",
    "        evaluate (bool, optional): If True, executes the model evaluation pipeline on\n",
    "                                   the test dataset using the best saved model. Defaults to False.\n",
    "    \"\"\"\n",
    "    # --- Phase 1: Data Preprocessing ---\n",
    "    # This block performs image preprocessing (hand detection, CLAHE, resizing, augmentation)\n",
    "    # and saves the processed images to specified 'prep_images' directories.\n",
    "    # It needs to be run once to prepare the dataset for training/evaluation.\n",
    "    if preprocess:\n",
    "        print(\"\\n--- Starting Data Preprocessing ---\")\n",
    "        preprocess_pipeline('data/Val/', Val=True)\n",
    "        preprocess_pipeline('data/Test/', Test=True, Val=False)  # Val=False ensures it's treated as Test\n",
    "        preprocess_pipeline('data/Train/', Train=True, Val=False)  # Val=False ensures it's treated as Train\n",
    "        print(\"--- Data Preprocessing Completed ---\")\n",
    "\n",
    "    # --- Phase 2: Model Training ---\n",
    "    # This block handles the training of the deep learning model.\n",
    "    # It calculates standardization parameters, attempts to load a previously\n",
    "    # trained model to continue training, and then runs the training pipeline.\n",
    "    base_dir_train = 'data/Train'\n",
    "    base_dir_val = 'data/Val'\n",
    "    mean_val, std_val = None, None\n",
    "    if training:\n",
    "        print(\"\\n--- Starting Model Training ---\")\n",
    "        # Define base directories and label CSVs for training and validation datasets.\n",
    "        # These paths assume a specific directory structure for your data.\n",
    "        label_train_csv = os.path.join(base_dir_train, 'train_labels.csv')  # Assuming specific CSV names\n",
    "        label_val_csv = os.path.join(base_dir_val, 'val_labels.csv')\n",
    "\n",
    "        # Calculate mean and standard deviation of pixel values from the preprocessed\n",
    "        # training images. These values are crucial for standardizing all datasets\n",
    "        # (train, validation, test) to ensure consistency during model inference.\n",
    "        train_prep_images_path = os.path.join(base_dir_train, 'prep_images')\n",
    "        mean_val, std_val = calculate_mean_std(train_prep_images_path, img_size=(TARGET_IMG_SIZE, TARGET_IMG_SIZE))\n",
    "\n",
    "        # Attempt to load a pre-trained model. If MODEL_PATH exists and the model\n",
    "        # can be loaded, training will resume from its current state. Otherwise,\n",
    "        # a new model will be initialized from scratch.\n",
    "        model = load_trained_model(model_save_path=MODEL_PATH)\n",
    "\n",
    "        # Execute the training pipeline.\n",
    "        # It orchestrates dataset loading, model compilation, and the training loop\n",
    "        # using the calculated standardization values and the loaded/new model.\n",
    "        training_pipeline(base_dir_train=base_dir_train,\n",
    "                          label_train=label_train_csv,\n",
    "                          label_val=label_val_csv,\n",
    "                          base_dir_val=base_dir_val,\n",
    "                          img_sizes=TARGET_IMG_SIZE,\n",
    "                          mean_pixel_value=mean_val,\n",
    "                          std_pixel_value=std_val,\n",
    "                          existing_model=model,  # Pass the loaded model (or None if new)\n",
    "                          model_save_path=MODEL_PATH)\n",
    "        print(\"--- Model Training Completed ---\")\n",
    "\n",
    "    # --- Phase 3: Model Evaluation ---\n",
    "    # This block evaluates the performance of the best saved model on the test dataset.\n",
    "    # It uses the same standardization parameters from training for consistency.\n",
    "    if evaluate:\n",
    "        print(\"\\n--- Starting Model Evaluation ---\")\n",
    "        # Define base directories and label CSV for the test dataset.\n",
    "        base_dir_test = 'data/Test'\n",
    "        label_test_csv = os.path.join(base_dir_test, 'test_labels.csv')\n",
    "\n",
    "        if mean_val is None or std_val is None:\n",
    "            train_prep_images_path = os.path.join(base_dir_train, 'prep_images')\n",
    "            mean_val, std_val = calculate_mean_std(train_prep_images_path, img_size=(TARGET_IMG_SIZE, TARGET_IMG_SIZE))\n",
    "\n",
    "        # Execute the evaluation pipeline.\n",
    "        # It loads the best saved model and evaluates it on the test dataset,\n",
    "        # applying the same standardization used during training.\n",
    "        evaluation_pipeline(model_save_path=MODEL_PATH,\n",
    "                            test_path=base_dir_test,\n",
    "                            label_path=label_test_csv,\n",
    "                            img_sizes=TARGET_IMG_SIZE,\n",
    "                            mean_val=mean_val,\n",
    "                            std_val=std_val)\n",
    "        print(\"--- Model Evaluation Completed ---\")\n"
   ],
   "id": "159d45eaff72af0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from src.pipelines.RadiographPipeline import radiograph_pipeline\n",
    "\n",
    "def main():\n",
    "    train_path = 'data/Train/train_labels.csv'\n",
    "    test_path = 'data/Test/test_labels.csv'\n",
    "    val_path = 'data/Val/val_labels.csv'\n",
    "\n",
    "    # Workspace preparation\n",
    "    # train_labels(train_path)\n",
    "    # test_labels(test_path)\n",
    "    # val_labels(val_path)\n",
    "\n",
    "    radiograph_pipeline(preprocess=False, training=False, evaluate=True)\n",
    "    # load_structure()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ],
   "id": "da3f511fdae890c3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
