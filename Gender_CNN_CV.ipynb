{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# import functions from Gender CNN notebook\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, callbacks\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import glob  # Can be useful but we use os\n",
        "\n",
        "\n",
        "\n",
        "SIZE = 300\n",
        "BATCH_SIZE = 32\n",
        "CHANNELS = 1  # Grayscale images\n",
        "INPUT_SHAPE = (SIZE, SIZE, CHANNELS)\n",
        "GENDER_FEATURE_SHAPE = (1,)  # gender (0 or 1)\n",
        "EPOCHS = 50\n",
        "LEARNING_RATE = 4e-4\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/MLHD'\n",
        "\n",
        "# Data Paths\n",
        "train_csv_path = os.path.join(base_dir, 'Train', 'train_labels.csv')\n",
        "val_csv_path = os.path.join(base_dir, 'Val', 'val_labels.csv')\n",
        "test_csv_path = os.path.join(base_dir, 'Test', 'test_labels.csv')\n",
        "\n",
        "train_image_dir = os.path.join(base_dir, 'Train', 'train_samples_pp')\n",
        "val_image_dir = os.path.join(base_dir, 'Val', 'val_samples_pp')\n",
        "test_image_dir = os.path.join(base_dir, 'Test', 'test_samples_pp')\n",
        "\n",
        "checkpoint_filepath = 'Models/gender_model.keras' # tf package says .keras is more efficient\n",
        "\n",
        "\n",
        "def load_labels(csv_path):\n",
        "    df = pd.read_csv(csv_path, index_col='id')\n",
        "\n",
        "    df = df[['boneage', 'male']].rename(columns={'male': 'gender'})\n",
        "    df['gender'] = df['gender'].astype(np.float32)\n",
        "    df['boneage'] = df['boneage'].astype(np.float32)\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_dataframe(image_dir, labels_df):\n",
        "    data = []\n",
        "\n",
        "    for filename in os.listdir(image_dir):\n",
        "        file_id = int(filename.split('.')[0])\n",
        "        if file_id in labels_df.index:\n",
        "            boneage = labels_df.loc[file_id, 'boneage']\n",
        "            gender = labels_df.loc[file_id, 'gender']\n",
        "            full_path = os.path.join(image_dir, filename)\n",
        "            data.append({'file_path': full_path, 'boneage': boneage, 'gender': gender})\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "def preprocess_image(image, labels):\n",
        "    \"\"\"preprocesses the image, passes labels through.\"\"\"\n",
        "    image = tf.image.resize(image, [SIZE, SIZE])\n",
        "    # Ensure correct channel dimension if decode_image didn't set it\n",
        "    if image.shape[-1] is None:\n",
        "        image = tf.reshape(image, [SIZE, SIZE, CHANNELS])\n",
        "    elif image.shape[-1] != CHANNELS:\n",
        "\n",
        "        print(f\"Warning: Image has {image.shape[-1]} channels, expected {CHANNELS}. Converting to grayscale.\")\n",
        "        image = tf.image.rgb_to_grayscale(image)  # example if source is RGB\n",
        "\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image, labels  # labels = (boneage, gender)\n",
        "\n",
        "\n",
        "def image_label_generator(file_paths, boneage_labels, gender_labels):\n",
        "    \"\"\"tuple of (boneage, gender) labels.\"\"\"\n",
        "    for path, boneage, gender in zip(file_paths, boneage_labels, gender_labels):\n",
        "        try:\n",
        "            img_bytes = tf.io.read_file(path)\n",
        "\n",
        "            image = tf.io.decode_image(img_bytes, channels=CHANNELS, expand_animations=False)\n",
        "\n",
        "            # allow dynamic height/width initially\n",
        "            image.set_shape([None, None, CHANNELS])\n",
        "            yield image, (boneage, gender)  # Yield image and label tuple\n",
        "        except tf.errors.InvalidArgumentError as e:\n",
        "            print(f\"Warning: Skipping file {path}. Error decoding image: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Skipping file {path}. Unexpected error: {e}\")\n",
        "\n",
        "\n",
        "def create_tf_dataset(dataframe, shuffle, repeat_flag, batch_size_local=BATCH_SIZE):\n",
        "\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "        image_label_generator,\n",
        "        args=[\n",
        "            dataframe['file_path'].values,\n",
        "            dataframe['boneage'].values,\n",
        "            dataframe['gender'].values],\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(None, None, CHANNELS), dtype=tf.uint8),\n",
        "            (tf.TensorSpec(shape=(), dtype=tf.float32), tf.TensorSpec(shape=(), dtype=tf.float32))))\n",
        "\n",
        "    dataset = dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)    # preprocessing\n",
        "\n",
        "    # Keras multi-input model:\n",
        "    # map (image, (boneage, gender)) to ((image, gender), boneage)\n",
        "    dataset = dataset.map(lambda img, labels: ((img, labels[1]), labels[0]),\n",
        "                          num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(buffer_size=len(dataframe), reshuffle_each_iteration=True)\n",
        "\n",
        "    if repeat_flag:\n",
        "        dataset = dataset.repeat()\n",
        "\n",
        "    dataset = dataset.batch(batch_size_local)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# labels\n",
        "train_labels_df = load_labels(train_csv_path)\n",
        "val_labels_df = load_labels(val_csv_path)\n",
        "test_labels_df = load_labels(test_csv_path)\n",
        "\n",
        "# df\n",
        "training_dataframe = create_dataframe(train_image_dir, train_labels_df)\n",
        "validation_dataframe = create_dataframe(val_image_dir, val_labels_df)\n",
        "test_dataframe = create_dataframe(test_image_dir, test_labels_df)\n",
        "\n",
        "\n",
        "# repeat=True for train/validation\n",
        "train_dataset = create_tf_dataset(training_dataframe, shuffle=True, repeat_flag=True)\n",
        "validation_dataset = create_tf_dataset(validation_dataframe, shuffle=False, repeat_flag=True)\n",
        "# repeat=False for final evaluation on test set\n",
        "test_dataset_eval = create_tf_dataset(test_dataframe, shuffle=False, repeat_flag=False)\n",
        "\n",
        "def gender_model(input_shape, gender_shape):\n",
        "\n",
        "    # inputs\n",
        "    image_input = keras.Input(shape=input_shape, name=\"image_input\")\n",
        "    gender_input = keras.Input(shape=gender_shape, name=\"gender_input\")\n",
        "\n",
        "    x = layers.Conv2D(32, (3, 3), padding='same', name='conv1a')(image_input)\n",
        "    x = layers.BatchNormalization(name='bn1a')(x)\n",
        "    x = layers.Activation('relu', name='relu1a')(x)\n",
        "    x = layers.Conv2D(32, (3, 3), padding='same', name='conv1b')(x)\n",
        "    x = layers.BatchNormalization(name='bn1b')(x)\n",
        "    x = layers.Activation('relu', name='relu1b')(x)\n",
        "    x = layers.MaxPooling2D((2, 2), name='pool1')(x)  # 500 -> 250\n",
        "\n",
        "    x = layers.Conv2D(64, (3, 3), padding='same', name='conv2a')(x)\n",
        "    x = layers.BatchNormalization(name='bn2a')(x)\n",
        "    x = layers.Activation('relu', name='relu2a')(x)\n",
        "    x = layers.Conv2D(64, (3, 3), padding='same', name='conv2b')(x)\n",
        "    x = layers.BatchNormalization(name='bn2b')(x)\n",
        "    x = layers.Activation('relu', name='relu2b')(x)\n",
        "    x = layers.MaxPooling2D((2, 2), name='pool2')(x)  # 250 -> 125\n",
        "\n",
        "    x = layers.Conv2D(128, (3, 3), padding='same', name='conv3a')(x)\n",
        "    x = layers.BatchNormalization(name='bn3a')(x)\n",
        "    x = layers.Activation('relu', name='relu3a')(x)\n",
        "    x = layers.Conv2D(128, (3, 3), padding='same', name='conv3b')(x)\n",
        "    x = layers.BatchNormalization(name='bn3b')(x)\n",
        "    x = layers.Activation('relu', name='relu3b')(x)\n",
        "    x = layers.MaxPooling2D((2, 2), name='pool3')(x)  # 125 -> 62\n",
        "\n",
        "    x = layers.Conv2D(256, (3, 3), padding='same', name='conv4a')(x)\n",
        "    x = layers.BatchNormalization(name='bn4a')(x)\n",
        "    x = layers.Activation('relu', name='relu4a')(x)\n",
        "    x = layers.Conv2D(256, (3, 3), padding='same', name='conv4b')(x)\n",
        "    x = layers.BatchNormalization(name='bn4b')(x)\n",
        "    x = layers.Activation('relu', name='relu4b')(x)\n",
        "    x = layers.MaxPooling2D((2, 2), name='pool4')(x)  # 62 -> 31\n",
        "\n",
        "    x = layers.Conv2D(256, (3, 3), padding='same', name='conv5a')(x)\n",
        "    x = layers.BatchNormalization(name='bn5a')(x)\n",
        "    x = layers.Activation('relu', name='relu5a')(x)\n",
        "    x = layers.Conv2D(256, (3, 3), padding='same', name='conv5b')(x)\n",
        "    x = layers.BatchNormalization(name='bn5b')(x)\n",
        "    x = layers.Activation('relu', name='relu5b')(x)\n",
        "    x = layers.MaxPooling2D((2, 2), name='pool5')(x)  # 31 -> 15 (approx)\n",
        "\n",
        "    # feature extraction\n",
        "    image_features = layers.GlobalAveragePooling2D(name='global_avg_pool')(x)  # (None, 256)\n",
        "\n",
        "    # fusion with gender\n",
        "    concatenated_features = layers.concatenate([image_features, gender_input],\n",
        "                                               name='concatenate_features')  # (None, 257)\n",
        "\n",
        "    # regression head\n",
        "    x = layers.Dense(128, name='dense_head1')(concatenated_features)\n",
        "    x = layers.BatchNormalization(name='bn_head1')(x)\n",
        "    x = layers.Activation('relu', name='relu_head1')(x)\n",
        "    x = layers.Dropout(0.4, name='dropout_head')(x)  # Regularization\n",
        "\n",
        "    bone_age_output = layers.Dense(1, activation='linear', name='bone_age_output')(x)\n",
        "\n",
        "    model = keras.Model(\n",
        "        inputs=[image_input, gender_input],\n",
        "        outputs=bone_age_output,\n",
        "        name=\"bone_age_predictor\")\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='mse',\n",
        "                  metrics=['mae'])  # months\n",
        "\n",
        "    return model\n",
        "\n",
        "model = gender_model(INPUT_SHAPE, GENDER_FEATURE_SHAPE)\n",
        "\n",
        "\n",
        "model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=False,\n",
        "    monitor='val_mae', # save the best mae\n",
        "    mode='min',\n",
        "    save_best_only=True)\n",
        "\n",
        "early_stopping_callback = callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=6,\n",
        "    mode='min',\n",
        "    restore_best_weights=True)\n",
        "# restore best weights because it tends to overfit\n",
        "# monitor loss because it's the actual improvement metric meanwhile mae can be a face value metric\n",
        "\n",
        "reduce_lr_callback = callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=3,\n",
        "    mode='min',\n",
        "    min_lr=1e-6)\n",
        "# trial and error came to a best hyperparam of 3 epochs\n",
        "\n",
        "callback_list = [model_checkpoint_callback, early_stopping_callback, reduce_lr_callback]"
      ],
      "metadata": {
        "id": "Oc4cez1mOxi8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "# how many folds\n",
        "n_splits = 5\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# to store histories\n",
        "all_hist = []"
      ],
      "metadata": {
        "id": "bewNZr0DOxv7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(training_dataframe), 1):\n",
        "    print(f\"\\n>>> Fold {fold}/{n_splits}\")\n",
        "\n",
        "    # split df\n",
        "    df_train = training_dataframe.iloc[train_idx]\n",
        "    df_val   = training_dataframe.iloc[val_idx]\n",
        "\n",
        "    # build datasets\n",
        "    train_ds = create_tf_dataset(df_train, shuffle=True, repeat_flag=True)\n",
        "    val_ds   = create_tf_dataset(df_val,   shuffle=False, repeat_flag=False)\n",
        "\n",
        "    steps     = len(df_train) // BATCH_SIZE\n",
        "    val_steps = len(df_val)   // BATCH_SIZE\n",
        "\n",
        "    # fresh model\n",
        "    model = gender_model(INPUT_SHAPE, GENDER_FEATURE_SHAPE)\n",
        "\n",
        "    # fit\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        epochs=EPOCHS,\n",
        "        steps_per_epoch=steps,\n",
        "        validation_data=val_ds,\n",
        "        validation_steps=val_steps,\n",
        "        callbacks=callback_list,\n",
        "        verbose=1)\n",
        "\n",
        "    # save history\n",
        "    all_hist.append(history.history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Rr01Ku4Ox-i",
        "outputId": "638ec5f4-5eb5-4638-f9d8-62de8832ef56"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> Fold 1/5\n",
            "Epoch 1/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 206ms/step - loss: 16599.4043 - mae: 122.3636 - val_loss: 16351.3467 - val_mae: 121.1388 - learning_rate: 4.0000e-04\n",
            "Epoch 2/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 204ms/step - loss: 13995.7500 - mae: 112.0503 - val_loss: 12306.5312 - val_mae: 104.8207 - learning_rate: 4.0000e-04\n",
            "Epoch 3/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 202ms/step - loss: 10721.3564 - mae: 97.8906 - val_loss: 3015.4346 - val_mae: 46.8515 - learning_rate: 4.0000e-04\n",
            "Epoch 4/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 200ms/step - loss: 7245.0088 - mae: 79.5228 - val_loss: 5654.0508 - val_mae: 65.4470 - learning_rate: 4.0000e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 197ms/step - loss: 3912.8035 - mae: 56.8100 - val_loss: 2396.4583 - val_mae: 44.8853 - learning_rate: 4.0000e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 191ms/step - loss: 1993.7014 - mae: 38.7887 - val_loss: 1462.3503 - val_mae: 30.9677 - learning_rate: 4.0000e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 174ms/step - loss: 1035.4839 - mae: 26.1885 - val_loss: 2265.5872 - val_mae: 37.6934 - learning_rate: 4.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 170ms/step - loss: 678.7999 - mae: 20.7723 - val_loss: 796.0355 - val_mae: 23.1787 - learning_rate: 4.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 581.4222 - mae: 18.8973 - val_loss: 339.1613 - val_mae: 14.4417 - learning_rate: 4.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 167ms/step - loss: 460.1445 - mae: 16.8600 - val_loss: 359.5765 - val_mae: 14.9426 - learning_rate: 4.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 170ms/step - loss: 425.9717 - mae: 16.3107 - val_loss: 336.4450 - val_mae: 14.0077 - learning_rate: 4.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 168ms/step - loss: 410.9403 - mae: 15.9685 - val_loss: 274.5835 - val_mae: 12.7558 - learning_rate: 4.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 376.3672 - mae: 15.3113 - val_loss: 181.2269 - val_mae: 10.5350 - learning_rate: 4.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 168ms/step - loss: 379.4380 - mae: 15.3719 - val_loss: 332.7731 - val_mae: 14.2082 - learning_rate: 4.0000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 355.4980 - mae: 14.8929 - val_loss: 219.3019 - val_mae: 11.5462 - learning_rate: 4.0000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 172ms/step - loss: 357.3030 - mae: 14.9895 - val_loss: 191.3066 - val_mae: 10.7005 - learning_rate: 4.0000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 319.2362 - mae: 14.0228 - val_loss: 164.2084 - val_mae: 9.9504 - learning_rate: 8.0000e-05\n",
            "Epoch 18/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 168ms/step - loss: 321.7424 - mae: 14.0673 - val_loss: 183.7440 - val_mae: 10.8012 - learning_rate: 8.0000e-05\n",
            "Epoch 19/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 307.4281 - mae: 13.8629 - val_loss: 166.1553 - val_mae: 10.0140 - learning_rate: 8.0000e-05\n",
            "Epoch 20/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 170ms/step - loss: 296.4369 - mae: 13.6177 - val_loss: 175.8362 - val_mae: 10.3678 - learning_rate: 8.0000e-05\n",
            "Epoch 21/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 299.0049 - mae: 13.6134 - val_loss: 130.1318 - val_mae: 8.7816 - learning_rate: 1.6000e-05\n",
            "Epoch 22/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 167ms/step - loss: 290.8040 - mae: 13.4337 - val_loss: 130.6572 - val_mae: 8.8078 - learning_rate: 1.6000e-05\n",
            "Epoch 23/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 279.0490 - mae: 13.2631 - val_loss: 132.9240 - val_mae: 8.8893 - learning_rate: 1.6000e-05\n",
            "Epoch 24/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 167ms/step - loss: 280.5577 - mae: 13.2139 - val_loss: 135.1680 - val_mae: 8.9929 - learning_rate: 1.6000e-05\n",
            "Epoch 25/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 297.1517 - mae: 13.6228 - val_loss: 129.2765 - val_mae: 8.7537 - learning_rate: 3.2000e-06\n",
            "Epoch 26/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 167ms/step - loss: 283.2094 - mae: 13.2923 - val_loss: 129.6889 - val_mae: 8.7750 - learning_rate: 3.2000e-06\n",
            "Epoch 27/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 271.2791 - mae: 12.9775 - val_loss: 128.7037 - val_mae: 8.7247 - learning_rate: 3.2000e-06\n",
            "Epoch 28/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 285.3506 - mae: 13.2392 - val_loss: 128.7980 - val_mae: 8.7397 - learning_rate: 3.2000e-06\n",
            "Epoch 29/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 171ms/step - loss: 281.3584 - mae: 13.3030 - val_loss: 128.7825 - val_mae: 8.7515 - learning_rate: 3.2000e-06\n",
            "Epoch 30/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 167ms/step - loss: 283.5621 - mae: 13.3231 - val_loss: 128.9296 - val_mae: 8.7364 - learning_rate: 3.2000e-06\n",
            "Epoch 31/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 277.6096 - mae: 13.1965 - val_loss: 128.4462 - val_mae: 8.7222 - learning_rate: 1.0000e-06\n",
            "Epoch 32/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 172ms/step - loss: 267.6945 - mae: 12.9180 - val_loss: 128.0125 - val_mae: 8.7093 - learning_rate: 1.0000e-06\n",
            "Epoch 33/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 280.9091 - mae: 13.1150 - val_loss: 128.2859 - val_mae: 8.7247 - learning_rate: 1.0000e-06\n",
            "Epoch 34/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 277.3559 - mae: 13.1435 - val_loss: 128.4911 - val_mae: 8.7379 - learning_rate: 1.0000e-06\n",
            "Epoch 35/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 277.9833 - mae: 13.1311 - val_loss: 128.1282 - val_mae: 8.7092 - learning_rate: 1.0000e-06\n",
            "Epoch 36/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 167ms/step - loss: 291.3304 - mae: 13.5420 - val_loss: 128.3053 - val_mae: 8.7136 - learning_rate: 1.0000e-06\n",
            "Epoch 37/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 283.1887 - mae: 13.3272 - val_loss: 128.1618 - val_mae: 8.7063 - learning_rate: 1.0000e-06\n",
            "Epoch 38/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 274.4644 - mae: 13.0549 - val_loss: 130.0177 - val_mae: 8.7808 - learning_rate: 1.0000e-06\n",
            "\n",
            ">>> Fold 2/5\n",
            "Epoch 1/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 206ms/step - loss: 16958.1055 - mae: 123.8270 - val_loss: 16257.7637 - val_mae: 120.5476 - learning_rate: 4.0000e-04\n",
            "Epoch 2/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 203ms/step - loss: 14511.0723 - mae: 114.3440 - val_loss: 15232.2832 - val_mae: 116.5346 - learning_rate: 4.0000e-04\n",
            "Epoch 3/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 205ms/step - loss: 10974.3516 - mae: 98.5516 - val_loss: 10967.8076 - val_mae: 96.3515 - learning_rate: 4.0000e-04\n",
            "Epoch 4/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 200ms/step - loss: 7357.1440 - mae: 79.3934 - val_loss: 6607.4414 - val_mae: 73.4347 - learning_rate: 4.0000e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 197ms/step - loss: 4066.9453 - mae: 57.5011 - val_loss: 9786.5371 - val_mae: 94.4730 - learning_rate: 4.0000e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 190ms/step - loss: 1899.8422 - mae: 37.4463 - val_loss: 2402.8572 - val_mae: 44.6939 - learning_rate: 4.0000e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 173ms/step - loss: 942.5096 - mae: 24.5140 - val_loss: 1944.3169 - val_mae: 39.0068 - learning_rate: 4.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 167ms/step - loss: 654.7689 - mae: 19.8035 - val_loss: 633.0734 - val_mae: 19.2525 - learning_rate: 4.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 168ms/step - loss: 598.5334 - mae: 18.8411 - val_loss: 2573.6936 - val_mae: 46.1954 - learning_rate: 4.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 541.0679 - mae: 17.9826 - val_loss: 436.6302 - val_mae: 15.9475 - learning_rate: 4.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 499.5612 - mae: 17.3752 - val_loss: 484.6524 - val_mae: 16.8534 - learning_rate: 4.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 168ms/step - loss: 439.7350 - mae: 16.5403 - val_loss: 704.2216 - val_mae: 21.9402 - learning_rate: 4.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 168ms/step - loss: 436.4434 - mae: 16.3576 - val_loss: 1669.6528 - val_mae: 37.4910 - learning_rate: 4.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 358.0569 - mae: 14.9785 - val_loss: 192.4588 - val_mae: 10.7865 - learning_rate: 8.0000e-05\n",
            "Epoch 15/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 168ms/step - loss: 345.4681 - mae: 14.6921 - val_loss: 185.4362 - val_mae: 10.6982 - learning_rate: 8.0000e-05\n",
            "Epoch 16/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 327.4511 - mae: 14.3312 - val_loss: 169.1831 - val_mae: 10.1475 - learning_rate: 8.0000e-05\n",
            "Epoch 17/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 170ms/step - loss: 338.7098 - mae: 14.6651 - val_loss: 262.6867 - val_mae: 12.9486 - learning_rate: 8.0000e-05\n",
            "Epoch 18/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 170ms/step - loss: 327.8536 - mae: 14.3187 - val_loss: 158.8466 - val_mae: 9.7577 - learning_rate: 8.0000e-05\n",
            "Epoch 19/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 317.7969 - mae: 14.1024 - val_loss: 206.3909 - val_mae: 11.1293 - learning_rate: 8.0000e-05\n",
            "Epoch 20/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 170ms/step - loss: 312.9429 - mae: 13.8944 - val_loss: 174.4773 - val_mae: 10.3248 - learning_rate: 8.0000e-05\n",
            "Epoch 21/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 335.7884 - mae: 14.4774 - val_loss: 159.6272 - val_mae: 9.7912 - learning_rate: 8.0000e-05\n",
            "Epoch 22/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 328.6727 - mae: 14.4332 - val_loss: 146.6066 - val_mae: 9.4075 - learning_rate: 1.6000e-05\n",
            "Epoch 23/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 297.5409 - mae: 13.6829 - val_loss: 143.7804 - val_mae: 9.3292 - learning_rate: 1.6000e-05\n",
            "Epoch 24/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 307.8366 - mae: 13.8697 - val_loss: 140.2466 - val_mae: 9.2314 - learning_rate: 1.6000e-05\n",
            "Epoch 25/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 167ms/step - loss: 290.6735 - mae: 13.3727 - val_loss: 140.7236 - val_mae: 9.2002 - learning_rate: 1.6000e-05\n",
            "Epoch 26/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 286.1468 - mae: 13.2989 - val_loss: 147.1215 - val_mae: 9.4163 - learning_rate: 1.6000e-05\n",
            "Epoch 27/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 306.9925 - mae: 13.8641 - val_loss: 143.1313 - val_mae: 9.2954 - learning_rate: 1.6000e-05\n",
            "Epoch 28/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 303.8541 - mae: 13.8422 - val_loss: 140.5332 - val_mae: 9.2183 - learning_rate: 3.2000e-06\n",
            "Epoch 29/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 294.8616 - mae: 13.5705 - val_loss: 139.4932 - val_mae: 9.1648 - learning_rate: 3.2000e-06\n",
            "Epoch 30/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 168ms/step - loss: 293.1378 - mae: 13.4987 - val_loss: 139.5734 - val_mae: 9.1608 - learning_rate: 3.2000e-06\n",
            "Epoch 31/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 293.5478 - mae: 13.4649 - val_loss: 139.6779 - val_mae: 9.1718 - learning_rate: 3.2000e-06\n",
            "Epoch 32/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 168ms/step - loss: 303.0857 - mae: 13.7103 - val_loss: 139.4158 - val_mae: 9.1630 - learning_rate: 3.2000e-06\n",
            "Epoch 33/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 167ms/step - loss: 318.0711 - mae: 14.0756 - val_loss: 138.9923 - val_mae: 9.1410 - learning_rate: 3.2000e-06\n",
            "Epoch 34/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 290.8991 - mae: 13.4247 - val_loss: 139.2605 - val_mae: 9.1553 - learning_rate: 3.2000e-06\n",
            "Epoch 35/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 310.7573 - mae: 13.9852 - val_loss: 139.6524 - val_mae: 9.1656 - learning_rate: 3.2000e-06\n",
            "Epoch 36/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 168ms/step - loss: 315.7155 - mae: 13.9862 - val_loss: 139.4023 - val_mae: 9.1611 - learning_rate: 3.2000e-06\n",
            "Epoch 37/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 300.9392 - mae: 13.7272 - val_loss: 138.9945 - val_mae: 9.1464 - learning_rate: 1.0000e-06\n",
            "Epoch 38/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 308.1578 - mae: 13.8284 - val_loss: 138.9310 - val_mae: 9.1419 - learning_rate: 1.0000e-06\n",
            "Epoch 39/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 295.9003 - mae: 13.5373 - val_loss: 138.6381 - val_mae: 9.1358 - learning_rate: 1.0000e-06\n",
            "Epoch 40/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 172ms/step - loss: 303.7725 - mae: 13.8004 - val_loss: 138.8489 - val_mae: 9.1381 - learning_rate: 1.0000e-06\n",
            "Epoch 41/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 298.7527 - mae: 13.5853 - val_loss: 138.6829 - val_mae: 9.1388 - learning_rate: 1.0000e-06\n",
            "Epoch 42/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 290.0233 - mae: 13.4138 - val_loss: 138.5992 - val_mae: 9.1298 - learning_rate: 1.0000e-06\n",
            "Epoch 43/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 294.8935 - mae: 13.5856 - val_loss: 138.4614 - val_mae: 9.1186 - learning_rate: 1.0000e-06\n",
            "Epoch 44/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 170ms/step - loss: 290.7830 - mae: 13.3807 - val_loss: 138.5807 - val_mae: 9.1217 - learning_rate: 1.0000e-06\n",
            "Epoch 45/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 170ms/step - loss: 291.4833 - mae: 13.5069 - val_loss: 138.7103 - val_mae: 9.1365 - learning_rate: 1.0000e-06\n",
            "Epoch 46/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 296.7882 - mae: 13.6837 - val_loss: 138.5654 - val_mae: 9.1329 - learning_rate: 1.0000e-06\n",
            "Epoch 47/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 297.0927 - mae: 13.6583 - val_loss: 138.6176 - val_mae: 9.1331 - learning_rate: 1.0000e-06\n",
            "Epoch 48/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 287.8297 - mae: 13.3526 - val_loss: 138.6695 - val_mae: 9.1291 - learning_rate: 1.0000e-06\n",
            "Epoch 49/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 289.0530 - mae: 13.3395 - val_loss: 138.7352 - val_mae: 9.1354 - learning_rate: 1.0000e-06\n",
            "\n",
            ">>> Fold 3/5\n",
            "Epoch 1/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 205ms/step - loss: 16951.9434 - mae: 123.7407 - val_loss: 16197.2002 - val_mae: 120.3733 - learning_rate: 4.0000e-04\n",
            "Epoch 2/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 203ms/step - loss: 14488.2314 - mae: 113.9518 - val_loss: 7892.9644 - val_mae: 79.9237 - learning_rate: 4.0000e-04\n",
            "Epoch 3/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 202ms/step - loss: 11334.6963 - mae: 100.3170 - val_loss: 5989.5586 - val_mae: 71.0403 - learning_rate: 4.0000e-04\n",
            "Epoch 4/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 200ms/step - loss: 7533.0151 - mae: 80.5657 - val_loss: 4525.2061 - val_mae: 61.8825 - learning_rate: 4.0000e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 196ms/step - loss: 4086.7976 - mae: 58.2185 - val_loss: 3217.8630 - val_mae: 53.1323 - learning_rate: 4.0000e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 194ms/step - loss: 2093.1062 - mae: 39.7940 - val_loss: 546.2382 - val_mae: 17.3510 - learning_rate: 4.0000e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 175ms/step - loss: 1055.2552 - mae: 26.5889 - val_loss: 581.8628 - val_mae: 19.4220 - learning_rate: 4.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 168ms/step - loss: 627.0440 - mae: 19.7293 - val_loss: 729.6373 - val_mae: 21.6472 - learning_rate: 4.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 172ms/step - loss: 557.3398 - mae: 18.4470 - val_loss: 2470.3208 - val_mae: 45.4826 - learning_rate: 4.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 168ms/step - loss: 420.1697 - mae: 16.0902 - val_loss: 230.2399 - val_mae: 11.2516 - learning_rate: 8.0000e-05\n",
            "Epoch 11/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 387.3845 - mae: 15.5137 - val_loss: 217.6761 - val_mae: 11.1102 - learning_rate: 8.0000e-05\n",
            "Epoch 12/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 397.3241 - mae: 15.6707 - val_loss: 388.5702 - val_mae: 15.5957 - learning_rate: 8.0000e-05\n",
            "Epoch 13/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 369.2912 - mae: 15.1140 - val_loss: 197.3795 - val_mae: 10.6684 - learning_rate: 8.0000e-05\n",
            "Epoch 14/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 172ms/step - loss: 351.9962 - mae: 14.8337 - val_loss: 270.0609 - val_mae: 12.7820 - learning_rate: 8.0000e-05\n",
            "Epoch 15/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 167ms/step - loss: 352.4769 - mae: 14.7970 - val_loss: 299.9116 - val_mae: 13.9111 - learning_rate: 8.0000e-05\n",
            "Epoch 16/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 367.9643 - mae: 15.2172 - val_loss: 193.2749 - val_mae: 10.4418 - learning_rate: 8.0000e-05\n",
            "Epoch 17/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 343.1283 - mae: 14.7186 - val_loss: 182.9150 - val_mae: 10.3314 - learning_rate: 8.0000e-05\n",
            "Epoch 18/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 336.8327 - mae: 14.5151 - val_loss: 184.6029 - val_mae: 10.0024 - learning_rate: 8.0000e-05\n",
            "Epoch 19/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 337.9937 - mae: 14.5848 - val_loss: 222.4984 - val_mae: 11.5735 - learning_rate: 8.0000e-05\n",
            "Epoch 20/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 335.8702 - mae: 14.5558 - val_loss: 257.4429 - val_mae: 12.7825 - learning_rate: 8.0000e-05\n",
            "Epoch 21/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 167ms/step - loss: 319.9426 - mae: 14.1378 - val_loss: 155.3246 - val_mae: 9.3240 - learning_rate: 1.6000e-05\n",
            "Epoch 22/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 302.4804 - mae: 13.6941 - val_loss: 162.8478 - val_mae: 9.5997 - learning_rate: 1.6000e-05\n",
            "Epoch 23/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 323.0298 - mae: 14.1767 - val_loss: 164.3195 - val_mae: 9.6841 - learning_rate: 1.6000e-05\n",
            "Epoch 24/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 315.3825 - mae: 13.8525 - val_loss: 164.5962 - val_mae: 9.6687 - learning_rate: 1.6000e-05\n",
            "Epoch 25/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 302.7114 - mae: 13.8287 - val_loss: 151.2494 - val_mae: 9.1716 - learning_rate: 3.2000e-06\n",
            "Epoch 26/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 168ms/step - loss: 328.5055 - mae: 14.1839 - val_loss: 151.1200 - val_mae: 9.1633 - learning_rate: 3.2000e-06\n",
            "Epoch 27/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 302.9312 - mae: 13.8724 - val_loss: 150.4132 - val_mae: 9.1310 - learning_rate: 3.2000e-06\n",
            "Epoch 28/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 303.7288 - mae: 13.6493 - val_loss: 152.0080 - val_mae: 9.1830 - learning_rate: 3.2000e-06\n",
            "Epoch 29/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 313.3061 - mae: 13.9930 - val_loss: 154.1044 - val_mae: 9.2744 - learning_rate: 3.2000e-06\n",
            "Epoch 30/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 312.3425 - mae: 13.9862 - val_loss: 151.1187 - val_mae: 9.1324 - learning_rate: 3.2000e-06\n",
            "Epoch 31/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 305.2164 - mae: 13.7930 - val_loss: 150.9263 - val_mae: 9.1361 - learning_rate: 1.0000e-06\n",
            "Epoch 32/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 168ms/step - loss: 313.4311 - mae: 13.9664 - val_loss: 151.4303 - val_mae: 9.1608 - learning_rate: 1.0000e-06\n",
            "Epoch 33/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 319.2270 - mae: 14.1099 - val_loss: 151.0546 - val_mae: 9.1468 - learning_rate: 1.0000e-06\n",
            "\n",
            ">>> Fold 4/5\n",
            "Epoch 1/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 206ms/step - loss: 17201.5684 - mae: 124.9362 - val_loss: 16715.3574 - val_mae: 122.4181 - learning_rate: 4.0000e-04\n",
            "Epoch 2/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 206ms/step - loss: 14842.7266 - mae: 115.0264 - val_loss: 16100.8701 - val_mae: 120.1702 - learning_rate: 4.0000e-04\n",
            "Epoch 3/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 202ms/step - loss: 11885.8447 - mae: 102.4202 - val_loss: 6521.4604 - val_mae: 70.0303 - learning_rate: 4.0000e-04\n",
            "Epoch 4/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 200ms/step - loss: 7965.2520 - mae: 82.8870 - val_loss: 8619.6455 - val_mae: 87.8044 - learning_rate: 4.0000e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 197ms/step - loss: 4362.5117 - mae: 60.5723 - val_loss: 1448.0957 - val_mae: 30.0126 - learning_rate: 4.0000e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 193ms/step - loss: 2077.3972 - mae: 39.8690 - val_loss: 561.1278 - val_mae: 18.4315 - learning_rate: 4.0000e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 175ms/step - loss: 958.7592 - mae: 25.4542 - val_loss: 1101.0580 - val_mae: 27.8911 - learning_rate: 4.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 168ms/step - loss: 541.3326 - mae: 18.4320 - val_loss: 547.7533 - val_mae: 19.2250 - learning_rate: 4.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 449.3546 - mae: 16.8278 - val_loss: 499.3588 - val_mae: 18.2014 - learning_rate: 4.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 423.5983 - mae: 16.2253 - val_loss: 220.3135 - val_mae: 11.4110 - learning_rate: 4.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 172ms/step - loss: 390.0336 - mae: 15.6116 - val_loss: 857.2379 - val_mae: 23.6151 - learning_rate: 4.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 399.5802 - mae: 15.7721 - val_loss: 369.9336 - val_mae: 15.3225 - learning_rate: 4.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 170ms/step - loss: 366.3886 - mae: 15.1858 - val_loss: 352.3365 - val_mae: 14.9439 - learning_rate: 4.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 171ms/step - loss: 337.7254 - mae: 14.6066 - val_loss: 154.5779 - val_mae: 9.5112 - learning_rate: 8.0000e-05\n",
            "Epoch 15/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 317.5392 - mae: 14.0316 - val_loss: 153.5832 - val_mae: 9.3595 - learning_rate: 8.0000e-05\n",
            "Epoch 16/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 315.7445 - mae: 14.0125 - val_loss: 180.1778 - val_mae: 10.1872 - learning_rate: 8.0000e-05\n",
            "Epoch 17/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 335.4149 - mae: 14.4566 - val_loss: 160.6199 - val_mae: 9.6251 - learning_rate: 8.0000e-05\n",
            "Epoch 18/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 308.1817 - mae: 13.9132 - val_loss: 177.8033 - val_mae: 10.2936 - learning_rate: 8.0000e-05\n",
            "Epoch 19/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 312.5331 - mae: 13.9693 - val_loss: 138.1647 - val_mae: 8.9044 - learning_rate: 1.6000e-05\n",
            "Epoch 20/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 168ms/step - loss: 311.0046 - mae: 13.9587 - val_loss: 134.2726 - val_mae: 8.7530 - learning_rate: 1.6000e-05\n",
            "Epoch 21/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 286.1104 - mae: 13.3287 - val_loss: 135.2296 - val_mae: 8.7727 - learning_rate: 1.6000e-05\n",
            "Epoch 22/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 170ms/step - loss: 298.0816 - mae: 13.5914 - val_loss: 134.9456 - val_mae: 8.7853 - learning_rate: 1.6000e-05\n",
            "Epoch 23/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 171ms/step - loss: 306.9951 - mae: 13.7631 - val_loss: 136.7693 - val_mae: 8.8485 - learning_rate: 1.6000e-05\n",
            "Epoch 24/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 168ms/step - loss: 306.0716 - mae: 13.7302 - val_loss: 133.3193 - val_mae: 8.7182 - learning_rate: 3.2000e-06\n",
            "Epoch 25/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 277.7586 - mae: 13.2208 - val_loss: 131.8720 - val_mae: 8.6704 - learning_rate: 3.2000e-06\n",
            "Epoch 26/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 167ms/step - loss: 275.1282 - mae: 13.1346 - val_loss: 132.3670 - val_mae: 8.6871 - learning_rate: 3.2000e-06\n",
            "Epoch 27/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 281.6887 - mae: 13.2441 - val_loss: 130.9800 - val_mae: 8.6531 - learning_rate: 3.2000e-06\n",
            "Epoch 28/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 170ms/step - loss: 294.4525 - mae: 13.4321 - val_loss: 131.3859 - val_mae: 8.6468 - learning_rate: 3.2000e-06\n",
            "Epoch 29/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 170ms/step - loss: 293.7998 - mae: 13.4974 - val_loss: 131.7701 - val_mae: 8.6716 - learning_rate: 3.2000e-06\n",
            "Epoch 30/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 168ms/step - loss: 278.6955 - mae: 12.9762 - val_loss: 131.4632 - val_mae: 8.6707 - learning_rate: 3.2000e-06\n",
            "Epoch 31/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 292.9845 - mae: 13.4361 - val_loss: 131.0105 - val_mae: 8.6568 - learning_rate: 1.0000e-06\n",
            "Epoch 32/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 284.1632 - mae: 13.2474 - val_loss: 130.9797 - val_mae: 8.6519 - learning_rate: 1.0000e-06\n",
            "Epoch 33/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 280.4195 - mae: 13.2689 - val_loss: 130.8876 - val_mae: 8.6532 - learning_rate: 1.0000e-06\n",
            "Epoch 34/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 284.2516 - mae: 13.3419 - val_loss: 131.1471 - val_mae: 8.6503 - learning_rate: 1.0000e-06\n",
            "Epoch 35/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 301.6053 - mae: 13.6562 - val_loss: 130.6828 - val_mae: 8.6478 - learning_rate: 1.0000e-06\n",
            "Epoch 36/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 172ms/step - loss: 283.7702 - mae: 13.3593 - val_loss: 131.0261 - val_mae: 8.6623 - learning_rate: 1.0000e-06\n",
            "Epoch 37/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 168ms/step - loss: 292.9485 - mae: 13.5880 - val_loss: 131.3851 - val_mae: 8.6792 - learning_rate: 1.0000e-06\n",
            "Epoch 38/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 168ms/step - loss: 278.3625 - mae: 13.2276 - val_loss: 131.1066 - val_mae: 8.6468 - learning_rate: 1.0000e-06\n",
            "Epoch 39/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 286.0729 - mae: 13.3801 - val_loss: 131.1162 - val_mae: 8.6455 - learning_rate: 1.0000e-06\n",
            "Epoch 40/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 168ms/step - loss: 303.1580 - mae: 13.7503 - val_loss: 131.4706 - val_mae: 8.6873 - learning_rate: 1.0000e-06\n",
            "Epoch 41/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 282.6343 - mae: 13.2375 - val_loss: 131.3013 - val_mae: 8.6784 - learning_rate: 1.0000e-06\n",
            "\n",
            ">>> Fold 5/5\n",
            "Epoch 1/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 205ms/step - loss: 17337.5312 - mae: 125.3331 - val_loss: 16244.9287 - val_mae: 120.6177 - learning_rate: 4.0000e-04\n",
            "Epoch 2/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 202ms/step - loss: 14914.3115 - mae: 115.9608 - val_loss: 10017.0586 - val_mae: 92.9140 - learning_rate: 4.0000e-04\n",
            "Epoch 3/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 205ms/step - loss: 11743.7178 - mae: 102.6777 - val_loss: 7887.6699 - val_mae: 83.9400 - learning_rate: 4.0000e-04\n",
            "Epoch 4/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 201ms/step - loss: 7892.2759 - mae: 83.5148 - val_loss: 2885.9397 - val_mae: 44.9522 - learning_rate: 4.0000e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 197ms/step - loss: 4509.9160 - mae: 61.6425 - val_loss: 1265.1779 - val_mae: 29.5079 - learning_rate: 4.0000e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 191ms/step - loss: 2306.8665 - mae: 42.0934 - val_loss: 521.6445 - val_mae: 17.5543 - learning_rate: 4.0000e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 176ms/step - loss: 1178.5760 - mae: 28.2252 - val_loss: 2199.1208 - val_mae: 37.2179 - learning_rate: 4.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 170ms/step - loss: 688.9492 - mae: 20.7590 - val_loss: 812.8666 - val_mae: 22.7499 - learning_rate: 4.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 511.2255 - mae: 17.8251 - val_loss: 682.7122 - val_mae: 19.7154 - learning_rate: 4.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 418.9861 - mae: 16.1591 - val_loss: 238.3330 - val_mae: 12.0510 - learning_rate: 8.0000e-05\n",
            "Epoch 11/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 399.4561 - mae: 15.7648 - val_loss: 251.1579 - val_mae: 12.2049 - learning_rate: 8.0000e-05\n",
            "Epoch 12/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 168ms/step - loss: 386.9929 - mae: 15.4938 - val_loss: 201.8174 - val_mae: 10.9030 - learning_rate: 8.0000e-05\n",
            "Epoch 13/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 168ms/step - loss: 370.9552 - mae: 15.1717 - val_loss: 229.9834 - val_mae: 11.9568 - learning_rate: 8.0000e-05\n",
            "Epoch 14/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 168ms/step - loss: 379.3267 - mae: 15.4300 - val_loss: 177.9299 - val_mae: 10.1866 - learning_rate: 8.0000e-05\n",
            "Epoch 15/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 168ms/step - loss: 343.5891 - mae: 14.5692 - val_loss: 294.9389 - val_mae: 13.7217 - learning_rate: 8.0000e-05\n",
            "Epoch 16/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 170ms/step - loss: 368.0832 - mae: 15.1465 - val_loss: 292.2704 - val_mae: 13.8878 - learning_rate: 8.0000e-05\n",
            "Epoch 17/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 171ms/step - loss: 376.3652 - mae: 15.2914 - val_loss: 395.5975 - val_mae: 16.5243 - learning_rate: 8.0000e-05\n",
            "Epoch 18/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 318.2841 - mae: 14.0884 - val_loss: 193.1943 - val_mae: 10.8297 - learning_rate: 1.6000e-05\n",
            "Epoch 19/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 329.2649 - mae: 14.2836 - val_loss: 163.6038 - val_mae: 9.6836 - learning_rate: 1.6000e-05\n",
            "Epoch 20/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 329.5646 - mae: 14.3398 - val_loss: 169.7667 - val_mae: 10.0858 - learning_rate: 1.6000e-05\n",
            "Epoch 21/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 170ms/step - loss: 331.7883 - mae: 14.3616 - val_loss: 161.1697 - val_mae: 9.7366 - learning_rate: 1.6000e-05\n",
            "Epoch 22/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 315.1324 - mae: 14.1021 - val_loss: 156.7420 - val_mae: 9.5679 - learning_rate: 1.6000e-05\n",
            "Epoch 23/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 329.4622 - mae: 14.3507 - val_loss: 161.3247 - val_mae: 9.6993 - learning_rate: 1.6000e-05\n",
            "Epoch 24/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 318.9376 - mae: 14.1735 - val_loss: 163.2913 - val_mae: 9.7944 - learning_rate: 1.6000e-05\n",
            "Epoch 25/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 168ms/step - loss: 313.4952 - mae: 13.9509 - val_loss: 156.4422 - val_mae: 9.5567 - learning_rate: 1.6000e-05\n",
            "Epoch 26/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 317.2174 - mae: 14.0129 - val_loss: 164.4249 - val_mae: 9.8591 - learning_rate: 1.6000e-05\n",
            "Epoch 27/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 309.2963 - mae: 13.8023 - val_loss: 159.5053 - val_mae: 9.6806 - learning_rate: 1.6000e-05\n",
            "Epoch 28/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 319.5316 - mae: 14.1522 - val_loss: 155.6408 - val_mae: 9.5117 - learning_rate: 1.6000e-05\n",
            "Epoch 29/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 168ms/step - loss: 308.5865 - mae: 13.8925 - val_loss: 158.8854 - val_mae: 9.6897 - learning_rate: 1.6000e-05\n",
            "Epoch 30/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 168ms/step - loss: 315.7745 - mae: 14.0726 - val_loss: 155.8814 - val_mae: 9.5420 - learning_rate: 1.6000e-05\n",
            "Epoch 31/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 168ms/step - loss: 320.7833 - mae: 14.1679 - val_loss: 153.8768 - val_mae: 9.4439 - learning_rate: 1.6000e-05\n",
            "Epoch 32/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 315.9136 - mae: 13.9497 - val_loss: 162.4928 - val_mae: 9.8138 - learning_rate: 1.6000e-05\n",
            "Epoch 33/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 331.4052 - mae: 14.3612 - val_loss: 156.6686 - val_mae: 9.5388 - learning_rate: 1.6000e-05\n",
            "Epoch 34/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 307.8916 - mae: 13.9340 - val_loss: 161.8440 - val_mae: 9.7588 - learning_rate: 1.6000e-05\n",
            "Epoch 35/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 172ms/step - loss: 313.7273 - mae: 14.0522 - val_loss: 151.3100 - val_mae: 9.3634 - learning_rate: 3.2000e-06\n",
            "Epoch 36/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 303.3420 - mae: 13.6877 - val_loss: 151.7349 - val_mae: 9.4223 - learning_rate: 3.2000e-06\n",
            "Epoch 37/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 310.2346 - mae: 13.8890 - val_loss: 153.0091 - val_mae: 9.4748 - learning_rate: 3.2000e-06\n",
            "Epoch 38/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 300.7899 - mae: 13.7630 - val_loss: 151.5851 - val_mae: 9.4046 - learning_rate: 3.2000e-06\n",
            "Epoch 39/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 301.5579 - mae: 13.7502 - val_loss: 151.1626 - val_mae: 9.3902 - learning_rate: 1.0000e-06\n",
            "Epoch 40/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 172ms/step - loss: 300.8564 - mae: 13.6322 - val_loss: 151.5641 - val_mae: 9.4128 - learning_rate: 1.0000e-06\n",
            "Epoch 41/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 170ms/step - loss: 322.4732 - mae: 14.1489 - val_loss: 152.4773 - val_mae: 9.4559 - learning_rate: 1.0000e-06\n",
            "Epoch 42/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 168ms/step - loss: 306.4116 - mae: 13.7898 - val_loss: 150.7570 - val_mae: 9.3778 - learning_rate: 1.0000e-06\n",
            "Epoch 43/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 168ms/step - loss: 328.2872 - mae: 14.2501 - val_loss: 150.7128 - val_mae: 9.3772 - learning_rate: 1.0000e-06\n",
            "Epoch 44/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 298.2054 - mae: 13.5799 - val_loss: 150.7923 - val_mae: 9.3738 - learning_rate: 1.0000e-06\n",
            "Epoch 45/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 168ms/step - loss: 311.8108 - mae: 13.8367 - val_loss: 150.3533 - val_mae: 9.3558 - learning_rate: 1.0000e-06\n",
            "Epoch 46/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 170ms/step - loss: 307.3628 - mae: 13.8322 - val_loss: 152.2714 - val_mae: 9.4481 - learning_rate: 1.0000e-06\n",
            "Epoch 47/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 301.4076 - mae: 13.6842 - val_loss: 149.6353 - val_mae: 9.3322 - learning_rate: 1.0000e-06\n",
            "Epoch 48/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 308.2480 - mae: 13.8606 - val_loss: 149.7569 - val_mae: 9.3288 - learning_rate: 1.0000e-06\n",
            "Epoch 49/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 314.5097 - mae: 14.0436 - val_loss: 149.7260 - val_mae: 9.3324 - learning_rate: 1.0000e-06\n",
            "Epoch 50/50\n",
            "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 169ms/step - loss: 310.4962 - mae: 13.8960 - val_loss: 150.1807 - val_mae: 9.3521 - learning_rate: 1.0000e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# after CV, average your metric, e.g.:\n",
        "val_maes = np.array([h[\"val_mae\"][-1] for h in all_hist])\n",
        "print(\"Per-fold final val MAE:\", val_maes)\n",
        "print(\"CV mean val MAE:    \", val_maes.mean())\n",
        "print(\"CV std  val MAE:    \", val_maes.std())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuBrrDFFOyLC",
        "outputId": "673d49f2-e6e7-481b-fb53-fcaf1f2305d0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Per-fold final val MAE: [8.78077793 9.13535118 9.14676666 8.67840099 9.35212135]\n",
            "CV mean val MAE:     9.018683624267577\n",
            "CV std  val MAE:     0.2504350974781421\n"
          ]
        }
      ]
    }
  ]
}