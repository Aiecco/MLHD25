{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Modello",
   "id": "368a1b7fcc0b24a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class SpatialAttention(layers.Layer):\n",
    "    \"\"\"\n",
    "    A custom Keras layer implementing a more attentive spatial attention mechanism.\n",
    "    It now includes an intermediate convolutional layer to learn richer attention features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size=7, **kwargs):\n",
    "        super(SpatialAttention, self).__init__(**kwargs)\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        # Intermediate conv layer to learn more complex features from concatenated avg/max pools\n",
    "        # More filters (e.g., 8 or 16) for more capacity, smaller kernel for local patterns.\n",
    "        self.intermediate_conv = layers.Conv2D(\n",
    "            filters=32,  # Increased filters for more \"intelligence\"\n",
    "            kernel_size=5,  # Small kernel for local feature extraction within attention\n",
    "            padding='same',\n",
    "            activation='relu',  # Added ReLU activation for non-linearity\n",
    "            name='attention_intermediate_conv'\n",
    "        )\n",
    "        # Final conv layer to produce the single-channel attention map\n",
    "        self.final_conv = layers.Conv2D(\n",
    "            filters=1,\n",
    "            kernel_size=self.kernel_size,  # Use the initial kernel_size for the final aggregation\n",
    "            padding='same',\n",
    "            activation='sigmoid',  # Sigmoid to output values between 0 and 1\n",
    "            use_bias=False,\n",
    "            name='attention_final_conv'\n",
    "        )\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Keras will automatically build internal layers like self.intermediate_conv and self.final_conv\n",
    "        super(SpatialAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Applies spatial attention to the input feature map.\n",
    "        The attention map is now generated by a slightly deeper sub-network.\n",
    "        \"\"\"\n",
    "        # Compute average and max pool along the channel axis\n",
    "        avg_pool = tf.reduce_mean(inputs, axis=-1, keepdims=True)\n",
    "        max_pool = tf.reduce_max(inputs, axis=-1, keepdims=True)\n",
    "        concat = layers.concatenate([avg_pool, max_pool], axis=-1, name='attention_concat_pools')\n",
    "\n",
    "        # Process through intermediate conv layer\n",
    "        x = self.intermediate_conv(concat)\n",
    "        # Apply final conv to get the attention map\n",
    "        attention_map = self.final_conv(x)\n",
    "\n",
    "        return inputs * attention_map\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(SpatialAttention, self).get_config()\n",
    "        config.update({\"kernel_size\": self.kernel_size})\n",
    "        return config"
   ],
   "id": "78fe0f2df36a2413"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers, regularizers, models\n",
    "\n",
    "from src.Models.AttentionLayer import \\\n",
    "    SpatialAttention  # Assuming this import path is correct and SpatialAttention is defined elsewhere\n",
    "\n",
    "\n",
    "# --- Age Prediction Model with Attention ---\n",
    "class AgePredictionModel:\n",
    "    \"\"\"\n",
    "    Represents a deep learning model for automated bone age prediction from hand radiographs.\n",
    "\n",
    "    This model employs a multi-layered Convolutional Neural Network (CNN) backbone\n",
    "    for feature extraction, integrated with a custom spatial attention mechanism,\n",
    "    and a robust fully connected regression head. It is designed to be gender-agnostic,\n",
    "    relying solely on image-derived features for prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=(128, 128)):\n",
    "        \"\"\"\n",
    "        Initializes the AgePredictionModel.\n",
    "\n",
    "        Args:\n",
    "            img_size (tuple, optional): The target dimensions (height, width) for input images.\n",
    "                                        Defaults to (128, 128). This should match the size\n",
    "                                        used in preprocessing.\n",
    "        \"\"\"\n",
    "        self.img_size = img_size\n",
    "        # Build the Keras model graph immediately upon initialization\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _create_cnn_branch(self, input_tensor: tf.Tensor, name_prefix: str) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Constructs a single Convolutional Neural Network (CNN) branch for feature extraction.\n",
    "\n",
    "        This branch consists of five sequential blocks, each typically comprising two\n",
    "        convolutional layers, Batch Normalization, ReLU activation, and MaxPooling.\n",
    "        The number of filters progressively increases with depth.\n",
    "\n",
    "        Args:\n",
    "            input_tensor (tf.Tensor): The input tensor to the CNN branch (e.g., image input).\n",
    "            name_prefix (str): A prefix for naming the layers within this branch\n",
    "                               to ensure uniqueness (e.g., 'prep', 'raw', 'extr').\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The output tensor from the final MaxPooling layer of this CNN branch,\n",
    "                       representing extracted spatial features.\n",
    "        \"\"\"\n",
    "        # Block 1: Conv -> BN -> ReLU -> Conv -> BN -> ReLU -> MaxPool\n",
    "        # Captures initial low-level features\n",
    "        x = layers.Conv2D(32, (3, 3), padding='same',\n",
    "                          kernel_regularizer=regularizers.l2(1e-4), name=f'{name_prefix}_conv1a')(input_tensor)\n",
    "        x = layers.BatchNormalization(name=f'{name_prefix}_bn1a')(x)\n",
    "        x = layers.Activation('relu', name=f'{name_prefix}_relu1a')(x)\n",
    "        x = layers.Conv2D(32, (3, 3), padding='same',\n",
    "                          kernel_regularizer=regularizers.l2(1e-4), name=f'{name_prefix}_conv1b')(x)\n",
    "        x = layers.BatchNormalization(name=f'{name_prefix}_bn1b')(x)\n",
    "        x = layers.Activation('relu', name=f'{name_prefix}_relu1b')(x)\n",
    "        x = layers.MaxPooling2D((2, 2), strides=(2, 2), name=f'{name_prefix}_pool1')(x)\n",
    "\n",
    "        # Block 2: Increase filters, capture more complex features\n",
    "        x = layers.Conv2D(64, (3, 3), padding='same',\n",
    "                          kernel_regularizer=regularizers.l2(1e-4), name=f'{name_prefix}_conv2a')(x)\n",
    "        x = layers.BatchNormalization(name=f'{name_prefix}_bn2a')(x)\n",
    "        x = layers.Activation('relu', name=f'{name_prefix}_relu2a')(x)\n",
    "        x = layers.Conv2D(64, (3, 3), padding='same',\n",
    "                          kernel_regularizer=regularizers.l2(1e-4), name=f'{name_prefix}_conv2b')(x)\n",
    "        x = layers.BatchNormalization(name=f'{name_prefix}_bn2b')(x)\n",
    "        x = layers.Activation('relu', name=f'{name_prefix}_relu2b')(x)\n",
    "        x = layers.MaxPooling2D((2, 2), strides=(2, 2), name=f'{name_prefix}_pool2')(x)\n",
    "\n",
    "        # Block 3: Further increase filters for higher-level feature abstraction\n",
    "        x = layers.Conv2D(128, (3, 3), padding='same',\n",
    "                          kernel_regularizer=regularizers.l2(1e-4), name=f'{name_prefix}_conv3a')(x)\n",
    "        x = layers.BatchNormalization(name=f'{name_prefix}_bn3a')(x)\n",
    "        x = layers.Activation('relu', name=f'{name_prefix}_relu3a')(x)\n",
    "        x = layers.Conv2D(128, (3, 3), padding='same',\n",
    "                          kernel_regularizer=regularizers.l2(1e-4), name=f'{name_prefix}_conv3b')(x)\n",
    "        x = layers.BatchNormalization(name=f'{name_prefix}_bn3b')(x)\n",
    "        x = layers.Activation('relu', name=f'{name_prefix}_relu3b')(x)\n",
    "        x = layers.MaxPooling2D((2, 2), strides=(2, 2), name=f'{name_prefix}_pool3')(x)\n",
    "\n",
    "        # Block 4: Continue increasing filter depth\n",
    "        x = layers.Conv2D(256, (3, 3), padding='same',\n",
    "                          kernel_regularizer=regularizers.l2(1e-4), name=f'{name_prefix}_conv4a')(x)\n",
    "        x = layers.BatchNormalization(name=f'{name_prefix}_bn4a')(x)\n",
    "        x = layers.Activation('relu', name=f'{name_prefix}_relu4a')(x)\n",
    "        x = layers.Conv2D(256, (3, 3), padding='same',\n",
    "                          kernel_regularizer=regularizers.l2(1e-4), name=f'{name_prefix}_conv4b')(x)\n",
    "        x = layers.BatchNormalization(name=f'{name_prefix}_bn4b')(x)\n",
    "        x = layers.Activation('relu', name=f'{name_prefix}_relu4b')(x)\n",
    "        x = layers.MaxPooling2D((2, 2), strides=(2, 2), name=f'{name_prefix}_pool4')(x)\n",
    "\n",
    "        # Block 5: Final convolutional block in the backbone\n",
    "        # Note: Filter count adjusted to 512 in the theoretical write-up for deeper abstraction,\n",
    "        # but kept at 256 here based on provided code's last working state.\n",
    "        # If input size is 256x256, after 5 pools, spatial dim becomes 8x8.\n",
    "        x = layers.Conv2D(256, (3, 3), padding='same', name='conv5a',\n",
    "                          kernel_regularizer=regularizers.l2(1e-4))(x)  # Added L2 regularization for consistency\n",
    "        x = layers.BatchNormalization(name='bn5a')(x)\n",
    "        x = layers.Activation('relu', name='relu5a')(x)\n",
    "        x = layers.Conv2D(256, (3, 3), padding='same', name='conv5b',\n",
    "                          kernel_regularizer=regularizers.l2(1e-4))(x)  # Added L2 regularization for consistency\n",
    "        x = layers.BatchNormalization(name='bn5b')(x)\n",
    "        x = layers.Activation('relu', name='relu5b')(x)\n",
    "        x = layers.MaxPooling2D((2, 2), name='pool5')(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _build_model(self) -> models.Model:\n",
    "        \"\"\"\n",
    "        Constructs the complete Keras model for bone age prediction.\n",
    "\n",
    "        The model integrates a CNN backbone for feature extraction,\n",
    "        a Spatial Attention layer, and a multi-layered regression head.\n",
    "\n",
    "        Returns:\n",
    "            tf.keras.Model: The compiled Keras Model instance.\n",
    "        \"\"\"\n",
    "        # Define the input layer for preprocessed images.\n",
    "        # The input shape is (height, width, channels), where channels=1 for grayscale.\n",
    "        prep_input = layers.Input(shape=(*self.img_size, 1), name='prep_input')\n",
    "\n",
    "        # Create the CNN branch for feature extraction from the preprocessed input.\n",
    "        prep_features = self._create_cnn_branch(prep_input, 'prep')\n",
    "\n",
    "        # Apply the custom Spatial Attention mechanism to the extracted features.\n",
    "        # This layer selectively re-weights spatial regions, focusing on diagnostically\n",
    "        # relevant areas of the radiograph.\n",
    "        attended_prep_features = SpatialAttention(name='attention_prep')(prep_features)\n",
    "\n",
    "        # Flatten the attended features to prepare for the fully connected layers.\n",
    "        x = layers.Flatten(name='flatten_features')(attended_prep_features)\n",
    "\n",
    "        # --- Regression Head: Fully Connected Layers for Age Prediction ---\n",
    "        # Dense Layer 1: Processes the flattened features.\n",
    "        # Followed by Batch Normalization and Dropout for regularization.\n",
    "        x = layers.Dense(512, activation='relu', name='dense1',\n",
    "                         kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "        x = layers.BatchNormalization(name='bn_dense1')(x)\n",
    "        x = layers.Dropout(0.4, name='dropout1')(x)\n",
    "\n",
    "        # Dense Layer 2: Further refines the features.\n",
    "        # Includes Batch Normalization and Dropout.\n",
    "        x = layers.Dense(256, activation='relu', name='dense2',\n",
    "                         kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "        x = layers.BatchNormalization(name='bn_dense2')(x)\n",
    "        x = layers.Dropout(0.4, name='dropout2')(x)\n",
    "\n",
    "        # Dense Layer 3: Adds more complexity to the mapping.\n",
    "        # Also includes Batch Normalization and Dropout.\n",
    "        x = layers.Dense(128, activation='relu', name='dense3',\n",
    "                         kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "        x = layers.BatchNormalization(name='bn_dense3')(x)\n",
    "        x = layers.Dropout(0.3, name='dropout3')(x)\n",
    "\n",
    "        # Final Output Layer: Predicts the bone age in months.\n",
    "        # 'linear' activation allows for any real value output.\n",
    "        # A subsequent 'relu' activation is applied to ensure predictions are non-negative.\n",
    "        output_linear = layers.Dense(1, name='age_output_linear',\n",
    "                                     kernel_regularizer=regularizers.l2(1e-4))(x)  # Added L2 for consistency\n",
    "        # Force predictions to be non-negative (bone age cannot be < 0)\n",
    "        output = layers.Activation('relu', name='age_output_relu')(output_linear)\n",
    "\n",
    "        # Create the Keras Model instance, defining its inputs and outputs.\n",
    "        model = models.Model(inputs=prep_input, outputs=output, name='AgePredictionModel')\n",
    "        return model\n",
    "\n",
    "    def compile_model(self, learning_rate: float = 0.0005):\n",
    "        \"\"\"\n",
    "        Compiles the Keras model with a specified optimizer, loss function, and metrics.\n",
    "\n",
    "        Args:\n",
    "            learning_rate (float, optional): The initial learning rate for the Adam optimizer.\n",
    "                                             Defaults to 0.0005.\n",
    "        \"\"\"\n",
    "        # Use Adam optimizer for efficient training with adaptive learning rates.\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "        # Compile the model specifying Mean Absolute Error (MAE) as the loss function\n",
    "        # (directly interpretable as error in months) and also track it as a metric.\n",
    "        self.model.compile(optimizer=optimizer, loss='mae', metrics=['mae'])"
   ],
   "id": "b2e152e44a6ed2c5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from typing import Optional, Tuple  # Import for type hinting\n",
    "\n",
    "\n",
    "def train_model(model: tf.keras.Model,\n",
    "                train_dataset: tf.data.Dataset,\n",
    "                epochs: int = 100,\n",
    "                validation_dataset: Optional[tf.data.Dataset] = None,\n",
    "                model_save_path: str = 'best_age_prediction_model.keras') -> Tuple[\n",
    "    tf.keras.Model, tf.keras.callbacks.History]:\n",
    "    \"\"\"\n",
    "    Trains the provided Keras model using specified datasets and callbacks.\n",
    "\n",
    "    This function orchestrates the training process, including saving the best model,\n",
    "    implementing early stopping to prevent overfitting, and dynamically adjusting\n",
    "    the learning rate during training based on validation performance.\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): The already compiled Keras model to be trained.\n",
    "        train_dataset (tf.data.Dataset): The TensorFlow Dataset for training.\n",
    "                                         It should yield (input_data, labels) tuples,\n",
    "                                         where input_data matches the model's input\n",
    "                                         (e.g., preprocessed images).\n",
    "        epochs (int, optional): The maximum number of training epochs. Training\n",
    "                                may stop earlier due to EarlyStopping. Defaults to 50.\n",
    "        validation_dataset (tf.data.Dataset, optional): The TensorFlow Dataset for validation.\n",
    "                                                        Used to monitor performance and guide\n",
    "                                                        EarlyStopping and ReduceLROnPlateau.\n",
    "                                                        Should have the same structure as `train_dataset`.\n",
    "                                                        Defaults to None.\n",
    "        model_save_path (str, optional): The file path where the best performing model\n",
    "                                         (based on validation MAE) will be saved.\n",
    "                                         Defaults to 'best_age_prediction_model.keras'.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[tf.keras.Model, tf.keras.callbacks.History]: A tuple containing:\n",
    "            - tf.keras.Model: The trained model instance. Its weights will be restored\n",
    "                              to the best observed performance during training if EarlyStopping\n",
    "                              with `restore_best_weights=True` is used.\n",
    "            - tf.keras.callbacks.History: An object containing the history of loss and\n",
    "                                           metric values during training.\n",
    "    \"\"\"\n",
    "    print(f\"\\nStarting model training for {epochs} epochs...\")\n",
    "\n",
    "    # Define Keras Callbacks for enhanced training control and regularization.\n",
    "    callbacks = [\n",
    "        # ModelCheckpoint: Saves the best model weights observed during training.\n",
    "        # It monitors 'val_mae' (Mean Absolute Error on validation set) and saves\n",
    "        # only if the monitored metric improves (mode='min' for MAE).\n",
    "        ModelCheckpoint(\n",
    "            filepath=model_save_path,\n",
    "            monitor='val_mae' if validation_dataset else 'mae',\n",
    "            # Monitor validation MAE if validation set exists, else training MAE\n",
    "            save_best_only=True,  # Only save the model when validation MAE improves\n",
    "            mode='min',  # 'min' mode means lower is better for the monitored metric (MAE)\n",
    "            verbose=1  # Display messages when a better model is saved\n",
    "        ),\n",
    "        # EarlyStopping: Halts training if the monitored metric does not improve\n",
    "        # for a specified number of epochs (patience). This prevents overfitting.\n",
    "        EarlyStopping(\n",
    "            monitor='val_mae' if validation_dataset else 'mae',  # Monitor validation MAE\n",
    "            patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
    "            mode='min',  # 'min' mode for MAE\n",
    "            verbose=1,  # Display messages when early stopping is triggered\n",
    "            restore_best_weights=True  # Restore model weights from the epoch with the best monitored value\n",
    "        ),\n",
    "        # ReduceLROnPlateau: Dynamically reduces the learning rate when a metric\n",
    "        # has stopped improving. This helps the model to converge more finely.\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_mae' if validation_dataset else 'mae',  # Monitor validation MAE\n",
    "            factor=0.5,  # Factor by which the learning rate will be reduced (new_lr = lr * factor)\n",
    "            patience=5,  # Number of epochs with no improvement after which the learning rate will be reduced\n",
    "            min_lr=1e-6,  # Lower bound on the learning rate\n",
    "            mode='min',  # 'min' mode for MAE\n",
    "            verbose=1  # Display messages when learning rate is reduced\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Start the model training process.\n",
    "    # The 'initial_epoch=5' parameter means training will start from epoch 5.\n",
    "    # This might be useful if resuming training and wanting to skip initial already-converged epochs\n",
    "    # or to allow callbacks to become active after a few epochs.\n",
    "    # If resuming from a loaded model, Keras automatically handles the initial epoch correctly.\n",
    "    # It's important to ensure this parameter aligns with the overall training strategy.\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_dataset,\n",
    "        callbacks=callbacks,\n",
    "        initial_epoch=0\n",
    "    )\n",
    "    print(\"\\nModel training completed.\")\n",
    "    return model, history"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T20:10:41.309424Z",
     "start_time": "2025-06-23T20:10:40.564098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# IPERPARAMETRI:\n",
    "learning_rate = 0.0005\n",
    "img_sizes = 256\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "model_builder = AgePredictionModel(img_size=(img_sizes, img_sizes))\n",
    "# Compile the newly created model with the specified learning rate.\n",
    "model_builder.compile_model(learning_rate=learning_rate)\n",
    "model_to_train = model_builder.model\n",
    "print(\"\\nCreating and compiling a new model for training.\")\n",
    "\n",
    "# --- Model Training Execution ---\n",
    "print(\"\\nStarting training:\")\n",
    "try:\n",
    "    # Call the core `train_model` function to execute the training loop.\n",
    "    # This function handles epochs, validation, and callbacks (e.g., ModelCheckpoint, EarlyStopping).\n",
    "    trained_model, history = train_model(\n",
    "        model=model_to_train,  # The model to be trained\n",
    "        train_dataset=train_dataset,\n",
    "        validation_dataset=val_dataset,\n",
    "        epochs=epochs,\n",
    "        model_save_path=model_save_path\n",
    "    )\n",
    "    # Plot the training and validation loss/metrics history.\n",
    "    #plot_training_history(history, save_path='training_history_plots.png')\n",
    "\n",
    "    print(f\"Training history keys: {history.history.keys()}\")\n",
    "    print(f\"Model trained and saved to '{model_save_path}'.\")\n",
    "    return trained_model  # Return the trained model instance\n",
    "except Exception as e:\n",
    "    # Catch any exceptions during training and print a detailed traceback.\n",
    "    print(f\"Error during model training: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()  # Print the full stack trace for debugging\n",
    "    return None  # Indicate that training failed by returning None"
   ],
   "id": "ec10070dc56ac2f7",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AgePredictionModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# IPERPARAMETRI:\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m model_builder \u001B[38;5;241m=\u001B[39m \u001B[43mAgePredictionModel\u001B[49m(img_size\u001B[38;5;241m=\u001B[39m(img_sizes, img_sizes))\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Compile the newly created model with the specified learning rate.\u001B[39;00m\n\u001B[0;32m      6\u001B[0m model_builder\u001B[38;5;241m.\u001B[39mcompile_model(learning_rate\u001B[38;5;241m=\u001B[39mlearning_rate)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'AgePredictionModel' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4894cf4d5f0089f7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
